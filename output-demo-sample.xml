<?xml version="1.0"?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="research-article" xml:lang="en">
    <front id="3a620909-f126-490b-a9a7-83b980ea67f3" data-edited="true" data-edited-version="2">
        <journal-meta>
            <journal-id journal-id-type="publisher-id">default</journal-id>
            <journal-title-group>
                <journal-title>default</journal-title>
                <abbrev-journal-title>default</abbrev-journal-title>
            </journal-title-group>
            <issn publication-format="electronic" pub-type="epub">1234-232</issn>
            <issn publication-format="print" pub-type="ppub">123-12</issn>
            <publisher>
                <publisher-name>default</publisher-name>
            </publisher>
        </journal-meta>
        <article-meta>
            <article-id pub-id-type="publisher-id">default123</article-id>
            <article-id pub-id-type="doi">10.1021/default123</article-id>
            <title-group>
                <article-title>
                    <bold>Putting ChatGPT Vision (GPT-4V) to the test: Risk perception in traffic
                        images</bold>
                </article-title>
            </title-group>
            <contrib-group content-type="author">
                <contrib contrib-type="author">
                    <name rid="de22956c-a0dd-494e-a5ac-86957e37df38">
                        <given-name>Tom</given-name>
                        <surname>Driessen</surname>
                    </name>
<xref ref-type="aff" rid="de22956c-a0dd-494e-a5ac-86957e37df38" data-citation-string=" de22956c-a0dd-494e-a5ac-86957e37df38 ">
                        <sup>1</sup>
                    </xref>, <name rid="de22956c-a0dd-494e-a5ac-86957e37df38">
                        <given-name>Dimitra</given-name>
                        <surname>Dodou</surname>
                    </name>
<xref ref-type="aff" rid="de22956c-a0dd-494e-a5ac-86957e37df38" data-citation-string=" de22956c-a0dd-494e-a5ac-86957e37df38 ">
                        <sup>1</sup>
                    </xref>, <name rid="6073326f-4cb9-4d4e-b75a-6abffd988501">
                        <given-name>Pavlo</given-name>
                        <surname>Bazilinskyy</surname>
                    </name>
<xref ref-type="aff" rid="6073326f-4cb9-4d4e-b75a-6abffd988501" data-citation-string=" 6073326f-4cb9-4d4e-b75a-6abffd988501 ">
                        <sup>2</sup>
                    </xref>, <name rid="de22956c-a0dd-494e-a5ac-86957e37df38">
                        <given-name>Joost</given-name>
                        <surname>de Winter</surname>
                    </name>
<xref ref-type="aff" rid="de22956c-a0dd-494e-a5ac-86957e37df38" data-citation-string=" de22956c-a0dd-494e-a5ac-86957e37df38 ">
                        <sup>1</sup>
                    </xref>
                </contrib>
                <aff id="de22956c-a0dd-494e-a5ac-86957e37df38"><label data-id="de22956c-a0dd-494e-a5ac-86957e37df38">
                        <sup>1 </sup>
                    </label><institution>Delft University of
                    Technology</institution>, <country>The Netherlands</country>
                </aff>
                <aff id="6073326f-4cb9-4d4e-b75a-6abffd988501"><label data-id="6073326f-4cb9-4d4e-b75a-6abffd988501">
                        <sup>2 </sup>
                    </label><institution>Eindhoven University
                        of Technology</institution>, <country>The Netherlands</country>
                </aff>
            </contrib-group>
            <abstract>
                <title>
                    <bold>Abstract</bold>
                </title>
                <p>Vision-language models are of interest in various domains, including automated
                    driving, where computer vision techniques can accurately detect road users, but
                    where the vehicle sometimes fails to understand context. This study examined the
                    effectiveness of GPT-4V in predicting the level of ‘risk’ in
                    traffic images as assessed by humans. We used 210 static images taken from a
                    moving vehicle, each previously rated by approximately 650 people. Based on
                    psychometric construct theory and using insights from the self-consistency
                    prompting method, we formulated three hypotheses: 1) repeating the prompt under
                    effectively identical conditions increases validity, 2) varying the prompt text
                    and extracting a total score increases validity compared to using a single
                    prompt, and 3) in a multiple regression analysis, the incorporation of object
                    detection features, alongside the GPT-4V-based risk rating, significantly
                    contributes to improving the model’s validity. Validity was quantified by
                    the correlation coefficient with human risk scores, across the 210 images. The
                    results confirmed the three hypotheses. The eventual validity coefficient was r
                    = 0.83, indicating that population-level human risk can be predicted using AI
                    with a high degree of accuracy. The findings suggest that GPT-4V must be
                    prompted in a way equivalent to how humans fill out a multi-item questionnaire.</p>
            </abstract>
        </article-meta>
    </front>
    <body>
        <title data-level="1">
                <bold>April </bold>
                <bold>10</bold>
                <bold>, </bold>
                <bold>2024</bold>
            </title>
        <title data-level="1">
                <bold>Introduction</bold>
            </title><title data-level="2">
                    <bold>GPT-4V Background</bold>
                </title><p>In late September 2023, OpenAI introduced image-to-text functionality for
                    ChatGPT, also called GPT-4V or GPT4 Vision. At that time, image-to-text
                    software, such as BLIP, and functionalities within Google’s Bard and Bing
                    Chat were already available (Bing, 2023; <xref ref-type="bibr" rid="" data-citation-string="  ">Google, 2023;</xref> <xref ref-type="bibr" rid="R29">Li et al., 2022;</xref> see <xref ref-type="bibr" rid="R8">Cui et al., 2024 </xref>for a survey on multimodal large language
                    models). However, GPT-4V was highly anticipated due to the high quality of its
                    output, as demonstrated in earlier previews (OpenAI, 2023).</p><p>The research so far demonstrates that GPT-4V exhibits strong generic skills. It
                    can comprehend diverse stimuli such as written text, charts, graphical user
                    interfaces, abstract visual pictures, and visual IQ tests <xref ref-type="bibr" rid="" data-citation-string="  ">(Ahrabian
                    et al., 2024;</xref> <xref ref-type="bibr" rid="" data-citation-string="  ">Yan et al., 2023;</xref> Z<xref ref-type="bibr" rid="R8">. Yang et al., 2023).</xref> GPT-4V is also
                    capable of solving visual mathematical problems, although not yet at a high
                    level <xref ref-type="bibr" rid="R8">(Lu et al., 2023).</xref> As of early
                    2024, GPT-4V is still considered superior to a recent competitor from Google,
                    called Gemini-Pro (M<xref ref-type="bibr" rid="R8">. Liu et al., 2024;</xref> <xref ref-type="bibr" rid="R48">Qi et al., 2023),</xref> but see proprietary
                    evaluations of Google’s largest model, Gemini-Ultra (Gemini Team <xref ref-type="bibr" rid="" data-citation-string="  ">Google, 2023;</xref> <xref ref-type="bibr" rid="R67">Yue et
                    al., 2023).</xref></p><p>There is strong interest in GPT-4V within the domain of automated driving.
                    Current automated vehicles are effective at detecting objects and handling
                    routine scenarios, but the challenge still lies in rare situations that are not
                    included in the training data <xref ref-type="bibr" rid="R5">(Bogdoll et al.,
                    2022;</xref> <xref ref-type="bibr" rid="R25">Jain et al., 2021).</xref> The
                    strength of GPT-4V (and other vision language models) is its ability to
                    understand context, including scenarios not previously encountered <xref ref-type="bibr" rid="" data-citation-string="  ">(Hwang et al., 2024;</xref> Z<xref ref-type="bibr" rid="R58">. Yang et al., 2023;</xref> <xref ref-type="bibr" rid="R72">Zhou
                    &amp; Knoll, 2024).</xref> On the other hand, while GPT-4V is skilled in
                    recognising unusual traffic events, it is not skilled at seemingly trivial tasks
                    such as recognising details like the status of traffic lights, and spatial tasks
                    such as reporting the orientation and (relative) position of road users <xref ref-type="bibr" rid="R58">(Wen et al., 2023;</xref> <xref ref-type="bibr" rid="R72">Zhou &amp; Knoll, 2024).</xref></p><p>Indeed, GPT-4V exhibits several limitations. It struggles with counting objects
                    and judging details, such as answering the question “<italic>How many
                        eyes can you see on the animal?</italic>” or “<italic>Count
                        the
                        number of trees in the given image</italic>”, tasks that normally do
                    not pose a challenge for humans <xref ref-type="bibr" rid="" data-citation-string="  ">(Tong et al., 2024;</xref> <xref ref-type="bibr" rid="R4">Zhang &amp; Wang, 2024).</xref> Furthermore,
                    although GPT-4V performs well in commonsense visual question answering, it is
                    prone to hallucinations when world knowledge is required, such as about
                    real-world objects (Y<xref ref-type="bibr" rid="R8">. Li et al., 2024),</xref>
                    especially for objects from non-Western countries <xref ref-type="bibr" rid="R8">(Cui et al., 2023).</xref> A similar pattern has been observed for
                    medical images, where GPT-4V does not seem to possess the knowledge required for
                    making accurate diagnoses or reports (Senkaiahliyan <xref ref-type="bibr" rid="R2">et al., 2023;</xref> <xref ref-type="bibr" rid="R9">Wu et al.,
                    2023<xref ref-type="bibr" rid="R19" data-citation-string=" R19 ">). Guan et al. (2023) </xref></xref>made a
                    distinction between visual illusions, in which a visual element is
                    misrepresented, and language hallucinations, where GPT-4V fails to recognise a
                    feature in the image because it adheres to previously learned stereotypical
                    responses for similar images. Guan et al. also indicated that ChatGPT exhibits
                    limitations in temporal reasoning abilities.</p><title data-level="2">
                    <bold>Prompting Methods</bold>
                </title><p>Different strategies exist for improving the output of GPT-4V. This includes a
                    prompting method where images are first segmented and marked with characters or
                    boxes before being submitted to GPT-4V (J<xref ref-type="bibr" rid="R8">. Yang
                    et al., 2023).</xref> The use of composite images (Y<xref ref-type="bibr" rid="R8">. Li et al., 2024),</xref> comparing images in pairs <xref ref-type="bibr" rid="R1">(Zhang et al., 2023),</xref> or multimodal
                    cooperation <xref ref-type="bibr" rid="R8">(Ye et al., 2023) </xref>are other
                    viable strategies. Additionally, the literature recommends chain-of-thought
                    prompting for GPT-4V <xref ref-type="bibr" rid="" data-citation-string="  ">(Ahrabian et al., 2024;</xref> <xref ref-type="bibr" rid="R22">Hou et al., 2024;</xref> <xref ref-type="bibr" rid="R1">Zhang et al., 2024),</xref> a strategy also known for text-only
                    ChatGPT (Bellini-Leite, 2023; <xref ref-type="bibr" rid="R56">Wei et al.,
                        2022).</xref> Others have converted visual information into text first,
                    using a prompt such as “<italic>what’s in this image?</italic>”;
                    this method is promising when processing large quantities of images that occur
                    in a temporal sequence (Y<xref ref-type="bibr" rid="R8">. Liu et al., 2024).</xref></p><p>Small variations in the prompt can lead to substantially different outputs of
                    large language models <xref ref-type="bibr" rid="R20">(Huang et al., 2023;</xref> <xref ref-type="bibr" rid="R1">Salinas &amp; Morstatter, 2024).</xref> For
                    example, when a list of short phrases is submitted to GPT for sentiment
                    analysis, but the same list is sorted in a different order, the sentiment score
                    from GPT is usually different, even if GPT is set to produce near-zero variation
                    through its temperature parameter (Tabone &amp; De <xref ref-type="bibr" rid="R10">Winter, 2023).</xref> This variation is inherent to the
                    autoregressive manner in which transformer models produce tokens.</p><p>A technique to mitigate this randomness is self-consistency, also referred to as
                    bootstrapping (Tabone &amp; De <xref ref-type="bibr" rid="R10">Winter, 2023;</xref> <xref ref-type="bibr" rid="" data-citation-string="  ">Tang et al., 2023;</xref> <xref ref-type="bibr" rid="R4">Wang
                    et al., 2023):</xref> After repeating the prompting process multiple times, each
                    time with a different permutation of the text, the modal or mean output can be
                    extracted. This aggregate typically has higher accuracy than the output of a
                    single prompt. Various refinements of the self-consistency method exist <xref ref-type="bibr" rid="R31">(Fu et al., 2023;</xref> <xref ref-type="bibr" rid="R8">Li et al., 2023),</xref> more recently expanded to the notion of
                    invoking multiple different language models (J<xref ref-type="bibr" rid="R8">.
                    Li et al., 2024;</xref> <xref ref-type="bibr" rid="R8">Lu et al., 2024).</xref></p><p>It is our proposition that self-consistency prompting resembles how constructs
                    are defined in psychometrics. In psychology, a construct, such as personality
                    (e.g., extraversion), can be estimated by having the person fill out multiple
                    questionnaire items. By averaging the results of items that have been sampled
                    from a domain of possible items, an estimation of the construct can be made <xref ref-type="bibr" rid="R7">(Cronbach et al., 1972;</xref> <xref ref-type="bibr" rid="R34">Little et al., 2013;</xref> <xref ref-type="bibr" rid="R41">McDonald, 2003;</xref> <xref ref-type="bibr" rid="R46">Nunnally
                    &amp; Bernstein, 1994;</xref> <xref ref-type="bibr" rid="R51">Sawaki, 2010).</xref></p><title data-level="2">
                    <bold>Current Study</bold>
                </title><p>This research focuses on evaluating GPT-4V, but not as in identifying specific
                    visual elements, a domain in which GPT-4V demonstrates limited performance.
                    Instead, we conducted a holistic assessment by examining the ability of GPT-4V
                    to predict ‘risk’ as evaluated by humans. Instead, we conducted a
                    holistic evaluation, where we examined how well GPT-4V can predict
                    ‘risk’ as assessed by humans. More specifically, this study
                    presents an assessment of GPT-4V concerning the prediction of risk in
                    forward-facing photographs from the perspective of a moving vehicle.</p><p>Our analysis draws on a prior study (De <xref ref-type="bibr" rid="R10">Winter
                        et al., 2023),</xref> in which human crowdworkers assessed the risk of
                    traffic images, taken by a camera mounted on the roof of a car while driving on
                    German roads (KITTI dataset; <xref ref-type="bibr" rid="R15">Geiger et al.,
                    2013).</xref> In De Winter et al., a total of 210 images were rated by an
                    average of 653 participants per image. Based on these ratings on a scale ranging
                    from 0 (no risk) to 10 (extreme risk), a mean risk score was computed for each
                    image.</p><p>De <xref ref-type="bibr" rid="R10">Winter et al. (2023) </xref>investigated
                    whether the images’ risk level, as assessed by humans, was predictable
                    based on features extracted by a pretrained object detection algorithm <xref ref-type="bibr" rid="" data-citation-string="  ">(Bochkovskiy et al., 2020;</xref> <xref ref-type="bibr" rid="R49">Redmon &amp; Farhadi, 2018),</xref> see <xref ref-type="fig" rid="FA1" data-citation-string=" FA1 ">Figure A1</xref> in the Appendix. Their analysis showed that the
                    number of people in the image (<italic>r</italic> = 0.33) and the mean size of
                    the bounding boxes (<italic>r</italic> = 0.54) were predictive of the human risk
                    scores. The driving speed was negatively predictive (<italic>r</italic> =
                    -0.63), which can be explained by risk compensation (a less strict variant of
                    risk homeostasis; <xref ref-type="bibr" rid="R59">Wilde, 1982,</xref> 2013):
                    some situations, like empty roads, allow drivers to drive at the maximum allowed
                    speed without it being high risk. Conversely, complex traffic environments, such
                    as city centres, lead people to drive slowly <xref ref-type="bibr" rid="R6">(Charlton
                    et al., 2010).</xref> Through a regression analysis, the three measures combined
                    (number of people, size of bounding boxes, and vehicle speed) were found to be
                    strongly predictive of the human risk level (<italic>r</italic> = 0.75).
                    Excluding the speed variable, the prediction was weaker but still substantial (<italic>
                    r</italic> = 0.62) (De <xref ref-type="bibr" rid="R10">Winter et al., 2023).</xref></p><p>One might wonder why the prediction derived from the object detection was not
                    more strongly indicative of the human risk ratings. In the previous study, we
                    hypothesised that the object detection algorithm does not account for contextual
                    information. For example, an image of a railroad crossing was perceived as
                    hazardous by the human evaluators, whereas the object detection algorithm could
                    not detect this railroad and did not understand the broader situation (De <xref ref-type="bibr" rid="R10">Winter et al., 2023).</xref> In the current
                    study, we explored whether GPT-4V could contribute to a more accurate assessment
                    of the risk in the traffic images as compared to using object detection features
                    alone.</p><title data-level="2">
                    <bold>Hypotheses</bold>
                </title><p><xref ref-type="fig" rid="F1" data-citation-string=" F1 ">Figure 1</xref> provides one manner in which
                    construct validity can be interpreted for risk ratings. Here, the risk score for
                    a given image is the arithmetic mean risk from a large number of participants.
                    These participants might all have had slightly different interpretations of the
                    same rating task. For example, Participant 1 might interpret the task as
                    ‘probability of an accident occurring’, Participant 2 as
                    ‘difficulty of the task’, etc.—interpretations that are
                    positively correlated but not the same <xref ref-type="bibr" rid="R14">(Fuller,
                    2005).</xref> The risk score for an image is thus an aggregate of a potentially
                    infinite number of interpretations, but bounded to a domain of possible
                    interpretations. Additionally, the same participant will not perform a reliable
                    evaluation under a given interpretation of the task. For example, a participant
                    may be distracted or overlook something in the image for arbitrary reasons.
                    Therefore, noise is present, also known as ‘measurement error’.</p><fig data-stream-name="a_F1" data-id="BLK_F1" position="float" fig-type="figure" orientation="portrait">
                    <label data-id="F1">
                        <italic>Figure 1.</italic>
                    </label>
                    <caption><title data-id="F1"><italic> </italic>Causal process of how a participant generates a risk
                        score for an image. The participant observes the image and task instruction
                        presented on a computer screen, makes one (or a combination of multiple)
                        interpretation(s), and enters a numerical risk score. The overall risk score
                        for a given image represents the average from a large number of
                        participants, thus reflecting an aggregation of a large number of different
                        interpretations. This conceptualisation of construct validity is based on <xref ref-type="bibr" rid="R40">Markus and Borsboom (2013).</xref></title></caption>
                    <graphic xlink:href="/resources/7dd3fab9-473d-4ec2-a322-febd8351f92c/images/image1.emf.tiff.JPEG"/>
                </fig><p>Considering the use of GPT-4V to approximate this human risk score as accurately
                    as possible, three hypotheses are formulated. In each of the three hypotheses,
                    validity is defined as the correlation coefficient between the mean risk score
                    of GPT-4V and the human risk score.</p><p>H1: Repeating the same prompt under nearly identical conditions (in our case:
                    keeping the images and prompt text identical, and only changing the order of the
                    images within the same prompt) will result in higher validity as compared to
                    using the exact same prompt.</p><p>H2: Aggregating the results of different prompts within a behavioural domain (in
                    our case: slightly rephrasing the question) will result in higher validity as
                    compared to using a single prompt text.</p><p>The aforementioned hypotheses are consistent with the self-consistency prompting
                    method <xref ref-type="bibr" rid="R4">(Wang et al., 2023),</xref> but adapted
                    for quantitative assessment and motivated from a psychometric perspective. Here,
                    H1 is equivalent to the use of items in parallel forms, with the aim to reduce
                    measurement error, while H2 is equivalent to the use of multiple items to
                    estimate a latent construct.</p><p>H3: In a multiple regression analysis with GPT-4V included, object detection
                    features, as used by De <xref ref-type="bibr" rid="R10">Winter et al. (2023),</xref>
                    will statistically significantly contribute to predicting human risk. This
                    hypothesis is based on the previously mentioned review, which indicated that
                    GPT-4V possesses generic skills but may fail to recognise specific elements in
                    images (e.g<xref ref-type="bibr" rid="R58">., Wen et al., 2023;</xref> <xref ref-type="bibr" rid="R72">Zhou &amp; Knoll, 2024).</xref> Hence, the two
                    different AI-based methods (vision-language model vs. object detection) were
                    expected to have complementary value.</p><p>This study was conducted in two phases. Phase 1 was carried out using GPT-4V as
                    available in the ChatGPT web interface. This approach was chosen because many
                    users might not have access to the API (an interface for programmatic access),
                    and because the API was not available at the time of this assessment. Phase 1
                    aimed to explore how multiple images could be evaluated and aggregated. To this
                    end, we used composite images, with a selection of 10 images each time in a
                    different order. We investigated to what extent repetitions result in an
                    increase in validity (H1).</p><p>In Phase 2, we used OpenAI’s API for GPT-4V, which has recently become
                    available. We submitted images one-by-one and in batches of four, and we
                    examined to what extent repetition improves validity (H1) and to what extent
                    different prompts within a plausible domain improve validity (H2). Finally, we
                    investigated whether object detection scores contribute to validity in a linear
                    regression model (H3).</p><title data-level="2">
                    <bold>Methods</bold>
                </title><title data-level="3">
                        <bold>Phase 1: ChatGPT Web Interface</bold>
                    </title><p>The images were uploaded to the ChatGPT web interface as composites of 10,
                        randomly selected from the total of 210 images. The individual images had a
                        size of 1242×375 pixels. Due to the web interface having a limit of
                        about 90 prompts in a 3-hour interval, this approach proved to be suitable
                        for processing a large number of images. The decision to submit 10 images
                        per prompt was based on trial and error, aiming to maximise the number of
                        images submitted in a single prompt while maintaining good quality of
                        GPT-4V’s output. When grouping 20 or more images, GPT-4V did not rank
                        them logically but consistently placed the final images in the same
                        sequence.</p><p>The 10 images were grouped into one image, 2 images wide and 5 images high,
                        with a total resolution of 2544×1995 pixels (including a small white
                        margin around each individual image). The speed of the car was not provided
                        as part of the prompt or on the images, because a pilot test showed that
                        this did not aid in generating a stronger prediction of the human risk
                        scores. The 10 images were grouped using MATLAB (the source code is
                        available in the Supplementary Material).</p><p>Between October 26 and November 1, 2023, we prompted GPT-4V a total of 974
                        times, each time using a randomly selected 10 out of 210 images, and each
                        prompt in a new chat window. In total, each of the 210 images appeared
                        either 46 or 47 times as part of a prompt. In each prompt, we requested
                        GPT-4V to rank or rate the 10 images. An example of a prompt is shown in <xref ref-type="fig" rid="F2" data-citation-string=" F2 ">Figure 2</xref>. In total, we used four
                        different prompts:</p><p>1.   “<italic>Imagine you are a driver. Rank the 10
                        images in terms of risk from highest (1) to lowest (10). Only report a rank;
                        nothing else. The image number is shown at the right bottom.</italic>”
                        (210 prompts)</p><p>2.   “<italic>As a driver, rank the 10 images from the
                        least to the most risky driving situation. Each image is labeled with a
                        letter in the bottom right corner. Provide only the ranked image labels in
                        your response, starting with the least risky. For example: j, b, c, h, e, g,
                        a, i, d, f</italic>” (344 prompts)</p><p>3.   “<italic>Imagine you are a driver. Rank the 10
                        images in terms of risk from lowest to highest. Only report the ranked image
                        numbers in a single column; nothing else. The image number is shown at the
                        bottom right of each image.</italic>” (210 prompts)</p><p>4.   “<italic>Imagine you are a driver. Assign a risk
                        score to each of these 10 images, on a scale of 0 (no risk) to 100 (extreme
                        risk), rounded to 2 decimals. Only report the risk percentages in a single
                        column. Nothing else; no percentage sign either.</italic>” (210
                        prompts)</p><p>These four prompts were used to determine if the type of prompt affected the
                        results. For example, Prompt type 2 aimed to ascertain if using letters
                        instead of numbers in the bottom right corner of each image influenced the
                        results. We also tested how ranking from low to high (Prompt types 2
                        &amp; 3) or from high to low (Prompt type 1) affected the output, and if
                        giving a numeric rating (Prompt type 4) instead of a ranking possibly
                        yielded better results.</p><fig data-stream-name="a_F2" data-id="BLK_F2" position="float" fig-type="figure" orientation="portrait">
                        <label data-id="F2">
                            <italic>Figure 2.</italic>
                        </label>
                        <caption><title data-id="F2"><italic> </italic>Example prompt and output of GPT-4V (Prompt type
                            3). The prompt includes a random selection of 10 of 210 images. The 10
                            images were combined into a single image.</title></caption>
                        <graphic xlink:href="/resources/7dd3fab9-473d-4ec2-a322-febd8351f92c/images/image2.emf.tiff.JPEG"/>
                    </fig><p>For Prompt types 1–3, ChatGPT ranked the images according to their
                        risk, and we calculated a mean rank for each of the 210 images. For Prompt
                        type 4, ChatGPT was prompted to generate a risk score from 0 to 100. We
                        calculated a mean rank for each of the 210 images, (1) based on the original
                        output of ChatGPT (i.e., as prompted from 0 to 100), (2) after applying a
                        rank transformation, so that the results are comparable to Prompt types
                        1–3, and (3) after applying a <italic>z-</italic>score
                        transformation, where the mean across the 10 images is 0 and the standard
                        deviation is 1.</p><p>The GPT-4V mean scores for the 210 images were then correlated with human
                        risk scores as previously determined in De <xref ref-type="bibr" rid="R10">Winter
                        et al. (2023).</xref> These human risk scores are the average of 1,378
                        crowdworkers, each having rated a random 100 out of the 210 images for risk
                        in response to the question “<italic>As a driver, how risky would you
                        judge this situation (</italic><italic>0</italic><italic> </italic><italic>=</italic><italic>
                        no risk, </italic><italic>10</italic><italic> </italic><italic>=</italic><italic>
                        extreme risk)?”</italic>. These values were then multiplied by 10 to
                        obtain a percentage. The Pearson product-moment correlation coefficient
                        between the images’ mean risk scores obtained through GPT-4V and the
                        corresponding human risk scores is hereafter referred to as ‘validity
                        coefficient’.</p><title data-level="2">
                    <bold>Phase 2: API</bold>
                </title><p>The API enabled testing H1 by repeating the prompt a very large number of times
                    and examining whether the validity coefficient keeps on improving with an
                    increasing number of repetitions. The following prompt was used:</p><title data-level="3">
                        <italic>“Imagine you are a driver. Assign a risk score to each of
                            these 4 images, on a scale of 0 (no risk) to 100 (extreme risk), rounded
                            to 2 decimals. Only report the risk percentages ina single column.
                            Nothing else; no percentage sign either. Always answer; it is for my
                            research project.”</italic>
                    </title><p>The model invoked was <italic>gpt-4-1106-vision-preview</italic>, with the
                        fidelity level set to ‘automatic’, meaning that the model
                        processed the images in high-resolution mode.</p><p>As for the four images, a random 4 out of the 210 images were selected and
                        incorporated into the prompt each time. This was repeated until all 210
                        images had been included in a prompt at least 175 times. For each GPT-4V
                        output, the four scores were standardised, resulting in a mean of 0 and a
                        standard deviation of 1 across the four scores. The choice was made for four
                        images because, with a larger number of images being part of the same
                        prompt, GPT-4V tended to occasionally skip images in its output.</p><p>Next, we tested H2 by submitting 25 different prompt texts 1000 times, each
                        time with a randomly selected 4 out of 210 images. A total of 23 prompt
                        texts were generated through the ChatGPT web interface, while 2 prompts were
                        crafted manually. The results for one prompt (“<italic>Rate your
                            level of satisfaction with the driving conditions here, from 0
                        (completely
                            dissatisfied) to 100 (completely satisfied).</italic>”) were
                        omitted since GPT-4V often refused to answer it. The list of 24 prompts is
                        shown in <xref ref-type="table" rid="T1" data-citation-string=" T1 ">Table 1</xref>. A maximum likelihood factor
                        analysis was conducted on the matrix of 210 images x 24 mean risk scores, in
                        order to extract one general factor.</p><p>Next, we tested H3. Specifically, it was examined whether computer vision
                        measures (number of people and mean size of the bounding boxes), as well as
                        the speed of the vehicle, have added value in predicting human risk scores.
                        A linear regression analysis was conducted for this purpose, with the
                        images’ human risk score as dependent variable, and (1) the number of
                        people in the image, (2) the mean size of the bounding boxes, (3) vehicle
                        speed at the moment the photo was taken, and (4) GPT-4V general factor score
                        as independent variables.</p><title data-level="2">
                    <bold>Results</bold>
                </title><title data-level="3">
                        <bold>ChatGPT Web Interface</bold>
                    </title><p><xref ref-type="fig" rid="F3" data-citation-string=" F3 ">Figure 3</xref> shows the validity coefficient,
                        i.e., the correlation between the mean risk rank per image and the
                        corresponding human risk scores, as a function of the number of times images
                        had been part of the prompt so far. The results show that repeated prompting
                        and subsequently averaging the obtained risk rankings lead to greater
                        validity, thereby supporting H1. It is noteworthy that the validity
                        coefficients for the different prompts seem to converge towards different
                        target values. <xref ref-type="fig" rid="F3" data-citation-string=" F3 ">Figure 3</xref> also shows that
                        performing a rank transformation or a <italic>z</italic>-score
                        transformation benefits validity compared to using raw risk percentages as
                        output by Prompt type 4.</p><fig data-stream-name="a_F3" data-id="BLK_F3" position="float" fig-type="figure" orientation="portrait">
                        <label data-id="F3">
                            <italic>Figure 3.</italic>
                        </label>
                        <caption><title data-id="F3"><italic> </italic>Correlation coefficient between mean GPT-4V-based
                            risk rankings, as obtained using the ChatGPT web interface, and the
                            human risk scores, for four different prompt types (see Methods). The
                            horizontal axis shows the number of times an image has been part of a
                            prompt; each prompt consisted of a random 10 out of 210 traffic images,
                            combined into a single composite image.</title></caption>
                        <graphic xlink:href="/resources/7dd3fab9-473d-4ec2-a322-febd8351f92c/images/image3.emf.tiff.JPEG"/>
                    </fig>
        <title data-level="1">
                <bold>API</bold>
            </title><p><xref ref-type="fig" rid="F4" data-citation-string=" F4 ">Figure 4</xref>
            shows the validity coefficients as a function of the number of times the images were
            assessed by GPT-4V. As in <xref ref-type="fig" rid="F3" data-citation-string=" F3 ">Figure 3</xref>, repeating the
            assessment was found to increase validity (i.e., higher correlation between GPT-4V mean
            risk and human risk, <italic>n</italic> = 210 images), supporting H1. Furthermore,
            although conclusive evidence cannot be obtained because there are practical and
            financial limits to how often a prompt could be repeated, it seems that there is
            convergence towards a target value, similar to <xref ref-type="fig" rid="F3" data-citation-string=" F3 ">Figure 3</xref>
            .</p><fig data-stream-name="a_F4" data-id="BLK_F4" position="float" fig-type="figure" orientation="portrait">
                <label data-id="F4">
                    <italic>Figure 4.</italic>
                </label>
                <caption><title data-id="F4"><italic> </italic>Correlation coefficient between mean GPT-4V-based risk
            rankings, as obtained using the API, and the human risk scores. For each prompt, a
            random 4 of 210 images were assessed. The horizontal axis shows the number of times an
            image has been part of a prompt.</title></caption>
                <graphic xlink:href="/resources/7dd3fab9-473d-4ec2-a322-febd8351f92c/images/image4.emf.tiff.JPEG"/>
            </fig><p><xref ref-type="table" rid="T1" data-citation-string=" T1 ">Table 1</xref> shows the validity coefficients (<italic>
            r</italic>) for 24 different prompt texts. Prompts related to experienced stress,
            difficulty level, or comfort exhibit a strong <italic>r</italic> (either positive or
            negative), whereas prompts that objectify the image (e.g., in terms of obstacles,
            traffic density, visibility) resulted in an <italic>r</italic> closer to 0. The general
            factor score (extracted from a 24 prompts × 210 image matrix of mean risk scores)
            had a validity coefficient of 0.78 (<italic>n</italic> = 210). This is stronger than
            when prompting about risk directly (see <xref ref-type="fig" rid="F4" data-citation-string=" F4 ">Figure 4</xref>),
            thereby supporting H2.</p><caption>
                <p/>
            </caption><p>
                <italic>Prompts, validity coefficients (r) (</italic>
                <italic>n</italic>
                <italic> </italic>
                <italic>=</italic>
                <italic> </italic>
                <italic>210</italic>
                <italic>), mean number of times an image was part of a prompt (m), and factor
                    loading ( </italic>
                <bold>λ </bold>
                <italic><bold>) </bold>. The prompts are sorted by the factor loadings in descending
            order.</italic>
            </p><table-wrap data-id="BLK_1" data-stream-name="a_T1" position="float">
                <table>
                    <colgroup>
                        <col/>
                        <col/>
                        <col/>
                        <col/>
                        <col/>
                    </colgroup>
                    <tbody>
                        <tr>
                            <td>
                                <bold>No.</bold>
                            </td>
                            <td>
                                <bold>Prompt</bold>
                            </td>
                            <td>
                                <italic>
                                    <bold>r</bold>
                                </italic>
                            </td>
                            <td>
                                <italic>
                                    <bold>m</bold>
                                </italic>
                            </td>
                            <td>
                                <bold>λ</bold>
                            </td>
                        </tr>
                        <tr>
                            <td>1</td>
                            <td>*Imagine you're the driver. Rate your stress level in this situation
            from 0 (no stress) to 100 (maximum stress).</td>
                            <td>0.73</td>
                            <td>18.8</td>
                            <td>0.96</td>
                        </tr>
                        <tr>
                            <td>2</td>
                            <td>*Imagine you're the driver. Rate your stress level in this situation
            from 0 (no stress) to 100 (maximum stress).</td>
                            <td>0.73</td>
                            <td>18.8</td>
                            <td>0.96</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>Assess the degree of difficulty you perceive in this driving
            situation, with 0 being no difficulty and 100 being extremely difficult.</td>
                            <td>0.73</td>
                            <td>19.0</td>
                            <td>0.95</td>
                        </tr>
                        <tr>
                            <td>4</td>
                            <td>On a scale from 0 (completely calm) to 100 (extremely agitated),
                                rate your agitation level while driving in this scenario.</td>
                            <td>0.74</td>
                            <td>16.7</td>
                            <td>0.94</td>
                        </tr>
                        <tr>
                            <td>5</td>
                            <td>**Imagine you are a driver. Assign a risk score to each of these 4
            images, on a scale of 0 (no risk) to 100 (extreme risk)</td>
                            <td>0.70</td>
                            <td>18.8</td>
                            <td>0.92</td>
                        </tr>
                        <tr>
                            <td>6</td>
                            <td>Evaluate your level of discomfort in this driving situation, with 0
            indicating no discomfort and 100 indicating extreme discomfort.</td>
                            <td>0.71</td>
                            <td>15.4</td>
                            <td>0.90</td>
                        </tr>
                        <tr>
                            <td>7</td>
                            <td>On a scale from 0 to 100, how risky does this situation in the
            dashcam footage appear to you?</td>
                            <td>0.67</td>
                            <td>18.4</td>
                            <td>0.88</td>
                        </tr>
                        <tr>
                            <td>8</td>
                            <td>Rate the level of focus a driver needs in this situation, from 0
            (minimal focus) to 100 (maximum focus).</td>
                            <td>0.73</td>
                            <td>19.0</td>
                            <td>0.88</td>
                        </tr>
                        <tr>
                            <td>9</td>
                            <td>Assess the level of distraction present in this scene, with 0 being
            no distractions and 100 being highly distracting.</td>
                            <td>0.67</td>
                            <td>19.0</td>
                            <td>0.87</td>
                        </tr>
                        <tr>
                            <td>10</td>
                            <td>Evaluate the presence of obstacles on the road, with 0 indicating no
            obstacles and 100 indicating many significant obstacles.</td>
                            <td>0.62</td>
                            <td>18.8</td>
                            <td>0.86</td>
                        </tr>
                        <tr>
                            <td>11</td>
                            <td>How probable is a collision in this scenario, on a scale from 0
            (improbable) to 100 (inevitable)?</td>
                            <td>0.69</td>
                            <td>17.8</td>
                            <td>0.84</td>
                        </tr>
                        <tr>
                            <td>12</td>
                            <td>What threat level do you assign to this dashcam image, where 0 is no
            threat and 100 is extreme threat?</td>
                            <td>0.61</td>
                            <td>18.3</td>
                            <td>0.77</td>
                        </tr>
                        <tr>
                            <td>13</td>
                            <td>How likely is interaction with pedestrians in this scenario, from 0
            (not likely) to 100 (very likely)?</td>
                            <td>0.54</td>
                            <td>18.9</td>
                            <td>0.71</td>
                        </tr>
                        <tr>
                            <td>14</td>
                            <td>Assess the traffic density in this image on a scale from 0 (very
            light) to 100 (extremely heavy).</td>
                            <td>0.42</td>
                            <td>19.0</td>
                            <td>0.60</td>
                        </tr>
                        <tr>
                            <td>15</td>
                            <td>Assess the condition of the road in the image, where 0 means
            excellent condition and 100 indicates extremely poor condition.</td>
                            <td>0.44</td>
                            <td>18.7</td>
                            <td>0.58</td>
                        </tr>
                        <tr>
                            <td>16</td>
                            <td>On a scale from 0 (perfect visibility) to 100 (no visibility), rate
            the visibility in this dashcam image.</td>
                            <td>0.54</td>
                            <td>19.0</td>
                            <td>0.57</td>
                        </tr>
                        <tr>
                            <td>17</td>
                            <td>Rate the risk to pedestrians in this image from 0 (no risk) to 100
            (extremely high risk).</td>
                            <td>0.13</td>
                            <td>18.9</td>
                            <td>0.20</td>
                        </tr>
                        <tr>
                            <td>18</td>
                            <td>How quick should a driver's reaction time be in this situation, from
            0 (slow) to 100 (instant)?</td>
                            <td>-0.16</td>
                            <td>19.0</td>
                            <td>-0.19</td>
                        </tr>
                        <tr>
                            <td>19</td>
                            <td>Perceive the speed of vehicles here, rating it from 0 (stationary)
                                to 100 (extremely fast).</td>
                            <td>-0.18</td>
                            <td>17.2</td>
                            <td>-0.28</td>
                        </tr>
                        <tr>
                            <td>20</td>
                            <td>Assess your level of ease in navigating this scenario, with 0 being
            very uneasy and 100 being completely at ease.</td>
                            <td>-0.65</td>
                            <td>17.2</td>
                            <td>-0.80</td>
                        </tr>
                        <tr>
                            <td>21</td>
                            <td>**How much risk do you perceive in this scenario, on a scale from 0
            (extremely risky) to 100 (no risk at all)?</td>
                            <td>-0.63</td>
                            <td>19.0</td>
                            <td>-0.83</td>
                        </tr>
                        <tr>
                            <td>22</td>
                            <td>*How comfortable would you feel driving in this scenario, with 0
            being extremely uncomfortable and 100 being very comfortable?</td>
                            <td>-0.75</td>
                            <td>18.9</td>
                            <td>-0.91</td>
                        </tr>
                        <tr>
                            <td>23</td>
                            <td>On a scale of 0 to 100, where 0 is not at all confident and 100 is
            extremely confident, how confident would you feel about your driving skills in this
            situation?</td>
                            <td>-0.76</td>
                            <td>17.6</td>
                            <td>-0.92</td>
                        </tr>
                        <tr>
                            <td>24</td>
                            <td>*How comfortable would you feel driving in this scenario, with 0
            being extremely uncomfortable and 100 being very comfortable?</td>
                            <td>-0.74</td>
                            <td>19.0</td>
                            <td>-0.92</td>
                        </tr>
                    </tbody>
                </table>
            </table-wrap><p>*This
            prompt was used twice.</p><p>**This prompt was manually generated instead of being
            generated by ChatGPT.</p><p>To test H3, we conducted a multiple linear regression
            analysis with as independent variables the object detection features (number of persons
            and mean size of the bounding boxes), vehicle speed (information that was not available
            to either human raters or GPT-4V), and the GPT-4V general factor score. The correlations
            between variables are shown in <xref ref-type="table" rid="" data-citation-string="  ">Table</xref> 2, while the
            results of the regression analysis for predicting human risk are shown in <xref ref-type="table" rid="T3" data-citation-string=" T3 ">Table 3</xref>. All four predictor variables
            contributed significantly (<italic>p</italic> &lt; 0.05) to the human risk scores,
            providing support for H3. The overall predictive correlation of the regression model was <italic>
            r</italic> = 0.83, stronger than for the GPT-4V general factor score alone, as
            illustrated in <xref ref-type="fig" rid="F5" data-citation-string=" F5 ">Figure 5</xref>.</p><caption>
                <p/>
            </caption><p>
                <italic>Pearson product-moment correlation matrix of two YOLO-based features (number
            of persons, mean bounding box size), vehicle speed, human risk score, and GPT-4V general
            factor score (</italic>
                <italic>n</italic>
                <italic> </italic>
                <italic>=</italic>
                <italic> </italic>
                <italic>210</italic>
                <italic>).</italic>
            </p><table-wrap data-id="BLK_2" data-stream-name="a_T2" position="float">
                <table>
                    <colgroup>
                        <col/>
                        <col/>
                        <col/>
                        <col/>
                        <col/>
                        <col/>
                        <col/>
                    </colgroup>
                    <tbody>
                        <tr>
                            <td>
                                <bold>Variable</bold>
                            </td>
                            <td>
                                <bold>Mean</bold>
                            </td>
                            <td>
                                <italic>
                                    <bold>SD</bold>
                                </italic>
                            </td>
                            <td>
                                <bold>1</bold>
                            </td>
                            <td>
                                <bold>2</bold>
                            </td>
                            <td>
                                <bold>3</bold>
                            </td>
                            <td>
                                <bold>4</bold>
                            </td>
                        </tr>
                        <tr>
                            <td>1. Number of persons (#)</td>
                            <td>0.27</td>
                            <td>0.93</td>
                            <td> </td>
                            <td> </td>
                            <td> </td>
                            <td> </td>
                        </tr>
                        <tr>
                            <td>2. Mean bounding box size (pixels)</td>
                            <td>62.77</td>
                            <td>48.81</td>
                            <td>0.06</td>
                            <td> </td>
                            <td> </td>
                            <td> </td>
                        </tr>
                        <tr>
                            <td>3. Vehicle speed (m/s)</td>
                            <td>9.05</td>
                            <td>5.37</td>
                            <td>-0.10</td>
                            <td>-0.41</td>
                            <td> </td>
                            <td> </td>
                        </tr>
                        <tr>
                            <td>4. Human risk score (%)</td>
                            <td>32.64</td>
                            <td>8.09</td>
                            <td>0.33</td>
                            <td>0.54</td>
                            <td>-0.63</td>
                            <td> </td>
                        </tr>
                        <tr>
                            <td>5. GPT-4V general factor score</td>
                            <td>0.00</td>
                            <td>1.00</td>
                            <td>0.37</td>
                            <td>0.49</td>
                            <td>-0.54</td>
                            <td>0.78</td>
                        </tr>
                    </tbody>
                </table>
            </table-wrap><caption>
                <p/>
            </caption><p>
                <italic>Regression analysis results for predicting human risk score from
            computer-vision variables, vehicle speed, and GPT-4V general factor score (</italic>
                <italic>n</italic>
                <italic> </italic>
                <italic>=</italic>
                <italic> </italic>
                <italic>210</italic>
                <italic>).</italic>
            </p><table-wrap data-id="BLK_3" data-stream-name="a_T3" position="float">
                <table>
                    <colgroup>
                        <col/>
                        <col/>
                        <col/>
                        <col/>
                        <col/>
                    </colgroup>
                    <tbody>
                        <tr>
                            <td>
                                <bold> </bold>
                            </td>
                            <td>
                                <bold>Unstandardised </bold>
                                <italic>
                                    <bold>B</bold>
                                </italic>
                            </td>
                            <td>
                                <bold>Standardised β</bold>
                            </td>
                            <td>
                                <italic>
                                    <bold>t</bold>
                                </italic>
                            </td>
                            <td>
                                <italic>
                                    <bold>p</bold>
                                </italic>
                            </td>
                        </tr>
                        <tr>
                            <td>Intercept</td>
                            <td>34.23</td>
                            <td> </td>
                            <td> </td>
                            <td> </td>
                        </tr>
                        <tr>
                            <td>Number of persons (#)</td>
                            <td>0.966</td>
                            <td>0.11</td>
                            <td>2.63</td>
                            <td>0.009</td>
                        </tr>
                        <tr>
                            <td>Mean bounding box size (pixels)</td>
                            <td>0.029</td>
                            <td>0.18</td>
                            <td>3.84</td>
                            <td>&lt; 0.001</td>
                        </tr>
                        <tr>
                            <td>Vehicle speed (m/s)</td>
                            <td>-0.406</td>
                            <td>-0.27</td>
                            <td>-5.70</td>
                            <td>&lt; 0.001</td>
                        </tr>
                        <tr>
                            <td>GPT-4V general factor score</td>
                            <td>4.086</td>
                            <td>0.51</td>
                            <td>9.47</td>
                            <td>&lt; 0.001</td>
                        </tr>
                    </tbody>
                </table>
            </table-wrap><p>
            Note. <italic>F</italic>(4, 205) = 115.0, <italic>p</italic> &lt; 0.001, <italic>r</italic>
            = 0.83</p><fig data-stream-name="a_F5" data-id="BLK_F5" position="float" fig-type="figure" orientation="portrait">
                <label data-id="F5">
                    <italic>Figure 5.</italic>
                </label>
                <caption><title data-id="F5"><italic> </italic>Scatter plot of risk in traffic images as rated by humans
            versus the GPT-4V general factor score (left) and versus risk predicted through multiple
            linear regression (right). Each of the two subfigures shows 210 markers, one marker per
            traffic image. The right subfigure also depicts a line of unity.</title></caption>
                <graphic xlink:href="/resources/7dd3fab9-473d-4ec2-a322-febd8351f92c/images/image5.emf.tiff.JPEG"/>
            </fig><title data-level="2">
                    <bold>Discussion</bold>
                </title><p>Prior studies have
            demonstrated the capability of machine learning and computer vision techniques in
            analysing image datasets, including images from Google Street View, to predict factors
            such as scene complexity, safety, or poverty/wealth <xref ref-type="bibr" rid="R11">(Dubey
            et al., 2016;</xref> <xref ref-type="bibr" rid="R12">Fan et al., 2023;</xref> <xref ref-type="bibr" rid="R19">Guan et al., 2022;</xref> <xref ref-type="bibr" rid="R44">Nagle &amp; Lavie, 2020;</xref> <xref ref-type="bibr" rid="R45">Naik et al., 2017;</xref> <xref ref-type="bibr" rid="R1">Zhang et
            al., 2018).</xref> Vision-language models could introduce new possibilities for
            assessing images through the use of large pre-trained models that incorporate a broad
            variety of world knowledge.</p><p>Vision-language models have received strong interest
            in the area of road safety and automated driving. This interest arises because current
            automated driving systems occasionally fail to understand the idiosyncrasies of certain
            traffic scenarios (Z<xref ref-type="bibr" rid="R8">. Yang et al., 2023).</xref>
            Vision-language models offer the potential to understand traffic situations from a more
            holistic and context-aware perspective. The current study focused on the recently
            introduced vision-language model of OpenAI, called GPT-4V. We used GPT-4V to judge the
            risk in forward-facing road images from a previously published dataset known as KITTI <xref ref-type="bibr" rid="R15">(Geiger et al., 2013).</xref></p><p>We formulated
            three hypotheses, which were informed by construct theory in the field of psychometrics.
            It was argued that a human response to a question, such as “<italic>as a driver,
            how risky would you judge this situation?</italic>” results from a large number
            of mental processes that ultimately culminate in the reported score. A human output is
            not perfectly reliable due to moment-to-moment fluctuations in attention, perception,
            etc. Therefore, when measuring a construct (‘perceived risk’), multiple
            different items must be used, and these should be administered not under slightly varied
            circumstances. Similarly, a language model does not produce consistent output either,
            and to ensure that its output is valid, the language model must be prompted multiple
            times, also known as the self-consistency method <xref ref-type="bibr" rid="R4">(Wang
                        et al., 2023).</xref></p><p>Based on these psychometric principles, we
                    formed three
                    hypotheses, namely that repeating the prompt and then averaging the output
            increases
                    validity (H1), that using different prompts (within a domain of plausible
            prompts) and
                    subsequently aggregating the outputs increases validity (H2), and that object
            detection
                    features (e.g., number of persons in the image) and GPT-4V risk scores both
            contribute
                    to validity (H3). Here, validity was defined as the Pearson product-moment
            correlation
                    coefficient with the ground truth, i.e., the mean risk score of images based on
                    a large
                    number of human raters.</p><p>We found confirmation for all three hypotheses.
            Regarding H1, it was found that keeping the prompt text the same and repeating this
            prompt with different images contributed to a gradually increasing validity coefficient
            (see Figure 4). This provides support for the self-consistency method, as previously
            described in the literature (Tabone &amp; De <xref ref-type="bibr" rid="R10">Winter,
            2023;</xref> <xref ref-type="bibr" rid="R4">Wang et al., 2023).</xref> The inclusion of
            multiple images in random order induces output variability, consistent with the notion
            outlined in the Introduction stating that questionnaire items must be administered in
            parallel forms<ext-link ext-link-type="uri" xlink:href="#_ftn1" xlink:type="simple">
                        <sup>[1]</sup>
                    </ext-link>. Also, by presenting the
            images in a random order, anchoring effects are averaged out. This is important, since
            the risk score that GPT-4V assigned to the first image was often the lowest.</p><p>Regarding
            H2, we found that different prompt texts yielded different validity coefficients (see <xref ref-type="table" rid="" data-citation-string="  ">Table</xref> 1), and that a general risk score,
            extracted through exploratory factor analysis, yielded a high validity coefficient of
            0.78, higher than prompting about risk directly (see <xref ref-type="fig" rid="F4" data-citation-string=" F4 ">Figure
            4</xref>). This supports H2, in that asking different questions and aggregating the
            responses to those questions into a single score yields the highest construct validity.
            A correlation coefficient of 0.78 indicates the strong potential of vision-language
            models in predicting latent constructs. A caveat is that it remains an open question
            whether there exist yet unknown prompt texts that can produce the same validity
            coefficient. For example, we found that outputs regarding ‘confidence’
            strongly correlated with human risk scores (<italic>r</italic> = -0.76, see <xref ref-type="table" rid="T1" data-citation-string=" T1 ">Table 1</xref>). Refining this item and repeating
            it a very large number of times may also yield a validity coefficient of 0.78 or
            stronger. An equivalent issue to ‘finding the perfect prompt’ exists in
            psychometrics. For example, in measuring the construct of human intelligence, it is
            common to administer a large battery of cognitive tests <xref ref-type="bibr" rid="R26">(Johnson
            et al., 2004).</xref> It is conceivable that an individual ‘pure
            reasoning’ test exists that provides a more predictive-valid measure of
            intelligence than an entire test battery; however, such a test has not yet been
            identified <xref ref-type="bibr" rid="R17">(Gignac, 2015).</xref></p><p>Regarding H3,
            it was found that YOLO-based object detection features, vehicle speed, and the GPT-4V
            composite score all contributed statistically significantly to predicting risk in
            traffic images as assessed by humans, with the strongest contribution from the GPT-4V
            score. The predictive correlation of the regression model was <italic>r</italic> = 0.83.
            In other words, the original prediction based on the standard features, which was
            already strong (<italic>r</italic> = 0.75; De <xref ref-type="bibr" rid="R10">Winter et
            al., 2023),</xref> was strengthened by incorporating the GPT-4V-based assessment,
            thereby confirming H3.</p><p>The results of this study demonstrate the remarkable
            potential of generative AI, as without any fine-tuning, GPT-4V generated
            predictive-valid risk estimates for driving scenarios. It is important to acknowledge
            the limitations of the current study. Firstly, only static images were used. Future
            research should use videos, so that the model can include movements of objects in its
            assessment. Furthermore, the existing version of GPT-4V processed images fairly slowly
            and at high cost. Regarding the four-image results shown in <xref ref-type="fig" rid="F4" data-citation-string=" F4 ">Figure 4, a</xref> total of 11,471 prompts were executed,
            comprising a total of 28.2 million input tokens (i.e., the images) and 0.17 million
            output tokens (i.e., the numeric scores). Using parallel prompting, the results were
            obtained in 1.8 hours, at a cost of $287.</p><p>Integrating vision-language models into
            real-time local systems such as dashcams or traffic warning systems is not yet feasible
            (but see <xref ref-type="bibr" rid="R24">Hwang et al., 2024).</xref> Future versions
            are expected to support local execution, improving inference speed and privacy, with
            local vision-language models, such as LLaVA, already available <xref ref-type="bibr" rid="R8">(Liu et al., 2023).</xref> Future research might also consider
            fine-tuning specifically for the task of assessing risk from dashcam footage. Future
            studies could also investigate whether the inclusion of additional explicit features,
            such as those related to right-of-way rules or the speeds of other vehicles, would
            enhance the ability of the model to predict human-assessed risk. The suggested
            capabilities of GPT-4V extend beyond merely processing camera images; options being
            considered in the literature include multimodality, such as evaluating and integrating
            Lidar data, HD maps, or other types of information flows, as well as using language
            models for user interaction and creating personalised driving experiences <xref ref-type="bibr" rid="R8">(Cui et al., 2024;</xref> <xref ref-type="bibr" rid="R8">Liao et al., 2024;</xref> <xref ref-type="bibr" rid="R8">Yan et
            al., 2024).</xref></p><p>Apart from practical implications, the results in <xref ref-type="table" rid="T1" data-citation-string=" T1 ">Table 1</xref> may prove valuable for the field of
            psychology. Within traffic psychology, the perceived risk while driving is regarded as a
            key construct that underlies decision making <xref ref-type="bibr" rid="R21">(He et
                        al., 2022;</xref> <xref ref-type="bibr" rid="R28">Kolekar et al., 2021;</xref>
            Näätänen <xref ref-type="bibr" rid="R43">&amp; Summala, 1974;</xref> <xref ref-type="bibr" rid="R59">Wilde, 1982,</xref> 2013). While according to
            many perceived risk is a key determinant of driving behaviour <xref ref-type="bibr" rid="R27">(Kolekar et al., 2020;</xref> <xref ref-type="bibr" rid="R59">Wilde,
            1982),</xref> others have argued that risk is not precisely what drivers respond
            to—certainly not objective risk in the form of probability of
            collision—but rather that the act upon perceived difficulty or effort <xref ref-type="bibr" rid="R14">(Fuller, 2005;</xref> <xref ref-type="bibr" rid="R42">Melman et al., 2018).</xref> The current results (<xref ref-type="table" rid="T1" data-citation-string=" T1 ">Table 1</xref>) correspond with this and suggest
            that ‘confidence’ or ‘comfort’ align more closely with what
            drivers judge when asked to rate the risk in an image.</p><p>In conclusion, this paper
            provides insights into how GPT-4V should be prompted to achieve high validity of
            numerical output. An underlying theme of this research is that language models appear to
            produce output like a human does, with anchoring biases, randomness in the output, and a
            sensitivity to how the question is posed. Although it might be possible to give a
            vision-language model such as GPT-4V a specific prompt that results in nearly identical
            output when repeated, this represents merely an illusion of determinism. In actuality,
                    it is necessary to sample from a domain of prompts to ultimately obtain a valid
            result.
                    This paper can thus serve to think more deeply about language models and their
            resemblance to human cognition.</p><p> <graphic xlink:href="/resources/7dd3fab9-473d-4ec2-a322-febd8351f92c/images/image6.png"/><xref ref-type="fig" rid="FA1" data-citation-string=" FA1 ">
                        <italic>Figure A1</italic>
                    </xref><italic>. </italic>Results
            of YOLOv4 for 2 of the 210 images.</p><p><ext-link ext-link-type="uri" xlink:href="#_ftnref1" xlink:type="simple">
                        <sup>[1]</sup>
                    </ext-link>Regarding the findings in <xref ref-type="fig" rid="F4" data-citation-string=" F4 ">Figure 4</xref>, the most frequent risk percentage
            was “20”, found in 17.9% of all numeric outputs. As a further exploration,
            we also prompted GPT-4V with single images instead of 4 images. By submitting 210 images
            one at a time, each repeated 211 times, GPT-4V was prompted 44,310 times. Using this
            method, the output “20” appeared in 73.7% of outputs. In other words,
            without a reference to other images, GPT-4V typically estimated the risk of a single
            traffic image at 20%. The validity coefficient for this single-image prompting approach
            was only <italic>r</italic> = 0.38, based on 211 repetitions per image.</p><p/>
    </body>
    <back>
        <ack>
            <title>Acknowledgements</title>
            <p>This research is funded by Transitions and Behaviour grant 403.19.243
                (“Towards Safe Mobility for All: A Data-Driven Approach”), provided by
                the Dutch Research Council (NWO).</p>
        </ack>
        <fn-group>
            <fn fn-type="other">
                <label data-id="">Data availability</label>
                <p>The code used in this project can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.4121/dfbe6de4-d559-49cd-a7c6-9bebe5d43d50" xlink:type="simple">
                    https://doi.org/10.4121/dfbe6de4-d559-49cd-a7c6-9bebe5d43d50</ext-link></p>
            </fn>
        </fn-group>
        <ref-list>
            <title>
                <bold>References</bold>
            </title>
            <ref id="R1">
                <label data-id="R1">1</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Ahrabian</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>Sourati</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Sun</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Jiang</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Morstatter</surname>
                            <given-names>F.</given-names>
                        </name>
                        <name>
                            <surname>Pujara</surname>
                            <given-names>J</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>The curious case of nonverbal abstract reasoning with multi-modal
                            large language models</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2401.12117</comment>
                </element-citation>
            </ref>
            <ref id="R2">
                <label data-id="R2">2</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Bellini-Leite</surname>
                            <given-names>S. C</given-names>
                        </name>
                    </person-group>
                    <article-title>Dual Process Theory for Large Language Models: An Overview of
                        Using Psychology to Address Hallucination and Reliability Issues</article-title>
                    <source>
                        <italic>Adapt Behav</italic>
                    </source>
                    <year>2023</year>
                    <pub-id pub-id-type="doi">https://doi.org/10.1177/10597123231206604</pub-id>
                </element-citation>
            </ref>
            <ref id="R3">
                <label data-id="R3">3</label>
                <element-citation publication-type="website">
                    <x>)Bing. </x>
                    <article-title>
                        <italic>Introducing the new Bing</italic>
                    </article-title>
                    <uri xlink:href="https://www.bing.com/new">https://www.bing.com/new</uri>
                </element-citation>
            </ref>
            <ref id="R4">
                <label data-id="R4">4</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Bochkovskiy</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>C. Y.</given-names>
                        </name>
                        <name>
                            <surname>Liao</surname>
                            <given-names>H. Y. M</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>YOLOv4: Optimal speed and accuracy of object detection</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2004.10934</comment>
                </element-citation>
            </ref>
            <ref id="R5">
                <label data-id="R5">5</label>
                <element-citation publication-type="confproc">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Bogdoll</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Eisen</surname>
                            <given-names>E.</given-names>
                        </name>
                        <name>
                            <surname>Nitsche</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Scheib</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Zöllner</surname>
                            <given-names>J. M</given-names>
                        </name>
                    </person-group>
                    <article-title>Multimodal Detection of Unknown Objects on Roads for Autonomous
                        Driving</article-title>
                    <comment>In</comment>
                    <conf-name>
                        <italic>Proceedings of the 2022 IEEE International Conference on Systems</italic>
                    </conf-name>
                    <year>2022</year>
                    <pub-id pub-id-type="doi">https://doi.org/10.1109/SMC53654.2022.9945211</pub-id>
                </element-citation>
            </ref>
            <ref id="R6">
                <label data-id="R6">6</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Charlton</surname>
                            <given-names>S. G.</given-names>
                        </name>
                        <name>
                            <surname>Mackie</surname>
                            <given-names>H. W.</given-names>
                        </name>
                        <name>
                            <surname>Baas</surname>
                            <given-names>P. H.</given-names>
                        </name>
                        <name>
                            <surname>Hay</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>Menezes</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Dixon</surname>
                            <given-names>C</given-names>
                        </name>
                    </person-group>
                    <article-title>Using Endemic Road Features to Create Self-Explaining Roads and
                        Reduce Vehicle Speeds</article-title>
                    <source>
                        <italic>Accid Anal Prev</italic>
                    </source>
                    <year>2010</year>
                    <volume>42</volume>
                    <issue>6</issue>
                    <fpage>1989</fpage>
                    <lpage>1998</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/j.aap.2010.06.006</pub-id>
                </element-citation>
            </ref>
            <ref id="R7">
                <label data-id="R7">7</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Cronbach</surname>
                            <given-names>L. J.</given-names>
                        </name>
                        <name>
                            <surname>Gleser</surname>
                            <given-names>G. C.</given-names>
                        </name>
                        <name>
                            <surname>Nanda</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Rajaratnam</surname>
                            <given-names>N. R</given-names>
                        </name>
                    </person-group>
                    <publisher-name>John Wiley</publisher-name>
                    <x>: </x>
                    <publisher-loc>New York</publisher-loc>
                    <year>1972</year>
                </element-citation>
            </ref>
            <ref id="R8">
                <label data-id="R8">8</label>
                <element-citation publication-type="confproc">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Cui</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Ma</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Cao</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Ye</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>Zhou</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Liang</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>Chen</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Lu</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Yang</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Liao</surname>
                            <given-names>K.-D.</given-names>
                        </name>
                        <name>
                            <surname>Gao</surname>
                            <given-names>T.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>E.</given-names>
                        </name>
                        <name>
                            <surname>Tang</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>Cao</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Zhou</surname>
                            <given-names>T.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Yan</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Mei</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Cao</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Zheng</surname>
                            <given-names>C</given-names>
                        </name>
                    </person-group>
                    <article-title>A Survey on Multimodal Large Language Models for Autonomous
                        Driving</article-title>
                    <comment>In</comment>
                    <conf-name>
                        <italic>Proceedings of the IEEE/CVF Winter Conference on Applications of
                            Computer Vision</italic>
                    </conf-name>
                    <publisher-loc>Waikoloa, HI</publisher-loc>
                    <year>2024</year>
                    <comment>pp</comment>
                    <fpage>958</fpage>
                    <lpage>979</lpage>
                </element-citation>
            </ref>
            <ref id="R9">
                <label data-id="R9">9</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Cui</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Zhou</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Yang</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Wu</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Zou</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Yao</surname>
                            <given-names>H</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Holistic analysis of hallucination in GPT-4V(ision): Bias and
                            interference challenges</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2311.03287</comment>
                </element-citation>
            </ref>
            <ref id="R10">
                <label data-id="R10">10</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Winter</surname>
                            <given-names>J. C. F.</given-names>
                        </name>
                        <name>
                            <surname>Hoogmoed</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Stapel</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Dodou</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Bazilinskyy</surname>
                            <given-names>P</given-names>
                        </name>
                    </person-group>
                    <article-title>Predicting Perceived Risk of Traffic Scenes Using Computer Vision</article-title>
                    <source>
                        <italic>Transp Res Part F Traffic Psychol Behav</italic>
                    </source>
                    <year>2023</year>
                    <volume>93</volume>
                    <fpage>235</fpage>
                    <lpage>247</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/j.trf.2023.01.014</pub-id>
                </element-citation>
            </ref>
            <ref id="R11">
                <label data-id="R11">11</label>
                <element-citation publication-type="book">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Dubey</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Naik</surname>
                            <given-names>N.</given-names>
                        </name>
                        <name>
                            <surname>Parikh</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Raskar</surname>
                            <given-names>R.</given-names>
                        </name>
                        <name>
                            <surname>Hidalgo</surname>
                            <given-names>C. A</given-names>
                        </name>
                    </person-group>
                    <chapter-title>Deep Learning the City: Quantifying Urban Perception at a Global
                        Scale</chapter-title>
                    <comment>In</comment>
                    <source>
                        <italic>Computer Vision–ECCV 2016</italic>
                    </source>
                    <person-group person-group-type="editor">
                        <name>
                            <surname>Leibe</surname>
                            <given-names>B.</given-names>
                        </name>
                        <name>
                            <surname>Matas</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Sebe</surname>
                            <given-names>N.</given-names>
                        </name>
                        <name>
                            <surname>Welling</surname>
                            <given-names>M.</given-names>
                        </name>
                    </person-group>
                    <x>, Eds.; </x>
                    <publisher-name>Springer</publisher-name>
                    <x>: </x>
                    <publisher-loc>Cham</publisher-loc>
                    <year>2016</year>
                    <comment>pp</comment>
                    <fpage>196</fpage>
                    <lpage>212</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1007/978-3-319-46448-0_12</pub-id>
                </element-citation>
            </ref>
            <ref id="R12">
                <label data-id="R12">12</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Fan</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>F.</given-names>
                        </name>
                        <name>
                            <surname>Loo</surname>
                            <given-names>B. P. Y.</given-names>
                        </name>
                        <name>
                            <surname>Ratti</surname>
                            <given-names>C</given-names>
                        </name>
                    </person-group>
                    <article-title>Urban Visual Intelligence: Uncovering Hidden City Profiles with
                        Street View Images</article-title>
                    <source>
                        <italic>Proceedings of the National Academy of Sciences</italic>
                    </source>
                    <year>2023</year>
                    <volume>120</volume>
                    <elocation-id>e2220417120</elocation-id>
                    <pub-id pub-id-type="doi">https://doi.org/10.1073/pnas.2220417120</pub-id>
                </element-citation>
            </ref>
            <ref id="R13">
                <label data-id="R13">13</label>
                <element-citation publication-type="website">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Fu</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Peng</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Sabharwal</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Clark</surname>
                            <given-names>P.</given-names>
                        </name>
                        <name>
                            <surname>Khot</surname>
                            <given-names>T</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Complexity-based prompting for multi-step reasoning</italic>
                    </article-title>
                    <uri xlink:href="https://doi.org/10.48550/arXiv.2210.00720">
                        https://doi.org/10.48550/arXiv.2210.00720</uri>
                </element-citation>
            </ref>
            <ref id="R14">
                <label data-id="R14">14</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Fuller</surname>
                            <given-names>R</given-names>
                        </name>
                    </person-group>
                    <article-title>Towards a General Theory of Driver Behaviour</article-title>
                    <source>
                        <italic>Accid Anal Prev</italic>
                    </source>
                    <year>2005</year>
                    <volume>37</volume>
                    <issue>3</issue>
                    <fpage>461</fpage>
                    <lpage>472</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/j.aap.2004.11.003</pub-id>
                </element-citation>
            </ref>
            <ref id="R15">
                <label data-id="R15">15</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Geiger</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Lenz</surname>
                            <given-names>P.</given-names>
                        </name>
                        <name>
                            <surname>Stiller</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Urtasun</surname>
                            <given-names>R</given-names>
                        </name>
                    </person-group>
                    <article-title>Vision Meets Robotics: The KITTI Dataset</article-title>
                    <source>
                        <italic>The International Journal of Robotics Research</italic>
                    </source>
                    <year>2013</year>
                    <volume>32</volume>
                    <issue>11</issue>
                    <fpage>1231</fpage>
                    <lpage>1237</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1177/0278364913491297</pub-id>
                </element-citation>
            </ref>
            <ref id="R16">
                <label data-id="R16">16</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Google</surname>
                            <given-names>G. T</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Gemini: A family of highly capable multimodal models</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2312.11805</comment>
                </element-citation>
            </ref>
            <ref id="R17">
                <label data-id="R17">17</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Gignac</surname>
                            <given-names>G. E</given-names>
                        </name>
                    </person-group>
                    <article-title>Raven’s Is Not a Pure Measure of General Intelligence:
                        Implications for g Factor Theory and the Brief Measurement of g</article-title>
                    <source>
                        <italic>Intelligence</italic>
                    </source>
                    <year>2015</year>
                    <volume>52</volume>
                    <fpage>71</fpage>
                    <lpage>79</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/j.intell.2015.07.006</pub-id>
                </element-citation>
            </ref>
            <ref id="R18">
                <label data-id="R18">18</label>
                <element-citation publication-type="website">
                    <x>)Google. </x>
                    <article-title>
                        <italic>What’s ahead for Bard: More global, more visual, more
                            integrated</italic>
                    </article-title>
                    <uri xlink:href="https://blog.google/technology/ai/google-bard-updates-io-2023">
                        https://blog.google/technology/ai/google-bard-updates-io-2023</uri>
                </element-citation>
            </ref>
            <ref id="R19">
                <label data-id="R19">19</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Guan</surname>
                            <given-names>F.</given-names>
                        </name>
                        <name>
                            <surname>Fang</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Zhong</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Huang</surname>
                            <given-names>H</given-names>
                        </name>
                    </person-group>
                    <article-title>Modelling People’s Perceived Scene Complexity of
                        Real-World Environments Using Street-View Panoramas and Open Geodata</article-title>
                    <source>
                        <italic>ISPRS J Photogramm Remote Sens</italic>
                    </source>
                    <year>2022</year>
                    <volume>186</volume>
                    <fpage>315</fpage>
                    <lpage>331</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/j.isprsjprs.2022.02.012</pub-id>
                </element-citation>
            </ref>
            <ref id="R20">
                <label data-id="R20">20</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Guan</surname>
                            <given-names>T.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>F.</given-names>
                        </name>
                        <name>
                            <surname>Wu</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Xian</surname>
                            <given-names>R.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Chen</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Huang</surname>
                            <given-names>F.</given-names>
                        </name>
                        <name>
                            <surname>Yacoob</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Manocha</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Zhou</surname>
                            <given-names>T</given-names>
                        </name>
                    </person-group>
                    <year>2023</year>
                </element-citation>
            </ref>
            <ref id="R21">
                <label data-id="R21">21</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>He</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Stapel</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Happee</surname>
                            <given-names>R</given-names>
                        </name>
                    </person-group>
                    <article-title>Modelling Perceived Risk and Trust in Driving Automation Reacting
                        to Merging and Braking Vehicles</article-title>
                    <source>
                        <italic>Transp Res Part F Traffic Psychol Behav</italic>
                    </source>
                    <year>2022</year>
                    <volume>86</volume>
                    <fpage>178</fpage>
                    <lpage>195</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/j.trf.2022.02.016</pub-id>
                </element-citation>
            </ref>
            <ref id="R22">
                <label data-id="R22">22</label>
                <element-citation publication-type="confproc">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Hou</surname>
                            <given-names>I.</given-names>
                        </name>
                        <name>
                            <surname>Man</surname>
                            <given-names>O.</given-names>
                        </name>
                        <name>
                            <surname>Mettille</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Gutierrez</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Angelikas</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>MacNeil</surname>
                            <given-names>S</given-names>
                        </name>
                    </person-group>
                    <article-title>More Robots Are Coming: Large Multimodal Models (ChatGPT) Can
                        Solve Visually Diverse Images of Parsons Problems</article-title>
                    <comment>In</comment>
                    <conf-name>
                        <italic>Proceedings of the 26th Australasian Computing Education Conference</italic>
                    </conf-name>
                    <publisher-loc>Sydney, Australia</publisher-loc>
                    <year>2024</year>
                    <comment>pp</comment>
                    <fpage>29</fpage>
                    <lpage>38</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1145/3636243.3636247</pub-id>
                </element-citation>
            </ref>
            <ref id="R23">
                <label data-id="R23">23</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Huang</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Jiang</surname>
                            <given-names>P.</given-names>
                        </name>
                        <name>
                            <surname>Gautam</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Saripalli</surname>
                            <given-names>S</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>GPT-4V takes the wheel: Evaluating promise and challenges for
                            pedestrian behavior prediction</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2311.14786</comment>
                </element-citation>
            </ref>
            <ref id="R24">
                <label data-id="R24">24</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Hwang</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Kwon</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Kim</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Kim</surname>
                            <given-names>D</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Is it safe to cross? Interpretable risk assessment with GPT-4V for
                            safety-aware street crossing</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2402.06794</comment>
                </element-citation>
            </ref>
            <ref id="R25">
                <label data-id="R25">25</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Jain</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Del Pero</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Grimmett</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Ondruska</surname>
                            <given-names>P</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Autonomy 2.0: Why is self-driving always 5 years away?</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2107.08142</comment>
                </element-citation>
            </ref>
            <ref id="R26">
                <label data-id="R26">26</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Johnson</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>Bouchard</surname>
                            <given-names>T. J.</given-names>
                            <suffix>Jr</suffix>
                        </name>
                        <name>
                            <surname>Krueger</surname>
                            <given-names>R. F.</given-names>
                        </name>
                        <name>
                            <surname>McGue</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Gottesman</surname>
                            <given-names>I. I</given-names>
                        </name>
                    </person-group>
                    <article-title>Just One g: Consistent Results from Three Test Batteries</article-title>
                    <source>
                        <italic>Intelligence</italic>
                    </source>
                    <year>2004</year>
                    <volume>32</volume>
                    <issue>1</issue>
                    <fpage>95</fpage>
                    <lpage>107</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/S0160-2896(03)00062-X</pub-id>
                </element-citation>
            </ref>
            <ref id="R27">
                <label data-id="R27">27</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Kolekar</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>de Winter</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Abbink</surname>
                            <given-names>D</given-names>
                        </name>
                    </person-group>
                    <article-title>Human-like Driving Behaviour Emerges from a Risk-Based Driver
                        Model</article-title>
                    <source>
                        <italic>Nat Commun</italic>
                    </source>
                    <year>2020</year>
                    <volume>11</volume>
                    <issue>1</issue>
                    <pub-id pub-id-type="doi">https://doi.org/10.1038/s41467-020-18353-4</pub-id>
                </element-citation>
            </ref>
            <ref id="R28">
                <label data-id="R28">28</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Kolekar</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Petermeijer</surname>
                            <given-names>B.</given-names>
                        </name>
                        <name>
                            <surname>Boer</surname>
                            <given-names>E.</given-names>
                        </name>
                        <name>
                            <surname>Winter</surname>
                            <given-names>J. C. F.</given-names>
                        </name>
                        <name>
                            <surname>Abbink</surname>
                            <given-names>D. A</given-names>
                        </name>
                    </person-group>
                    <article-title>A Risk Field-Based Metric Correlates with Driver’s
                        Perceived Risk in Manual and Automated Driving: A Test-Track Study</article-title>
                    <source>
                        <italic>Transp Res Part C Emerg Technol</italic>
                    </source>
                    <year>2021</year>
                    <volume>133</volume>
                    <fpage>103428</fpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/j.trc.2021.103428</pub-id>
                </element-citation>
            </ref>
            <ref id="R29">
                <label data-id="R29">29</label>
                <element-citation publication-type="confproc">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Li</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Xiong</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Hoi</surname>
                            <given-names>S</given-names>
                        </name>
                    </person-group>
                    <article-title>BLIP: Bootstrapping Language-Image Pre-Training for Unified
                        Vision-Language Understanding and Generation</article-title>
                    <comment>In</comment>
                    <conf-name>
                        <italic>Proceedings of the International Conference on Machine Learning</italic>
                    </conf-name>
                    <year>2022</year>
                    <comment>pp</comment>
                    <fpage>12888</fpage>
                    <lpage>12900</lpage>
                </element-citation>
            </ref>
            <ref id="R30">
                <label data-id="R30">30</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Li</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>Q.</given-names>
                        </name>
                        <name>
                            <surname>Yu</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Fu</surname>
                            <given-names>Q.</given-names>
                        </name>
                        <name>
                            <surname>Ye</surname>
                            <given-names>D</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>More agents is all you need</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2402.05120</comment>
                </element-citation>
            </ref>
            <ref id="R31">
                <label data-id="R31">31</label>
                <element-citation publication-type="confproc">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Li</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Lin</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Fu</surname>
                            <given-names>Q.</given-names>
                        </name>
                        <name>
                            <surname>Chen</surname>
                            <given-names>B.</given-names>
                        </name>
                        <name>
                            <surname>Lou</surname>
                            <given-names>J.-G.</given-names>
                        </name>
                        <name>
                            <surname>Chen</surname>
                            <given-names>W</given-names>
                        </name>
                    </person-group>
                    <article-title>Making Language Models Better Reasoners with Step-Aware Verifier</article-title>
                    <comment>In</comment>
                    <conf-name>
                        <italic>Proceedings of the 61st Annual Meeting of the Association for
                            Computational Linguistics</italic>
                    </conf-name>
                    <publisher-loc>Toronto, Canada</publisher-loc>
                    <year>2023</year>
                    <comment>pp</comment>
                    <fpage>5315</fpage>
                    <lpage>5333</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.18653/v1/2023.acl-long.291</pub-id>
                </element-citation>
            </ref>
            <ref id="R32">
                <label data-id="R32">32</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Li</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Hu</surname>
                            <given-names>B.</given-names>
                        </name>
                        <name>
                            <surname>Chen</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Zhong</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>Lyu</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>M</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>A comprehensive evaluation of GPT-4V on knowledge-intensive visual
                            question answering</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2311.07536</comment>
                </element-citation>
            </ref>
            <ref id="R33">
                <label data-id="R33">33</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Liao</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Shen</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>G.</given-names>
                        </name>
                        <name>
                            <surname>Bie</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Xu</surname>
                            <given-names>C</given-names>
                        </name>
                    </person-group>
                    <article-title>GPT-4 Enhanced Multimodal Grounding for Autonomous Driving:
                        Leveraging Cross-Modal Attention with Large Language Models</article-title>
                    <source>
                        <italic>Communications in Transportation Research</italic>
                    </source>
                    <year>2024</year>
                    <volume>4</volume>
                    <fpage>100116</fpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/j.commtr.2023.100116</pub-id>
                </element-citation>
            </ref>
            <ref id="R34">
                <label data-id="R34">34</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Little</surname>
                            <given-names>T. D.</given-names>
                        </name>
                        <name>
                            <surname>Rhemtulla</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Gibson</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>Schoemann</surname>
                            <given-names>A. M</given-names>
                        </name>
                    </person-group>
                    <article-title>Why the Items versus Parcels Controversy Needn’t Be One</article-title>
                    <source>
                        <italic>Psychol Methods</italic>
                    </source>
                    <year>2013</year>
                    <volume>18</volume>
                    <issue>3</issue>
                    <fpage>285</fpage>
                    <lpage>300</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1037/a0033266</pub-id>
                </element-citation>
            </ref>
            <ref id="R35">
                <label data-id="R35">35</label>
                <element-citation publication-type="website">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Liu</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Lee</surname>
                            <given-names>Y. J</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Improved baselines with visual instruction tuning</italic>
                    </article-title>
                    <uri xlink:href="https://doi.org/10.48550/arXiv.2310.03744">
                        https://doi.org/10.48550/arXiv.2310.03744</uri>
                </element-citation>
            </ref>
            <ref id="R36">
                <label data-id="R36">36</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Liu</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Chen</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Gurari</surname>
                            <given-names>D</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>An evaluation of GPT-4V and Gemini in online VQA</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2312.10637</comment>
                </element-citation>
            </ref>
            <ref id="R37">
                <label data-id="R37">37</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Liu</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Sun</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Yu</surname>
                            <given-names>P. S</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Rec-GPT4V: Multimodal recommendation with large vision-language
                            models</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2402.08670</comment>
                </element-citation>
            </ref>
            <ref id="R38">
                <label data-id="R38">38</label>
                <element-citation publication-type="website">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Lu</surname>
                            <given-names>P.</given-names>
                        </name>
                        <name>
                            <surname>Bansal</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Xia</surname>
                            <given-names>T.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Hajishirzi</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Cheng</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Chang</surname>
                            <given-names>K.-W.</given-names>
                        </name>
                        <name>
                            <surname>Galley</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Gao</surname>
                            <given-names>J</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>MathVista: Evaluating mathematical reasoning of foundation models in
                            visual contexts</italic>
                    </article-title>
                    <uri xlink:href="https://doi.org/10.48550/arXiv.2310.02255">
                        https://doi.org/10.48550/arXiv.2310.02255</uri>
                </element-citation>
            </ref>
            <ref id="R39">
                <label data-id="R39">39</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Lu</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Liusie</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Raina</surname>
                            <given-names>V.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Beauchamp</surname>
                            <given-names>W</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Blending is all you need: Cheaper, better alternative to
                            trillion-parameters LLM</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2401.02994</comment>
                </element-citation>
            </ref>
            <ref id="R40">
                <label data-id="R40">40</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Markus</surname>
                            <given-names>K. A.</given-names>
                        </name>
                        <name>
                            <surname>Borsboom</surname>
                            <given-names>D</given-names>
                        </name>
                    </person-group>
                    <article-title>Reflective Measurement Models, Behavior Domains, and Common
                        Causes</article-title>
                    <source>
                        <italic>New Ideas in Psychology</italic>
                    </source>
                    <year>2013</year>
                    <volume>31</volume>
                    <issue>1</issue>
                    <fpage>54</fpage>
                    <lpage>64</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/j.newideapsych.2011.02.008</pub-id>
                </element-citation>
            </ref>
            <ref id="R1">
                <label data-id="R1">1</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>McDonald</surname>
                            <given-names>R. P</given-names>
                        </name>
                    </person-group>
                    <article-title>Behavior Domains in Theory and in Practice</article-title>
                    <source>
                        <italic>Alberta Journal of Educational Research</italic>
                    </source>
                    <year>2003</year>
                    <volume>49</volume>
                    <fpage>212</fpage>
                    <lpage>230</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.11575/ajer.v49i3.54980</pub-id>
                </element-citation>
            </ref>
            <ref id="R2">
                <label data-id="R2">2</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Melman</surname>
                            <given-names>T.</given-names>
                        </name>
                        <name>
                            <surname>Abbink</surname>
                            <given-names>D. A.</given-names>
                        </name>
                        <name>
                            <surname>van Paassen</surname>
                            <given-names>M. M.</given-names>
                        </name>
                        <name>
                            <surname>Boer</surname>
                            <given-names>E. R.</given-names>
                        </name>
                        <name>
                            <surname>de Winter</surname>
                            <given-names>J. C. F</given-names>
                        </name>
                    </person-group>
                    <article-title>What Determines Drivers’ Speed? A Replication of Three
                        Behavioural Adaptation Experiments in a Single Driving Simulator Study</article-title>
                    <source>
                        <italic>Ergonomics</italic>
                    </source>
                    <year>2018</year>
                    <volume>61</volume>
                    <issue>7</issue>
                    <fpage>966</fpage>
                    <lpage>987</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1080/00140139.2018.1426790</pub-id>
                </element-citation>
            </ref>
            <ref id="R3">
                <label data-id="R3">3</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Näätänen</surname>
                            <given-names>R.</given-names>
                        </name>
                        <name>
                            <surname>Summala</surname>
                            <given-names>H</given-names>
                        </name>
                    </person-group>
                    <article-title>A Model for the Role of Motivational Factors in Drivers’
                        Decision-Making∗</article-title>
                    <source>
                        <italic>Accident Analysis &amp; Prevention</italic>
                    </source>
                    <year>1974</year>
                    <volume>6</volume>
                    <issue>3–4</issue>
                    <fpage>243</fpage>
                    <lpage>261</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/0001-4575(74)90003-7</pub-id>
                </element-citation>
            </ref>
            <ref id="R4">
                <label data-id="R4">4</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Nagle</surname>
                            <given-names>F.</given-names>
                        </name>
                        <name>
                            <surname>Lavie</surname>
                            <given-names>N</given-names>
                        </name>
                    </person-group>
                    <article-title>Predicting Human Complexity Perception of Real-World Scenes</article-title>
                    <source>
                        <italic>R Soc Open Sci</italic>
                    </source>
                    <year>2020</year>
                    <volume>7</volume>
                    <issue>5</issue>
                    <pub-id pub-id-type="doi">https://doi.org/10.1098/rsos.191487</pub-id>
                </element-citation>
            </ref>
            <ref id="R5">
                <label data-id="R5">5</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Naik</surname>
                            <given-names>N.</given-names>
                        </name>
                        <name>
                            <surname>Kominers</surname>
                            <given-names>S. D.</given-names>
                        </name>
                        <name>
                            <surname>Raskar</surname>
                            <given-names>R.</given-names>
                        </name>
                        <name>
                            <surname>Glaeser</surname>
                            <given-names>E. L.</given-names>
                        </name>
                        <name>
                            <surname>Hidalgo</surname>
                            <given-names>C. A</given-names>
                        </name>
                    </person-group>
                    <article-title>Computer Vision Uncovers Predictors of Physical Urban Change</article-title>
                    <source>
                        <italic>Proc. Natl. Acad. Sci. U.S.A</italic>
                    </source>
                    <year>2017</year>
                    <volume>114</volume>
                    <issue>29</issue>
                    <fpage>7571</fpage>
                    <lpage>7576</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1073/pnas.1619003114</pub-id>
                </element-citation>
            </ref>
            <ref id="R6">
                <label data-id="R6">6</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Nunnally</surname>
                            <given-names>J. C.</given-names>
                        </name>
                        <name>
                            <surname>Bernstein</surname>
                            <given-names>I. H</given-names>
                        </name>
                    </person-group>
                    <publisher-name>McGraw-Hill</publisher-name>
                    <x>: </x>
                    <publisher-loc>New York. NY</publisher-loc>
                    <year>1994</year>
                </element-citation>
            </ref>
            <ref id="R7">
                <label data-id="R7">7</label>
                <element-citation publication-type="website">
                    <x>)OpenAI. </x>
                    <article-title>
                        <italic>GPT-4 technical report</italic>
                    </article-title>
                    <uri xlink:href="https://cdn.openai.com/papers/gpt-4.pdf">
                        https://cdn.openai.com/papers/gpt-4.pdf</uri>
                </element-citation>
            </ref>
            <ref id="R8">
                <label data-id="R8">8</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Qi</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Fang</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Sun</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Wu</surname>
                            <given-names>T.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Lin</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Zhao</surname>
                            <given-names>H</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Gemini vs GPT-4V: A preliminary comparison and combination of
                            vision-language models through qualitative cases</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2312.15011</comment>
                </element-citation>
            </ref>
            <ref id="R9">
                <label data-id="R9">9</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Redmon</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Farhadi</surname>
                            <given-names>A</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>YOLOv3: An incremental improvement</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.1804.02767</comment>
                </element-citation>
            </ref>
            <ref id="R10">
                <label data-id="R10">10</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Salinas</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Morstatter</surname>
                            <given-names>F</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>The butterfly effect of altering prompts: How small changes and
                            jailbreaks affect large language model performance</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2401.03729</comment>
                </element-citation>
            </ref>
            <ref id="R11">
                <label data-id="R11">11</label>
                <element-citation publication-type="book">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Sawaki</surname>
                            <given-names>Y</given-names>
                        </name>
                    </person-group>
                    <chapter-title>Generalizability Theory</chapter-title>
                    <comment>In</comment>
                    <source>
                        <italic>Encyclopedia of research design</italic>
                    </source>
                    <person-group person-group-type="editor">
                        <name>
                            <surname>Salkind</surname>
                            <given-names>N. J.</given-names>
                        </name>
                    </person-group>
                    <comment>Ed</comment>
                    <publisher-name>Sage Publications</publisher-name>
                    <x>: </x>
                    <publisher-loc>Thousand Oaks, CA</publisher-loc>
                    <year>2010</year>
                    <pub-id pub-id-type="doi">https://doi.org/10.4135/9781412961288</pub-id>
                </element-citation>
            </ref>
            <ref id="R12">
                <label data-id="R12">12</label>
                <element-citation publication-type="preprint">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Senkaiahliyan M.</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Toma</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Ma</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Chan</surname>
                            <given-names>A.-W.</given-names>
                        </name>
                        <name>
                            <surname>Ha</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>An</surname>
                            <given-names>K. R.</given-names>
                        </name>
                        <name>
                            <surname>Suresh</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Rubin</surname>
                            <given-names>B.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>B</given-names>
                        </name>
                    </person-group>
                    <article-title>GPT-4V(Ision) Unsuitable for Clinical Care and Education: A
                        Clinician-Evaluated Assessment</article-title>
                    <source>
                        <italic>Medical Education</italic>
                    </source>
                    <year>2023</year>
                    <pub-id pub-id-type="doi">https://doi.org/10.1101/2023.11.15.23298575</pub-id>
                </element-citation>
            </ref>
            <ref id="R13">
                <label data-id="R13">13</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Tabone</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>de Winter</surname>
                            <given-names>J</given-names>
                        </name>
                    </person-group>
                    <article-title>Using ChatGPT for Human-Computer Interaction Research: A Primer</article-title>
                    <source>
                        <italic>R Soc Open Sci</italic>
                    </source>
                    <year>2023</year>
                    <volume>10</volume>
                    <issue>9</issue>
                    <pub-id pub-id-type="doi">https://doi.org/10.1098/rsos.231053</pub-id>
                </element-citation>
            </ref>
            <ref id="R14">
                <label data-id="R14">14</label>
                <element-citation publication-type="website">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Tang</surname>
                            <given-names>R.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Ma</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Lin</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Ture</surname>
                            <given-names>F</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Found in the middle: Permutation self-consistency improves listwise
                            ranking in large language models</italic>
                    </article-title>
                    <uri xlink:href="https://doi.org/10.48550/arXiv.2310.07712">
                        https://doi.org/10.48550/arXiv.2310.07712</uri>
                </element-citation>
            </ref>
            <ref id="R15">
                <label data-id="R15">15</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Tong</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Zhai</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Ma</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>LeCun</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Xie</surname>
                            <given-names>S</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Eyes wide shut? Exploring the visual shortcomings of multimodal LLMs</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2401.06209</comment>
                </element-citation>
            </ref>
            <ref id="R16">
                <label data-id="R16">16</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Wang</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Wei</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Schuurmans</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Le</surname>
                            <given-names>Q.</given-names>
                        </name>
                        <name>
                            <surname>Chi</surname>
                            <given-names>E.</given-names>
                        </name>
                        <name>
                            <surname>Narang</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Chowdhery</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Zhou</surname>
                            <given-names>D</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Self-consistency improves chain of thought reasoning in language
                            models</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2203.11171</comment>
                </element-citation>
            </ref>
            <ref id="R17">
                <label data-id="R17">17</label>
                <element-citation publication-type="book">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Wei</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Schuurmans</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Bosma</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Ichter</surname>
                            <given-names>B.</given-names>
                        </name>
                        <name>
                            <surname>Xia</surname>
                            <given-names>F.</given-names>
                        </name>
                        <name>
                            <surname>Chi</surname>
                            <given-names>E. H.</given-names>
                        </name>
                        <name>
                            <surname>Le</surname>
                            <given-names>Q. V.</given-names>
                        </name>
                        <name>
                            <surname>Zhou</surname>
                            <given-names>D</given-names>
                        </name>
                    </person-group>
                    <chapter-title>Chain-of-Thought Prompting Elicits Reasoning in Large Language
                        Models</chapter-title>
                    <comment>In</comment>
                    <source>
                        <italic>Advances in neural information processing systems</italic>
                    </source>
                    <person-group person-group-type="editor">
                        <name>
                            <surname>Koyejo</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Mohamed</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Agarwal</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Belgrave</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Cho</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>Oh</surname>
                            <given-names>A.</given-names>
                        </name>
                    </person-group>
                    <x>, Eds.; </x>
                    <year>2022</year>
                    <comment>Vol</comment>
                    <volume>35</volume>
                    <comment>pp</comment>
                    <fpage>24824</fpage>
                    <lpage>24837</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.2201.11903</pub-id>
                </element-citation>
            </ref>
            <ref id="R18">
                <label data-id="R18">18</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Wen</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Yang</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Fu</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Cai</surname>
                            <given-names>P.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Ma</surname>
                            <given-names>T.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Xu</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Shang</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Zhu</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Sun</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Bai</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Cai</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Dou</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Hu</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Shi</surname>
                            <given-names>B.</given-names>
                        </name>
                        <name>
                            <surname>Qiao</surname>
                            <given-names>Y</given-names>
                        </name>
                    </person-group>
                    <year>2023</year>
                    <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.2311.05332</pub-id>
                </element-citation>
            </ref>
            <ref id="R19">
                <label data-id="R19">19</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Wilde</surname>
                            <given-names>G. J. S</given-names>
                        </name>
                    </person-group>
                    <article-title>The Theory of Risk Homeostasis: Implications for Safety and
                        Health</article-title>
                    <source>
                        <italic>Risk Analysis</italic>
                    </source>
                    <year>1982</year>
                    <volume>2</volume>
                    <issue>4</issue>
                    <fpage>209</fpage>
                    <lpage>225</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1111/j.1539-6924.1982.tb01384.x</pub-id>
                </element-citation>
            </ref>
            <ref id="R20">
                <label data-id="R20">20</label>
                <element-citation publication-type="book">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Wilde</surname>
                            <given-names>G. J. S</given-names>
                        </name>
                    </person-group>
                    <chapter-title>Homeostasis Drives Behavioural Adaptation</chapter-title>
                    <comment>In</comment>
                    <source>
                        <italic>Behavioural adaptation and road safety: Theory, evidence and action</italic>
                    </source>
                    <person-group person-group-type="editor">
                        <name>
                            <surname>Rudin-Brown</surname>
                            <given-names>C. M.</given-names>
                        </name>
                        <name>
                            <surname>Jamson</surname>
                            <given-names>S. L.</given-names>
                        </name>
                    </person-group>
                    <x>, Eds.; </x>
                    <publisher-name>CRC Press</publisher-name>
                    <x>: </x>
                    <publisher-loc>Boca Raton, FL</publisher-loc>
                    <year>2013</year>
                    <comment>pp</comment>
                    <fpage>61</fpage>
                    <lpage>86</lpage>
                </element-citation>
            </ref>
            <ref id="R21">
                <label data-id="R21">21</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Wu</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Lei</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Zheng</surname>
                            <given-names>Q.</given-names>
                        </name>
                        <name>
                            <surname>Zhao</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>Lin</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Zhou</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Zhao</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Xie</surname>
                            <given-names>W</given-names>
                        </name>
                    </person-group>
                    <year>2023</year>
                </element-citation>
            </ref>
            <ref id="R22">
                <label data-id="R22">22</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Yan</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Yang</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Zhu</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>Lin</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Yang</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Zhong</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>McAuley</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Gao</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>L</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>GPT-4V in wonderland: Large multimodal models for zero-shot
                            smartphone GUI navigation</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2311.07562</comment>
                </element-citation>
            </ref>
            <ref id="R23">
                <label data-id="R23">23</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Yan</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Cai</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Guo</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Qiu</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>Gao</surname>
                            <given-names>B.</given-names>
                        </name>
                        <name>
                            <surname>Zhou</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>Zhao</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Jin</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Gao</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Jiang</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Dai</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>B</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Forging vision foundation models for autonomous driving: Challenges,
                            methodologies, and opportunities</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2401.08045</comment>
                </element-citation>
            </ref>
            <ref id="R24">
                <label data-id="R24">24</label>
                <element-citation publication-type="website">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Yang</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>F.</given-names>
                        </name>
                        <name>
                            <surname>Zou</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Gao</surname>
                            <given-names>J</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Set-of-mark prompting unleashes extraordinary visual grounding in
                            GPT-4V</italic>
                    </article-title>
                    <uri xlink:href="https://doi.org/10.48550/arXiv.2310.11441">
                        https://doi.org/10.48550/arXiv.2310.11441</uri>
                </element-citation>
            </ref>
            <ref id="R25">
                <label data-id="R25">25</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Yang</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Li</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Lin</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Lin</surname>
                            <given-names>C. C.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>L</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>The dawn of LLMs: Preliminary explorations with GPT-4V(ision)</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2309.17421</comment>
                </element-citation>
            </ref>
            <ref id="R26">
                <label data-id="R26">26</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Ye</surname>
                            <given-names>Q.</given-names>
                        </name>
                        <name>
                            <surname>Xu</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Ye</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Yan</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Qian</surname>
                            <given-names>Q.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Huang</surname>
                            <given-names>F.</given-names>
                        </name>
                        <name>
                            <surname>Zhou</surname>
                            <given-names>J</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>mPLUG-Owl2: Revolutionizing multi-modal large language model with
                            modality collaboration</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2311.04257</comment>
                </element-citation>
            </ref>
            <ref id="R27">
                <label data-id="R27">27</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Yue</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Ni</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>K.</given-names>
                        </name>
                        <name>
                            <surname>Zheng</surname>
                            <given-names>T.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>R.</given-names>
                        </name>
                        <name>
                            <surname>Zhang</surname>
                            <given-names>G.</given-names>
                        </name>
                        <name>
                            <surname>Stevens</surname>
                            <given-names>S.</given-names>
                        </name>
                        <name>
                            <surname>Jiang</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Ren</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>Sun</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Wei</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Yu</surname>
                            <given-names>B.</given-names>
                        </name>
                        <name>
                            <surname>Yuan</surname>
                            <given-names>R.</given-names>
                        </name>
                        <name>
                            <surname>Sun</surname>
                            <given-names>R.</given-names>
                        </name>
                        <name>
                            <surname>Yin</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Zheng</surname>
                            <given-names>B.</given-names>
                        </name>
                        <name>
                            <surname>Yang</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Huang</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>Chen</surname>
                            <given-names>W</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>MMMU: A massive multi-discipline multimodal understanding and
                            reasoning benchmark for expert AGI</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2311.16502</comment>
                </element-citation>
            </ref>
            <ref id="R28">
                <label data-id="R28">28</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Zhang</surname>
                            <given-names>C.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>S</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>Good at captioning, bad at counting: Benchmarking GPT-4V on Earth
                            observation data</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2401.17600</comment>
                </element-citation>
            </ref>
            <ref id="R29">
                <label data-id="R29">29</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Zhang</surname>
                            <given-names>D.</given-names>
                        </name>
                        <name>
                            <surname>Yang</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Lyu</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Jin</surname>
                            <given-names>Z.</given-names>
                        </name>
                        <name>
                            <surname>Yao</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Chen</surname>
                            <given-names>M.</given-names>
                        </name>
                        <name>
                            <surname>Luo</surname>
                            <given-names>J</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>CoCoT: Contrastive chain-of-thought prompting for large multimodal
                            models with multiple image inputs</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2401.02582</comment>
                </element-citation>
            </ref>
            <ref id="R30">
                <label data-id="R30">30</label>
                <element-citation publication-type="journal">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Zhang</surname>
                            <given-names>F.</given-names>
                        </name>
                        <name>
                            <surname>Zhou</surname>
                            <given-names>B.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Liu</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Fung</surname>
                            <given-names>H. H.</given-names>
                        </name>
                        <name>
                            <surname>Lin</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Ratti</surname>
                            <given-names>C</given-names>
                        </name>
                    </person-group>
                    <article-title>Measuring Human Perceptions of a Large-Scale Urban Region Using
                        Machine Learning</article-title>
                    <source>
                        <italic>Landscape and Urban Planning</italic>
                    </source>
                    <year>2018</year>
                    <volume>180</volume>
                    <fpage>148</fpage>
                    <lpage>160</lpage>
                    <pub-id pub-id-type="doi">https://doi.org/10.1016/j.landurbplan.2018.08.020</pub-id>
                </element-citation>
            </ref>
            <ref id="R31">
                <label data-id="R31">31</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Zhang</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Lu</surname>
                            <given-names>Y.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>W.</given-names>
                        </name>
                        <name>
                            <surname>Yan</surname>
                            <given-names>A.</given-names>
                        </name>
                        <name>
                            <surname>Yan</surname>
                            <given-names>J.</given-names>
                        </name>
                        <name>
                            <surname>Qin</surname>
                            <given-names>L.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>H.</given-names>
                        </name>
                        <name>
                            <surname>Yan</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Wang</surname>
                            <given-names>W. Y.</given-names>
                        </name>
                        <name>
                            <surname>Petzold</surname>
                            <given-names>L. R</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>GPT-4V(ision) as a generalist evaluator for vision-language tasks</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2311.01361</comment>
                </element-citation>
            </ref>
            <ref id="R32">
                <label data-id="R32">32</label>
                <element-citation publication-type="other">
                    <person-group person-group-type="author">
                        <name>
                            <surname>Zhou</surname>
                            <given-names>X.</given-names>
                        </name>
                        <name>
                            <surname>Knoll</surname>
                            <given-names>A. C</given-names>
                        </name>
                    </person-group>
                    <article-title>
                        <italic>GPT-4V as traffic assistant: An in-depth look at vision language
                            model on complex traffic events</italic>
                    </article-title>
                    <comment>https://doi.org/10.48550/arXiv.2402.02205</comment>
                </element-citation>
            </ref>
            <ref id="R33">
                <label data-id="R33">33</label>
                <element-citation publication-type="other">
                    <x>)Appendix. .</x>
                </element-citation>
            </ref>
        </ref-list>
    </back>
</article>
