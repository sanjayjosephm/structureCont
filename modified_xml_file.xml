<?xml version="1.0" encoding="UTF-8"?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="research-article" xml:lang="en">
    <front xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  data-edited="true" data-edited-version="2">
        <journal-meta >
            <journal-id journal-id-type="publisher-id" >
                RSOS</journal-id>
            <journal-id journal-id-type="hwp" >royopensci</journal-id>
            <journal-title-group >
                <journal-title >Royal Society Open Science</journal-title>
                <abbrev-journal-title >R. Soc. Open Sci.</abbrev-journal-title>
            </journal-title-group>
            <issn pub-type="ppub" />
            <issn pub-type="epub" >2054-5703</issn>
            <publisher >
                <publisher-name >The Royal Society</publisher-name>
            </publisher>
        </journal-meta>
        <article-meta >
            <article-id pub-id-type="publisher-id" >
                10.1098/rsos.231676</article-id>
            <article-id pub-id-type="doi" >RSOS-231676</article-id>
            <article-categories >
                <subj-group subj-group-type="display-channel" >
                    <subject >Research</subject>
                </subj-group>
                <subj-group subj-group-type="discipline-code" >
                    <subject >COMPUTER SCIENCE</subject>
                </subj-group>
                <subj-group subj-group-type="discipline-code" >
                    <subject >BIOLOGY</subject>
                </subj-group>
                <subj-group subj-group-type="discipline-code" >
                    <subject >BIOLOGY</subject>
                </subj-group>
                <subj-group subj-group-type="primary-section" >
                    <subject >Computer Science and
                        Artificial Intelligence</subject>
                </subj-group>
                <subj-group subj-group-type="secondary-section" >
                    <subject >Artificial intelligence</subject>
                </subj-group>
                <subj-group subj-group-type="secondary-section" >
                    <subject >psychology</subject>
                </subj-group>
                <subj-group subj-group-type="secondary-section" >
                    <subject >cognition</subject>
                </subj-group>
            </article-categories>
            <title-group >
                <article-title data-pos-index="0" >Putting
                    ChatGPT Vision (GPT-4V) to the test: Risk perception in traffic images</article-title>
            </title-group>
            <contrib-group data-group="author" >
                <sup >10, 2024</sup>
                <contrib contrib-type="author"  data-id="author-113429498" data-edited-node="true">
                    <name >
                        <surname >Driessen</surname>
                        <given-names initial="PT" >Tom Tim</given-names>
                    </name>
                    <xref  rid="aff1" ref-type="aff">1</xref>
                    <xref data-con-type="visualization"  rid="con1" ref-type="fn">1</xref>
                    <email >t.driessen@tudelft.nl</email>
                </contrib>
                <contrib contrib-type="author"  data-id="author-9225251" data-edited-node="true">
                    <name >
                        <surname >Dodou</surname>
                        <given-names initial="V" >
                            Dimitra-Sumithra</given-names>
                    </name>
                    <xref  rid="aff2" ref-type="aff">2</xref>
                    <xref data-con-type="writing-review-and-editing"  rid="con2" ref-type="fn">2</xref>
                    <email >d.dodou@tudelft.nl</email>
                </contrib>
                <contrib contrib-type="author"  data-id="author-113429450" data-edited-node="true">
                    <name >
                        <surname >Bazilinskyy</surname>
                        <given-names initial="O V B" >Pavlo
                            C</given-names>
                    </name>
                    <xref data-con-type="software"  rid="con1715057471106" ref-type="fn"/>
                    <xref data-con-type="writing-review-and-editing"  rid="con2" ref-type="fn">2</xref>
                    <email >p.bazilinskyy@tue.nl</email>
                </contrib>
                <contrib corresp="yes" contrib-type="author"  data-id="author-106115153" data-edited-node="true">
                    <name >
                        <surname >de Winter</surname>
                        <given-names >R R Joost</given-names>
                    </name>
                    <xref ref-type="corresp" rid="cor1" >*</xref>
                    <xref data-con-type="visualization"  rid="con1" ref-type="fn">1</xref>
                    <xref data-con-type="data-curation"  rid="con1715057034808" ref-type="fn"/>
                    <email >j.c.f.dewinter@tudelft.nl</email>
                    <contrib-id contrib-id-type="orcid" data-retain="true" >
                        http://orcid.org/0000-0002-1281-8200</contrib-id>
                </contrib>
                <contrib data-title="Inserted by Jincy (PREEDITOR) - May 7 2024 10:14am" contrib-type="author" data-id="contrib5"  data-track="ins" data-cid="1715057064713" data-username="Jincy (PREEDITOR)" data-userid="preeditor 46da008d-2952-4b91-828a-cc28eaa3aade" data-time="1715057064712" data-edited-node="true">
                    <name >
                        <surname >Dume</surname>
                        <given-names >Ricky R Sock</given-names>
                    </name>
                    <xref  rid="aff2" ref-type="aff">2</xref>
                </contrib>
                <contrib data-title="Inserted by Jincy (PREEDITOR) - May 7 2024 10:14am" contrib-type="author" data-id="contrib6"  data-track="ins" data-cid="1715057095311" data-username="Jincy (PREEDITOR)" data-userid="preeditor 46da008d-2952-4b91-828a-cc28eaa3aade" data-time="1715057095310">
                    <name >
                        <surname >Sunil</surname>
                        <given-names >Hitler-de S</given-names>
                    </name>
                </contrib>
                <aff node-insert-xpath="//front//contrib-group" node-insertafter-xpath="//front//contrib[last()]" data-id="aff1"  data-track="ins" data-cid="1715058144586" data-username="Jincy (PREEDITOR)" data-userid="preeditor 46da008d-2952-4b91-828a-cc28eaa3aade" data-time="1715058144586" data-edited-node="true"><label >1</label><named-content content-type="institution" >
                    Institution</named-content>, <named-content content-type="city" >City</named-content> <named-content content-type="postal-code" >14325</named-content>
                    , <named-content content-type="country" >Albania</named-content></aff>
                <aff node-insert-xpath="//front//contrib-group" node-insertafter-xpath="//front//contrib[last()]" data-pos-index="4" data-affs="true"  data-id="aff2"><label >2</label>Eindhoven University of
                    Technology, The Netherlands</aff>
                <aff node-insert-xpath="//front//contrib-group" node-insertafter-xpath="//front//contrib[last()]" data-pos-index="3" data-affs="true"  data-id="aff1715058276114" data-track="del" data-cid="1714383364116" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364115"><label >3</label>Delft
                    University of Technology, The Netherlands</aff>
            </contrib-group>
            <contrib-group content-type="section" ><sup >1</sup>, <sup >1</sup>, <sup >2</sup>, <sup >1</sup></contrib-group>
            <author-notes >
                <corresp node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta"  data-id="cor1"><name data-class="jrnlAuthor" >
                        <given-names >R R Joost</given-names>
                        <surname >de Winter</surname>
                    </name>
                    ; <email >j.c.f.dewinter@tudelft.nl</email></corresp>
                <fn fn-type="declaration-of-ai" data-label="Declaration of AI use" >
                    <label >Declaration of AI use</label>
                    <p >We have not used AI-assisted
                        technologies in creating this article.</p>
                </fn>
                <fn fn-type="conflict" data-label="Competing interests" >
                    <label >Conflict of interest
                        declaration</label>
                    <p >We declare we have no competing
                        interests.</p>
                </fn>
                <fn fn-type="data-sharing-statement" data-label="Data accessibility" >
                    <label >Data accessibility</label>
                    <p >The code used in this project can
                        be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.4121/dfbe6de4-d559-49cd-a7c6-9bebe5d43d50" >
                        https://doi.org/10.4121/dfbe6de4-d559-49cd-a7c6-9bebe5d43d50</ext-link></p>
                </fn>
                <fn fn-type="ethics" data-label="Ethics approval" >
                    <label >Ethics</label>
                    <p >This work did not require ethical
                        approval from a human subject or animal welfare committee.</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1" data-con-type="visualization">
                    <p >Visualization</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="Con"  data-id="con2" data-con-type="writing-review-and-editing">
                    <p >Writing – review and editing</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145408" data-con-type="conceptualization" data-track="del" data-cid="1714383364186" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364186">
                    <p >Conceptualization</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145406" data-con-type="data-curation" data-track="del" data-cid="1714383364188" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364188">
                    <p >Data curation</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145411" data-con-type="formal-analysis" data-track="del" data-cid="1714383364190" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364190">
                    <p >Formal analysis</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145412" data-con-type="investigation" data-track="del" data-cid="1714383364192" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364192">
                    <p >Investigation</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145413" data-con-type="methodology" data-track="del" data-cid="1714383364193" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364193">
                    <p >Methodology</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145418" data-con-type="software" data-track="del" data-cid="1714383364197" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364197">
                    <p >Software</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145416" data-con-type="validation" data-track="del" data-cid="1714383364200" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364200">
                    <p >Validation</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145421" data-con-type="writing-original-draft" data-track="del" data-cid="1714383364201" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364201">
                    <p >Writing – original draft</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145422" data-con-type="writing-review-&amp;-editing" data-track="del" data-cid="1714383364203" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364203">
                    <p >Writing – review &amp;
                        editing</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145427" data-con-type="project-administration" data-track="del" data-cid="1714383364205" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364205">
                    <p >Project administration</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145431" data-con-type="resources" data-track="del" data-cid="1714383364207" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364207">
                    <p >Resources</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145429" data-con-type="funding-acquisition" data-track="del" data-cid="1714383364209" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364209">
                    <p >Funding acquisition</p>
                </fn>
                <fn node-parent-node="author-notes" node-parent-insertafter-xpath="//front//contrib-group[last()]" node-parent-insert-xpath="//front//article-meta" fn-type="con" data-label="con"  data-id="con1715058145434" data-con-type="supervision" data-track="del" data-cid="1714383364211" data-username="Harish (PREEDITOR)" data-userid="preeditor 5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" data-time="1714383364211">
                    <p >Supervision</p>
                </fn>
            </author-notes>
            <pub-date pub-type="ppub" />
            <pub-date pub-type="epub" />
            <volume />
            <issue />
            <fpage >1</fpage>
            <lpage >12</lpage>
            <elocation-id >rsos.231676</elocation-id>
            <history >
                <date date-type="received" >
                    <day >03</day>
                    <month >11</month>
                    <year >2023</year>
                </date>
                <date date-type="rev-recd" >
                    <day >12</day>
                    <month >03</month>
                    <year >2024</year>
                </date>
                <date date-type="accepted" >
                    <day >15</day>
                    <month >04</month>
                    <year >2024</year>
                </date>
            </history>
            <permissions >
                <copyright-statement >Copyright ©
                    2024, The Authors.</copyright-statement>
                <copyright-year >2024</copyright-year>
                <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/" >
                    <license-p >This article is Open
                        Access: CC BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/" >
                        https://creativecommons.org/licenses/by/4.0/</ext-link>)</license-p>
                </license>
            </permissions>
            <files >
                <file file-type="Response to Referees" file_category="" file_name="RSOS-231676-File001.docx" submission_medium="online" >
                    <file_createddate >2024-04-10
                        03:42:09.9</file_createddate>
                    <file-format >docx</file-format>
                    <file-extension >docx</file-extension>
                    <file-name >Response to reviewers3.docx</file-name>
                    <file-caption />
                    <file_tag />
                    <attribute attr_name="File Designation" >Response to Referees</attribute>
                </file>
                <file file-type="Main Document" file_category="" file_name="RSOS-231676-File002.docx" submission_medium="online" >
                    <file_createddate >2024-04-10
                        03:35:41.41</file_createddate>
                    <file-format >docx</file-format>
                    <file-extension >docx</file-extension>
                    <file-name >
                        Manuscript_April2024(18).docx</file-name>
                    <file-caption />
                    <file_tag />
                    <attribute attr_name="File Designation" >Main Document</attribute>
                </file>
                <file file-type="Figure" file_category="" file_name="RSOS-231676-File003.zip" submission_medium="online" >
                    <file_createddate >2024-04-12
                        11:15:57.57</file_createddate>
                    <file-format >zip</file-format>
                    <file-extension >zip</file-extension>
                    <file-name >Final files.zip</file-name>
                    <file-caption >Not applicable: The zip
                        folder contains all figures, tables, and corresponding captions. Note that
                        we provide both vector (.eps) and .png formats, for your convenience.</file-caption>
                    <file_tag />
                    <attribute attr_name="File Designation" >Figure</attribute>
                </file>
                <file file-type="Figure" file_category="" file_name="RSOS-231676-File004.png" submission_medium="online" >
                    <file_createddate >2024-04-15
                        11:26:14.14</file_createddate>
                    <file-format >png</file-format>
                    <file-extension >png</file-extension>
                    <file-name >Figure5 FIN.png</file-name>
                    <file-caption />
                    <file_tag />
                    <attribute attr_name="File Designation" >Figure</attribute>
                </file>
                <file file-type="Figure" file_category="" file_name="RSOS-231676-File005.eps" submission_medium="online" >
                    <file_createddate >2024-04-15
                        11:26:44.44</file_createddate>
                    <file-format >eps</file-format>
                    <file-extension >eps</file-extension>
                    <file-name >Figure5 FIN.eps</file-name>
                    <file-caption />
                    <file_tag />
                    <attribute attr_name="File Designation" >Figure</attribute>
                </file>
            </files>
            <abstract >
                <title data-level="1" data-pos-index="5" >
                    Abstract</title>
                <p data-pos-index="6" >Vision-language
                    models are of interest in various domains, including automated driving, where
                    computer vision techniques can accurately detect road users, but where the
                    vehicle sometimes fails to understand context. This study examined the
                    effectiveness of GPT-4V in predicting the level of ‘risk’ in
                    traffic images as assessed by humans. We used 210 static images taken from a
                    moving vehicle, each previously rated by approximately 650 people. Based on
                    psychometric construct theory and using insights from the self-consistency
                    prompting method, we formulated three hypotheses: 1) repeating the prompt under
                    effectively identical conditions increases validity, 2) varying the prompt text
                    and extracting a total score increases validity compared to using a single
                    prompt, and 3) in a multiple regression analysis, the incorporation of object
                    detection features, alongside the GPT-4V-based risk rating, significantly
                    contributes to improving the model’s validity. Validity was quantified by
                    the correlation coefficient with human risk scores, across the 210 images. The
                    results confirmed the three hypotheses. The eventual validity coefficient was r
                    = 0.83, indicating that population-level human risk can be predicted using AI
                    with a high degree of accuracy. The findings suggest that GPT-4V must be
                    prompted in a way equivalent to how humans fill out a multi-item questionnaire.</p>
            </abstract>
            <funding-group >
                <award-group >
                    <funding-source >
                        <institution-wrap >
                            <institution-id >
                                http://dx.doi.org/10.13039/501100003246</institution-id>
                            <institution >Nederlandse
                                Organisatie voor Wetenschappelijk Onderzoek</institution>
                        </institution-wrap>
                    </funding-source>
                </award-group>
            </funding-group>
            <kwd-group kwd-group-type="author-keywords" >
                <kwd >risk perception</kwd>
                <kwd >traffic</kwd>
                <kwd >image-to-text</kwd>
                <kwd >ChatGPT</kwd>
            </kwd-group>
            <custom-meta-group >
                <custom-meta >
                    <meta-name >TPR</meta-name>
                    <meta-value >No</meta-value>
                </custom-meta>
                <custom-meta >
                    <meta-name >Copy editing requirement</meta-name>
                    <meta-value >2</meta-value>
                </custom-meta>
                <custom-meta >
                    <meta-name >proofingEngine</meta-name>
                    <meta-value >InDesignSetterCC</meta-value>
                </custom-meta>
                <custom-meta >
                    <meta-name >Job Loaded At</meta-name>
                    <meta-value >1713351698693</meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Bing, 2023</span> is cited in the text but
                            does not appear in the reference list. Please provide the complete
                            details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">OpenAI, 2023</span> is cited in the text
                            but does not appear in the reference list. Please provide the complete
                            details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Ahrabian et al., 2024</span> is cited in
                            the text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Lu et al., 2023</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">M. Liu et al., 2024</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Qi et al., 2023</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Jain et al., 2021</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Hwang et al., 2024</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Wen et al., 2023</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Tong et al., 2024</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Cui et al., 2023</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Wei et al., 2022</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Huang et al., 2023</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Salinas &amp; Morstatter, 2024</span> is
                            cited in the text but does not appear in the reference list. Please
                            provide the complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Tabone &amp; De Winter, 2023</span> is
                            cited in the text but does not appear in the reference list. Please
                            provide the complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Tang et al., 2023</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Wang et al., 2023</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Lu et al., 2024</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Bochkovskiy et al., 2020</span> is cited
                            in the text but does not appear in the reference list. Please provide
                            the complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Redmon &amp; Farhadi, 2018</span> is cited
                            in the text but does not appear in the reference list. Please provide
                            the complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that <span data-selector=".//text()" data-type="htmlComponent">Table 2</span> is cited in the text but
                            does not appear in the content. Please provide the complete details for
                            this <span data-selector=".//text()" data-type="htmlComponent">Table 2</span>
                            or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that <span data-selector=".//text()" data-type="htmlComponent">Table 3</span> is cited in the text but
                            does not appear in the content. Please provide the complete details for
                            this <span data-selector=".//text()" data-type="htmlComponent">Table 3</span>
                            or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Dubey et al., 2016</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="query"  xlink:type="action" data-type="action" data-query-action="uncited-citation">
                    <meta-value>
                        <named-content xlink:title="Kriya (PREEDITOR)" specific-use="17 Apr 2024 4:40 PM" content-type="Author" alt="preeditor">
                            Please note that reference <span data-selector=".//text()" data-type="htmlComponent">Liu et al., 2023</span> is cited in the
                            text but does not appear in the reference list. Please provide the
                            complete details for this reference or delete the citation.</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="5a8d6fcc-8437-453b-af56-ed3ed3606cdc" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >deleted</named-content>
                            <named-content specific-use="oldVal" data-title="" >Department</named-content>
                            <named-content specific-use="" data-title="" > in Department field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:34 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="56aa463f-7bad-4979-a7e1-1fd98e937a01" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >14325</named-content>
                            <named-content specific-use="" data-title="" > in Postal code field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:34 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="c8927a40-31e8-4706-b2d3-98a66345eada" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >deleted</named-content>
                            <named-content specific-use="oldVal" data-title="" >State</named-content>
                            <named-content specific-use="" data-title="" > in State field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:34 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="e8102b7f-972b-49df-a789-6cd21bbe495c" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >deleted</named-content>
                            <named-content specific-use="oldVal" data-title="" >2435467</named-content>
                            <named-content specific-use="" data-title="" > in Postal code field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:33 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="5a8d6fcc-8437-453b-af56-ed3ed3606cdc" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >Department</named-content>
                            <named-content specific-use="" data-title="" > in Department field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:33 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="c8927a40-31e8-4706-b2d3-98a66345eada" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >State</named-content>
                            <named-content specific-use="" data-title="" > in State field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:33 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="41329ccb-c5a0-47b9-8f0f-579b283897d7" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >deleted</named-content>
                            <named-content specific-use="oldVal" data-title="" >Department</named-content>
                            <named-content specific-use="" data-title="" > in Department field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:33 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="e8102b7f-972b-49df-a789-6cd21bbe495c" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >2435467</named-content>
                            <named-content specific-use="" data-title="" > in Postal code field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:32 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="545e23a1-b369-48e4-a10c-7e14ba5a09e3" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >3456</named-content>
                            <named-content specific-use="" data-title="" > in Postal code field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:32 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="ac70edf8-c7ec-4b6f-a6e8-b756d136559d" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >deleted</named-content>
                            <named-content specific-use="oldVal" data-title="" >State</named-content>
                            <named-content specific-use="" data-title="" > in State field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:32 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="6ea9c0fb-dc72-4dbf-8cb9-206336e0136c" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >deleted</named-content>
                            <named-content specific-use="oldVal" data-title="" >238934</named-content>
                            <named-content specific-use="" data-title="" > in Postal code field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:32 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-113429498" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="13248841-47ec-4bde-a6f4-f2e3451f036c" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="Department, Institution, City, State 238934, Albania" >Department, Institution,
                                City, State 238934, Albania</named-content>
                            <named-content specific-use="" data-title="" >in Affiliations on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:32 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-113429498" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="c2dcc5d9-9299-46ed-bfe6-5ac4dd827f9d" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >changed</named-content>
                            <named-content specific-use="oldVal" data-title="" >TT</named-content>
                            <named-content specific-use="" data-title="" >to</named-content>
                            <named-content specific-use="newVal" data-title="" >PT</named-content>
                            <named-content specific-use="" data-title="" > in Initials field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:22 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-113429450" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="ed7b66a7-eb61-4e7d-9ec1-a17d7e46e2cd" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >O V B</named-content>
                            <named-content specific-use="" data-title="" > in Initials field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:21 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-9225251" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="b11ac996-7093-405d-9ca4-1bcfe6abc4e4" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >V</named-content>
                            <named-content specific-use="" data-title="" > in Initials field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:21 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-113429498" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="c2dcc5d9-9299-46ed-bfe6-5ac4dd827f9d" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >TT</named-content>
                            <named-content specific-use="" data-title="" > in Initials field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:21 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="d1dbacfd-0c78-493c-90e0-e63f10578476" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="28d35629-6b5f-4ea5-b57c-f88f4d0cabbd" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >Sunil</named-content>
                            <named-content specific-use="" data-title="" > in Surname field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:14 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="d1dbacfd-0c78-493c-90e0-e63f10578476" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="6941cfa9-6389-42b8-9e8e-1be68c247693" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >Hitler-de S</named-content>
                            <named-content specific-use="" data-title="" > in Given name field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:14 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="f72f6b4b-02ae-4034-8fc1-46154bcbd07e" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="fa10adbd-7ee4-4ded-987b-f6ae275b3f3e" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="Eindhoven University of Technology, The Netherlands" >Eindhoven University of
                                Technology, The Netherlands</named-content>
                            <named-content specific-use="" data-title="" >in Affiliations on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:14 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="f72f6b4b-02ae-4034-8fc1-46154bcbd07e" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="6e78a9f2-f0f8-4016-b2b3-093c64d400fe" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >Dume</named-content>
                            <named-content specific-use="" data-title="" > in Surname field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:14 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="f72f6b4b-02ae-4034-8fc1-46154bcbd07e" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="a1be923f-5d1e-4e93-85e7-4a7ae84f6781" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >inserted</named-content>
                            <named-content specific-use="newVal" data-title="" >Ricky R Sock</named-content>
                            <named-content specific-use="" data-title="" > in Given name field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:14 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-106115153" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="38e769d7-c98c-41d6-a0b9-36744ac58939" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >changed</named-content>
                            <named-content specific-use="oldVal" data-title="" >Joost</named-content>
                            <named-content specific-use="" data-title="" >to</named-content>
                            <named-content specific-use="newVal" data-title="" >R R Joost</named-content>
                            <named-content specific-use="" data-title="" > in Given name field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:14 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-113429450" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="59eed060-921b-4f73-ac4e-fb782ae0a4cd" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >changed</named-content>
                            <named-content specific-use="oldVal" data-title="" >Pavlo</named-content>
                            <named-content specific-use="" data-title="" >to</named-content>
                            <named-content specific-use="newVal" data-title="" >Pavlo C</named-content>
                            <named-content specific-use="" data-title="" > in Given name field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:13 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-9225251" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="b726d21f-7ee6-4158-851b-95f711788a54" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >changed</named-content>
                            <named-content specific-use="oldVal" data-title="" >Dimitra</named-content>
                            <named-content specific-use="" data-title="" >to</named-content>
                            <named-content specific-use="newVal" data-title="Dimitra-Sumithra" >Dimitra-Sumithra</named-content>
                            <named-content specific-use="" data-title="" > in Given name field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:13 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-113429498" xlink:actuate="46da008d-2952-4b91-828a-cc28eaa3aade" xlink:role="preeditor" data-username="" xlink:href="e3309cab-a2f5-430d-af55-dc4bb01c37d0" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Jincy</named-content>
                            <named-content specific-use="" data-title="" >changed</named-content>
                            <named-content specific-use="oldVal" data-title="" >Tom</named-content>
                            <named-content specific-use="" data-title="" >to</named-content>
                            <named-content specific-use="newVal" data-title="" >Tom Tim</named-content>
                            <named-content specific-use="" data-title="" > in Given name field on</named-content>
                            <named-content specific-use="change-time" data-title="" >7 May 2024 10:13 AM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-106115153" xlink:actuate="5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" xlink:role="preeditor" data-username="" xlink:href="99af3039-28a3-423e-b5c7-4f26c3da2014" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Harish</named-content>
                            <named-content specific-use="newVal" data-title="" >checked</named-content>
                            <named-content specific-use="" data-title="" >Data curation on</named-content>
                            <named-content specific-use="change-time" data-title="" >29 Apr 2024 4:16 PM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-113429450" xlink:actuate="5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" xlink:role="preeditor" data-username="" xlink:href="896c3dac-e305-4af8-96b4-18a61fa8650d" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Harish</named-content>
                            <named-content specific-use="newVal" data-title="" >checked</named-content>
                            <named-content specific-use="" data-title="" >Writing – review
                                and editing on</named-content>
                            <named-content specific-use="change-time" data-title="" >29 Apr 2024 4:14 PM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-113429450" xlink:actuate="5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" xlink:role="preeditor" data-username="" xlink:href="26336284-7ec8-4425-98a6-5534b4aa58e1" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Harish</named-content>
                            <named-content specific-use="newVal" data-title="" >checked</named-content>
                            <named-content specific-use="" data-title="" >Software on</named-content>
                            <named-content specific-use="change-time" data-title="" >29 Apr 2024 4:10 PM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-106115153" xlink:actuate="5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" xlink:role="preeditor" data-username="" xlink:href="5899e3ce-55c5-40a5-b832-e9018a6414b7" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Harish</named-content>
                            <named-content specific-use="newVal" data-title="" >checked</named-content>
                            <named-content specific-use="" data-title="" >Visualization on</named-content>
                            <named-content specific-use="change-time" data-title="" >29 Apr 2024 4:08 PM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-9225251" xlink:actuate="5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" xlink:role="preeditor" data-username="" xlink:href="50615292-c462-4f36-9a29-58ca706e3683" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Harish</named-content>
                            <named-content specific-use="newVal" data-title="" >checked</named-content>
                            <named-content specific-use="" data-title="" >Writing – review
                                and editing on</named-content>
                            <named-content specific-use="change-time" data-title="" >29 Apr 2024 3:06 PM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="history"  xlink:title="author-113429498" xlink:actuate="5f92c56b-d0d9-4fda-8cb2-5461b0dd8422" xlink:role="preeditor" data-username="" xlink:href="b46e1ed9-05c1-45bf-bf32-8a04db6ccdbe" xlink:fn-type="">
                    <meta-value>
                        <named-content specific-use="historyContent" data-title="" >
                            <named-content specific-use="change-person" data-title="" >Harish</named-content>
                            <named-content specific-use="newVal" data-title="" >checked</named-content>
                            <named-content specific-use="" data-title="" >Visualization on</named-content>
                            <named-content specific-use="change-time" data-title="" >29 Apr 2024 3:06 PM</named-content>
                        </named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="track-changes"  xlink:role="preeditor" data-username="Jincy (PREEDITOR)" xlink:href="author-113429498" xlink:title="7 May 2024 10:22 AM">
                    <meta-name>Author</meta-name>
                    <meta-value>
                        <named-content specific-use="changeData" data-title="" >Author was changed</named-content>
                    </meta-value>
                </custom-meta>
                <custom-meta xmlns:xlink="http://www.w3.org/1999/xlink" specific-use="track-changes"  xlink:role="preeditor" data-username="Jincy (PREEDITOR)" xlink:href="af6bdc5e-eb9b-4359-8a22-172fc8102c41" xlink:title="7 May 2024 10:34 AM">
                    <meta-name> Affiliation 1</meta-name>
                    <meta-value>
                        <named-content specific-use="changeData" data-title="" > Affiliation 1 was changed</named-content>
                    </meta-value>
                </custom-meta>
            </custom-meta-group>
            <counts >
                <word-count count="388" />
                <page-count count="12"/>
            </counts>
        </article-meta>
    </front>
    <body  data-edited="true" data-edited-version="2">
        <title data-level="1" data-class="jrnlHead1" data-pos-index="7" data-group-type="body" >Introduction</title>
        <title data-level="2" data-class="jrnlHead2" data-pos-index="8" >GPT-4V Background</title>
        <p data-class="jrnlSecPara" data-pos-index="9" >In
            late September 2023, OpenAI introduced image-to-text functionality for ChatGPT, also
            called GPT-4V or GPT4 Vision. At that time, image-to-text software, such as BLIP, and
            functionalities within Google’s Bard and Bing Chat were already available (<xref  ref-type="bibr" rid="R3" data-citation-string=" R3 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="251" data-match="Bing, 2023; " data-pattern="" data-tag-index="3_2"><named-content data-class="jrnlQueryRef" rid="c8a1272d-bb52-478b-901f-adda6cc9f2e9" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Bing, 2023</xref>; <xref  ref-type="bibr" rid="R18" data-citation-string=" R18 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="263" data-match="Google, 2023; " data-pattern="" data-tag-index="3_3">Google, 2023</xref>; <xref  ref-type="bibr" rid="R29" data-citation-string=" R29 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="277" data-match="Li et al., 2022; " data-pattern="" data-tag-index="3_4">Li et al., 2022</xref>; see <xref  ref-type="bibr" rid="R8" data-citation-string=" R8 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="298" data-match="Cui et al., 2024 " data-pattern="" data-tag-index="3_5">Cui et al., 2024</xref> for a survey on
            multimodal large language models). However, GPT-4V was highly anticipated due to the
            high quality of its output, as demonstrated in earlier previews (<xref  ref-type="bibr" rid="R46" data-citation-string=" R46 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="481" data-match="OpenAI, 2023)" data-pattern="" data-tag-index="3_6"><named-content data-class="jrnlQueryRef" rid="bd4b9a5c-8b77-4922-9e6e-9b0cda5dcced" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>OpenAI, 2023</xref>).</p>
        <p data-class="jrnlSecPara" data-pos-index="10" >The
            research so far demonstrates that GPT-4V exhibits strong generic skills. It can
            comprehend diverse stimuli such as written text, charts, graphical user interfaces,
            abstract visual pictures, and visual IQ tests (<xref  ref-type="bibr" rid="R1" data-citation-string=" R1 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="215" data-match="Ahrabian et al., 2024; " data-pattern="" data-tag-index="4_1"><named-content data-class="jrnlQueryRef" rid="bc963c17-f776-4383-a25d-c3b1994f48f2" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Ahrabian et al., 2024</xref>; <xref  ref-type="bibr" rid="R61" data-citation-string=" R61 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="238" data-match="Yan et al., 2023; " data-pattern="" data-tag-index="4_2">Yan et al., 2023</xref>; <xref  ref-type="bibr" rid="R63 R64" data-citation-string=" R63 R64 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="256" data-match="Z. Yang et al., 2023)" data-pattern="" data-tag-index="4_3">Z. Yang et al., 2023</xref>). GPT-4V is also
            capable of solving visual mathematical problems, although not yet at a high level (<xref  ref-type="bibr" rid="R38" data-citation-string=" R38 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="377" data-match="Lu et al., 2023)" data-pattern="" data-tag-index="4_4"><named-content data-class="jrnlQueryRef" rid="81b2e663-0b6c-4bf3-a2f8-a3afe2684139" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Lu et al., 2023</xref>). As
            of early 2024, GPT-4V is still considered superior to a recent competitor from Google,
            called Gemini-Pro (<xref  ref-type="bibr" rid="R36 R37" data-citation-string=" R36 R37 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="504" data-match="M. Liu et al., 2024; " data-pattern="" data-tag-index="4_6"><named-content data-class="jrnlQueryRef" rid="3bc12043-482b-4d0c-bc33-2948c3ae2389" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>M. Liu et al., 2024</xref>; <xref  ref-type="bibr" rid="R47" data-citation-string=" R47 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="525" data-match="Qi et al., 2023)" data-pattern="" data-tag-index="4_7"><named-content data-class="jrnlQueryRef" rid="f3278e2d-534a-43ce-9bff-b2ef328a74f6" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Qi et al., 2023</xref>), but
            see proprietary evaluations of Google’s largest model, Gemini-Ultra (<xref  ref-type="bibr" rid="R16" data-citation-string=" R16 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="618" data-match="Gemini Team Google, 2023; " data-pattern="" data-tag-index="4_8">Gemini
            Team Google, 2023</xref>; <xref  ref-type="bibr" rid="R66" data-citation-string=" R66 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="644" data-match="Yue et al., 2023)" data-pattern="" data-tag-index="4_9">Yue et al., 2023</xref>).</p>
        <p data-class="jrnlSecPara" data-pos-index="11" >There
            is strong interest in GPT-4V within the domain of automated driving. Current automated
            vehicles are effective at detecting objects and handling routine scenarios, but the
            challenge still lies in rare situations that are not included in the training data (<xref  ref-type="bibr" rid="R5" data-citation-string=" R5 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="261" data-match="Bogdoll et al., 2022; " data-pattern="" data-tag-index="5_1">Bogdoll et al., 2022</xref>; <xref  ref-type="bibr" rid="R25" data-citation-string=" R25 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="283" data-match="Jain et al., 2021)" data-pattern="" data-tag-index="5_2"><named-content data-class="jrnlQueryRef" rid="c4249096-0e50-4eea-8e40-5a1770b11c86" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Jain et al., 2021</xref>).
            The strength of GPT-4V (and other vision language models) is its ability to understand
            context, including scenarios not previously encountered (<xref  ref-type="bibr" rid="R24" data-citation-string=" R24 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="447" data-match="Hwang et al., 2024; " data-pattern="" data-tag-index="5_3"><named-content data-class="jrnlQueryRef" rid="fb98a4ab-74a2-4be9-9913-f55915796394" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Hwang et al., 2024</xref>; <xref  ref-type="bibr" rid="R63 R64" data-citation-string=" R63 R64 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="467" data-match="Z. Yang et al., 2023; " data-pattern="" data-tag-index="5_4">Z. Yang et al., 2023</xref>; <xref  ref-type="bibr" rid="R71" data-citation-string=" R71 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="489" data-match="Zhou &amp;amp; Knoll, 2024)" data-pattern="" data-tag-index="5_5">Zhou
            &amp; Knoll, 2024</xref>). On the other hand, while GPT-4V is skilled in recognising
            unusual traffic events, it is not skilled at seemingly trivial tasks such as recognising
            details like the status of traffic lights, and spatial tasks such as reporting the
            orientation and (relative) position of road users (<xref  ref-type="bibr" rid="R57" data-citation-string=" R57 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="791" data-match="Wen et al., 2023; " data-pattern="" data-tag-index="5_6"><named-content data-class="jrnlQueryRef" rid="3d707ee8-7703-472b-b5c8-aa74dcff7bbd" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Wen et al., 2023</xref>; <xref  ref-type="bibr" rid="R71" data-citation-string=" R71 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="809" data-match="Zhou &amp;amp; Knoll, 2024)" data-pattern="" data-tag-index="5_7">Zhou
            &amp; Knoll, 2024</xref>).</p>
        <p data-class="jrnlSecPara" data-pos-index="12" >Indeed,
            GPT-4V exhibits several limitations. It struggles with counting objects and judging
            details, such as answering the question <named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="103c935b-de50-43c5-a642-b2184c9f62a0" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="LQUOTE" data-track-detail="" data-tag-desc="LQUOTE" data-tag-type="LQUOTE" data-offset="132" data-match="“" data-replace="‘" data-pattern="‘" data-tag-index="6_16" data-function="addTrackChanges" data-time="" data-username="" data-userid="">“</named-content><italic >How many eyes can you see on the animal?</italic><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="3abde94b-d42a-4156-915d-b7bcc518b07e" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="RQUOTE" data-track-detail="" data-tag-desc="RQUOTE" data-tag-type="RQUOTE" data-offset="175" data-match="”" data-replace="’" data-pattern="’" data-tag-index="6_18" data-function="addTrackChanges" data-time="" data-username="" data-userid="">”</named-content> or <named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="9e262f79-dc4d-4290-9bec-c49a4cbc57c9" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="LQUOTE" data-track-detail="" data-tag-desc="LQUOTE" data-tag-type="LQUOTE" data-offset="182" data-match="“" data-replace="‘" data-pattern="‘" data-tag-index="6_17" data-function="addTrackChanges" data-time="" data-username="" data-userid="">“</named-content><italic >Count the number of trees in the given
            image</italic><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="316dfbed-f7e2-4166-a487-847e881b3e10" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="RQUOTE" data-track-detail="" data-tag-desc="RQUOTE" data-tag-type="RQUOTE" data-offset="229" data-match="”" data-replace="’" data-pattern="’" data-tag-index="6_19" data-function="addTrackChanges" data-time="" data-username="" data-userid="">”</named-content>, tasks that normally do not pose a challenge
            for humans (<xref  ref-type="bibr" rid="R54" data-citation-string=" R54 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="290" data-match="Tong et al., 2024; " data-pattern="" data-tag-index="6_1"><named-content data-class="jrnlQueryRef" rid="88abaab6-f1e8-4b4e-aaa6-77d20fb66277" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Tong et al., 2024</xref>; <xref  ref-type="bibr" rid="R67" data-citation-string=" R67 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="309" data-match="Zhang &amp;amp; Wang, 2024)" data-pattern="" data-tag-index="6_2">Zhang
            &amp; Wang, 2024</xref>). Furthermore, although GPT-4V performs well in commonsense
            visual question answering, it is prone to hallucinations when world knowledge is
            required, such as about real-world objects (<xref  ref-type="bibr" rid="R30 R32" data-citation-string=" R30 R32 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="513" data-match="Y. Li et al., 2024)" data-pattern="" data-tag-index="6_3">Y. Li et al., 2024</xref>), especially for
            objects from non-Western countries (<xref  ref-type="bibr" rid="R9" data-citation-string=" R9 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="585" data-match="Cui et al., 2023)" data-pattern="" data-tag-index="6_4"><named-content data-class="jrnlQueryRef" rid="b47e10e0-e823-40b4-ba83-2ea094ca64e8" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Cui et al., 2023</xref>). A
            similar pattern has been observed for medical images, where GPT-4V does not seem to
            possess the knowledge required for making accurate diagnoses or reports (<xref  ref-type="bibr" rid="R51" data-citation-string=" R51 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="763" data-match="Senkaiahliyan et al., 2023; " data-pattern="" data-tag-index="6_5">Senkaiahliyan
            et al., 2023</xref>; <xref  ref-type="bibr" rid="R60" data-citation-string=" R60 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="791" data-match="Wu et al., 2023)" data-pattern="" data-tag-index="6_6">Wu et al., 2023</xref>). <xref  ref-type="bibr" rid="R20" data-citation-string=" R20 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="809" data-match="Guan et al. (2023)" data-pattern="" data-tag-index="6_7">Guan et al. (2023)</xref> made a distinction
            between visual illusions, in which a visual element is misrepresented, and language
            hallucinations, where GPT-4V fails to recognise a feature in the image because it
            adheres to previously learned stereotypical responses for similar images. Guan <named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="04798ad9-caee-4516-aca1-cb4d8b39df8d" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="ETL" data-track-detail="" data-tag-desc="ETL" data-tag-type="ETL" data-offset="1092" data-match="et al" data-replace="&lt;i>et al&lt;/i>" data-pattern="&lt;i>et al&lt;/i>" data-tag-index="6_14" data-time="" data-username="" data-userid="">et al</named-content>.
            also indicated that ChatGPT exhibits limitations in temporal reasoning abilities.</p>
        <title data-level="2" data-class="jrnlHead2" data-pos-index="13" >Prompting Methods</title>
        <p data-class="jrnlSecPara" data-pos-index="14" >Different
            strategies exist for improving the output of GPT-4V. This includes a prompting method
            where images are first segmented and marked with characters or boxes before being
            submitted to GPT-4V (<xref  ref-type="bibr" rid="R63 R64" data-citation-string=" R63 R64 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="199" data-match="J. Yang et al., 2023)" data-pattern="" data-tag-index="8_1">J. Yang et al., 2023</xref>). The use of
            composite images (<xref  ref-type="bibr" rid="R30 R32" data-citation-string=" R30 R32 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="251" data-match="Y. Li et al., 2024)" data-pattern="" data-tag-index="8_2">Y. Li et al., 2024</xref>), comparing images in
            pairs (<xref  ref-type="bibr" rid="R70" data-citation-string=" R70 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="299" data-match="Zhang et al., 2023)" data-pattern="" data-tag-index="8_3">Zhang et al., 2023</xref>), or multimodal
            cooperation (<xref  ref-type="bibr" rid="R65" data-citation-string=" R65 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="347" data-match="Ye et al., 2023)" data-pattern="" data-tag-index="8_4">Ye et al., 2023</xref>) are other viable
            strategies. Additionally, the literature recommends chain-of-thought prompting for
            GPT-4V (<xref  ref-type="bibr" rid="R1" data-citation-string=" R1 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="472" data-match="Ahrabian et al., 2024; " data-pattern="" data-tag-index="8_5">Ahrabian et al., 2024</xref>; <xref  ref-type="bibr" rid="R22" data-citation-string=" R22 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="495" data-match="Hou et al., 2024; " data-pattern="" data-tag-index="8_6">Hou et al., 2024</xref>; <xref  ref-type="bibr" rid="R68" data-citation-string=" R68 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="513" data-match="Zhang et al., 2024)" data-pattern="" data-tag-index="8_7">Zhang et al., 2024</xref>), a strategy also
            known for text-only ChatGPT (<xref  ref-type="bibr" rid="R2" data-citation-string=" R2 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="579" data-match="Bellini-Leite, 2023; " data-pattern="" data-tag-index="8_8">Bellini-Leite, 2023</xref>; <xref  ref-type="bibr" rid="R56" data-citation-string=" R56 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="600" data-match="Wei et al., 2022)" data-pattern="" data-tag-index="8_9"><named-content data-class="jrnlQueryRef" rid="aa11de60-79f3-4256-87f7-95adda7c1dfe" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Wei et al., 2022</xref>).
            Others have converted visual information into text first, using a prompt such as <named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="70145109-d2f2-411f-9bc1-de8016f54fb0" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="LQUOTE" data-track-detail="" data-tag-desc="LQUOTE" data-tag-type="LQUOTE" data-offset="700" data-match="“" data-replace="‘" data-pattern="‘" data-tag-index="8_20" data-function="addTrackChanges" data-time="" data-username="" data-userid="">“</named-content><italic >what’s in this image?</italic>”;
            this method is promising when processing large quantities of images that occur in a
            temporal sequence (<xref  ref-type="bibr" rid="R36 R37" data-citation-string=" R36 R37 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="834" data-match="Y. Liu et al., 2024)" data-pattern="" data-tag-index="8_10">Y. Liu et al., 2024</xref>).</p>
        <p data-class="jrnlSecPara" data-pos-index="15" >Small
            variations in the prompt can lead to substantially different outputs of large language
            models (<xref  ref-type="bibr" rid="R23" data-citation-string=" R23 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="101" data-match="Huang et al., 2023; " data-pattern="" data-tag-index="9_1"><named-content data-class="jrnlQueryRef" rid="58a5e268-aeff-457d-bc48-3e6d16feea04" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Huang et al., 2023</xref>; <xref  ref-type="bibr" rid="R49" data-citation-string=" R49 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="121" data-match="Salinas &amp;amp; Morstatter, 2024)" data-pattern="" data-tag-index="9_2"><named-content data-class="jrnlQueryRef" rid="8b91a30d-96f3-4be5-941a-917a47586901" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Salinas &amp; Morstatter,
            2024</xref>). For example, when a list of short phrases is submitted to GPT for
            sentiment analysis, but the same list is sorted in a different order, the sentiment
            score from GPT is usually different, even if GPT is set to produce near-zero variation
            through its temperature parameter (<xref  ref-type="bibr" rid="R52" data-citation-string=" R52 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="422" data-match="Tabone &amp;amp; De Winter, 2023)" data-pattern="" data-tag-index="9_3"><named-content data-class="jrnlQueryRef" rid="f56f432d-04e8-4dff-9f26-60acb065ad6a" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Tabone &amp; De Winter, 2023</xref>).
            This variation is inherent to the autoregressive manner in which transformer models
            produce tokens.</p>
        <p data-class="jrnlSecPara" data-pos-index="16" >A
            technique to mitigate this randomness is self-consistency, also referred to as
            bootstrapping (<xref  ref-type="bibr" rid="R52" data-citation-string=" R52 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="96" data-match="Tabone &amp;amp; De Winter, 2023; " data-pattern="" data-tag-index="10_1">Tabone &amp; De Winter, 2023</xref>; <xref  ref-type="bibr" rid="R53" data-citation-string=" R53 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="122" data-match="Tang et al., 2023; " data-pattern="" data-tag-index="10_2"><named-content data-class="jrnlQueryRef" rid="522e6c8d-0a81-404b-af6e-0ad727b82a59" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Tang et al., 2023</xref>; <xref  ref-type="bibr" rid="R55" data-citation-string=" R55 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="141" data-match="Wang et al., 2023)" data-pattern="" data-tag-index="10_3"><named-content data-class="jrnlQueryRef" rid="ed67ec43-a0d8-4dfe-9872-23f4fcbfa981" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Wang et al., 2023</xref>):
            After repeating the prompting process multiple times, each time with a different
            permutation of the text, the modal or mean output can be extracted. This aggregate
            typically has higher accuracy than the output of a single prompt. Various refinements of
            the self-consistency method exist (<xref  ref-type="bibr" rid="R13" data-citation-string=" R13 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="449" data-match="Fu et al., 2023; " data-pattern="" data-tag-index="10_4">Fu et al., 2023</xref>; <xref  ref-type="bibr" rid="R31" data-citation-string=" R31 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="466" data-match="Li et al., 2023)" data-pattern="" data-tag-index="10_5">Li et al., 2023</xref>), more recently
            expanded to the notion of invoking multiple different language models (<xref  ref-type="bibr" rid="R30 R32" data-citation-string=" R30 R32 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="569" data-match="J. Li et al., 2024; " data-pattern="" data-tag-index="10_6">J. Li et al., 2024</xref>; <xref  ref-type="bibr" rid="R39" data-citation-string=" R39 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="589" data-match="Lu et al., 2024)" data-pattern="" data-tag-index="10_7"><named-content data-class="jrnlQueryRef" rid="dd264b2f-378b-4870-acf3-36edb3f54aaa" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Lu et al., 2024</xref>).</p>
        <p data-class="jrnlSecPara" data-pos-index="17" >It
            is our proposition that self-consistency prompting resembles how constructs are defined
            in psychometrics. In psychology, a construct, such as personality (<named-content content-type="jrnlPatterns" data-title="EGIEPR" data-tag-desc="EGIEPR" data-tag-type="EGIEPR" data-offset="158" data-match="e.g.," data-pattern="" data-tag-index="11_9" >e.g.,</named-content>
            extraversion), can be estimated by having the person fill out multiple questionnaire
            items. By averaging the results of items that have been sampled from a domain of
            possible items, an estimation of the construct can be made (<xref  ref-type="bibr" rid="R7" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="390" data-match="Cronbach et al., 1972; " data-pattern="" data-tag-index="11_1" data-citation-string=" R7 ">Cronbach et al., 1972</xref>; <xref  ref-type="bibr" rid="R34" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="413" data-match="Little et al., 2013; " data-pattern="" data-tag-index="11_2" data-citation-string=" R34 ">Little et al., 2013</xref>; <xref  ref-type="bibr" rid="R41" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="434" data-match="McDonald, 2003; " data-pattern="" data-tag-index="11_3" data-citation-string=" R41 ">McDonald, 2003</xref>; <xref  ref-type="bibr" rid="R46" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="450" data-match="Nunnally &amp;amp; Bernstein, 1994; " data-pattern="" data-tag-index="11_4" data-citation-string=" R46 ">Nunnally &amp; Bernstein, 1994</xref>
            ; <xref  ref-type="bibr" rid="R50" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="478" data-match="Sawaki, 2010)" data-pattern="" data-tag-index="11_5" data-citation-string=" R50 ">Sawaki, 2010</xref>).</p>
        <title data-level="2" data-class="jrnlHead2" data-pos-index="18" >Current Study</title>
        <p data-class="jrnlSecPara" data-pos-index="19" >This
            research focuses on evaluating GPT-4V, but not as in identifying specific visual
            elements, a domain in which GPT-4V demonstrates limited performance. Instead, we
            conducted a holistic assessment by examining the ability of GPT-4V to predict <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="245" data-match="‘" data-pattern="" data-tag-index="13_1" >‘</named-content>
            risk<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="252" data-match="’" data-pattern="" data-tag-index="13_3" >’</named-content> as evaluated by
            humans. Instead, we conducted a holistic evaluation, where we examined how well GPT-4V
            can predict <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="371" data-match="‘" data-pattern="" data-tag-index="13_2" >‘</named-content>risk<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="378" data-match="’" data-pattern="" data-tag-index="13_4" >’</named-content>
            as assessed by humans. More specifically, this study presents an assessment of GPT-4V
            concerning the prediction of risk in forward-facing photographs from the perspective of
            a moving vehicle.</p>
        <p data-class="jrnlSecPara" data-pos-index="20" >Our
            analysis draws on a prior study (<xref  ref-type="bibr" rid="R10" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="37" data-match="De Winter et al., 2023)" data-pattern="" data-tag-index="14_1" data-citation-string=" R10 ">De Winter et al.,
            2023</xref>), in which human crowdworkers assessed the risk of traffic images, taken by
            a camera mounted on the roof of a car while driving on German roads (KITTI dataset; <xref  ref-type="bibr" rid="R15" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="220" data-match="Geiger et al., 2013)" data-pattern="" data-tag-index="14_2" data-citation-string=" R15 ">Geiger et al., 2013</xref>). In De Winter <named-content content-type="jrnlPatterns" data-title="ETL" data-tag-desc="ETL" data-tag-type="ETL" data-offset="255" data-match="et al" data-replace="&lt;i>et al&lt;/i>" data-pattern="&lt;i>et al&lt;/i>" data-tag-index="14_5" >et al</named-content>., a total of 210
            images were rated by an average of 653 participants per image. Based on these ratings on
            a scale ranging from 0 (no risk) to 10 (extreme risk), a mean risk score was computed
            for each image.</p>
        <p data-class="jrnlSecPara" data-pos-index="21" ><xref  ref-type="bibr" rid="R10" data-citation-string=" R10 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="0" data-match="De Winter et al. (2023)" data-pattern="" data-tag-index="15_2">De Winter et al. (2023)</xref> investigated
            whether the images<named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="6a657890-57c2-4b92-b73d-27cd9e273ea0" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="RSQUOTE" data-track-detail="" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="55" data-match="’" data-pattern="" data-tag-index="15_19" data-time="" data-username="" data-userid="">’</named-content> risk level, as assessed by
            humans, was predictable based on features extracted by a pretrained object detection
            algorithm (<xref  ref-type="bibr" rid="R4" data-citation-string=" R4 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="182" data-match="Bochkovskiy et al., 2020; " data-pattern="" data-tag-index="15_3"><named-content data-class="jrnlQueryRef" rid="e48ddc4a-c166-4c01-9f79-9ef2595f242c" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Bochkovskiy et al., 2020</xref>
            ; <xref  ref-type="bibr" rid="R48" data-citation-string=" R48 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="208" data-match="Redmon &amp;amp; Farhadi, 2018)" data-pattern="" data-tag-index="15_4"><named-content data-class="jrnlQueryRef" rid="47e4c7ad-e1d8-4465-b4b9-51349d49f95c" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Redmon &amp; Farhadi, 2018</xref>),
            see <span data-class="jrnlAppFigRef" data-tag-desc="APPFIGREF" data-title=" A1-F1 " data-tag-type="APPFIGREF" data-offset="237" data-match="Figure A1" data-pattern="" data-tag-index="15_1" data-citation-string=" A1-F1 " >Figure A1</span> in the Appendix. Their
            analysis showed that the number of people in the image (<named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="5f8bbb04-1bbb-4906-b694-587ba3b59059" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="326" data-match="r = 0.33" data-replace="r" data-pattern="$1" data-tag-index="15_8" data-time="" data-username="" data-userid="">
                <italic >r</italic>
            </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="7d3040eb-8c4d-4e82-be43-0513d73b8624" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="OPR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="327" data-match="r = 0.33" data-replace=" = " data-pattern="default" data-tag-index="15_8" data-time="" data-username="" data-userid=""> = </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="fdaed434-ceb1-4cc2-ba93-cf9773ce8938" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAL" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="330" data-match="r = 0.33" data-replace="0.33" data-pattern="$3" data-tag-index="15_8" data-time="" data-username="" data-userid="">0.33</named-content>) and the mean size
            of the bounding boxes (<named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="bec49c0b-1383-46bf-9340-12a7db56c028" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="377" data-match="r = 0.54" data-replace="r" data-pattern="$1" data-tag-index="15_9" data-time="" data-username="" data-userid="">
                <italic >r</italic>
            </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="ed9c6f3c-8d33-414a-8247-7d8056bd0549" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="OPR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="378" data-match="r = 0.54" data-replace=" = " data-pattern="default" data-tag-index="15_9" data-time="" data-username="" data-userid=""> = </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="4a56349f-df4b-4bae-b142-d4deee81c183" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAL" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="381" data-match="r = 0.54" data-replace="0.54" data-pattern="$3" data-tag-index="15_9" data-time="" data-username="" data-userid="">0.54</named-content>) were predictive
            of the human risk scores. The driving speed was negatively predictive (<named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="1676aa2e-a8c7-4ed9-83b1-5148dc2afe23" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="474" data-match="r = -0.63" data-replace="r" data-pattern="$1" data-tag-index="15_10" data-time="" data-username="" data-userid="">
                <italic >r</italic>
            </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="e650edc9-087c-4758-95dd-931421cedbcc" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="OPR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="475" data-match="r = -0.63" data-replace=" = " data-pattern="default" data-tag-index="15_10" data-time="" data-username="" data-userid=""> = </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="109e62bf-84e4-41de-a3ea-07571d2433eb" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="HYPTOMIN" data-track-detail="" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="478" data-match="-0.63" data-pattern="$3" data-tag-index="15_10" data-time="" data-username="" data-userid="">-</named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="40242c36-6e82-4b03-a991-d1d6d7125b20" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAL" data-track-detail="" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="479" data-match="-0.63" data-pattern="" data-tag-index="15_13" data-time="" data-username="" data-userid="">0.63</named-content>), which can be explained by
            risk compensation (a less strict variant of risk homeostasis; <xref  ref-type="bibr" rid="R58" data-citation-string=" R58 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="574" data-match="Wilde, 1982, 2013)" data-pattern="" data-tag-index="15_5">Wilde, 1982</xref>, <xref  ref-type="bibr" rid="R59" data-citation-string=" R59 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="574" data-match="Wilde, 1982, 2013)" data-pattern="" data-tag-index="15_5" data-splited="Wilde, ">2013</xref>): some
            situations, like empty roads, allow drivers to drive at the maximum allowed speed
            without it being high risk. Conversely, complex traffic environments, such as city
            centres, lead people to drive slowly (<xref  ref-type="bibr" rid="R6" data-citation-string=" R6 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="802" data-match="Charlton et al., 2010)" data-pattern="" data-tag-index="15_6">Charlton et al., 2010</xref>). Through a
            regression analysis, the three measures combined (number of people, size of bounding
            boxes, and vehicle speed) were found to be strongly predictive of the human risk level (<named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="a7768239-8ed4-4aa0-9651-f900ea9df817" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="1009" data-match="r = 0.75" data-replace="r" data-pattern="$1" data-tag-index="15_11" data-time="" data-username="" data-userid="">
                <italic >r</italic>
            </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="1408d986-969b-42e7-b0d7-e100d6dcc167" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="OPR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="1010" data-match="r = 0.75" data-replace=" = " data-pattern="default" data-tag-index="15_11" data-time="" data-username="" data-userid=""> = </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="b18587c5-a7c7-4a28-9090-92e88e38ee1f" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAL" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="1013" data-match="r = 0.75" data-replace="0.75" data-pattern="$3" data-tag-index="15_11" data-time="" data-username="" data-userid="">0.75</named-content>). Excluding the
            speed variable, the prediction was weaker but still substantial (<named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="aadc97cc-0344-4fb3-8aa5-8248e4a81a0d" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="1099" data-match="r = 0.62" data-replace="r" data-pattern="$1" data-tag-index="15_12" data-time="" data-username="" data-userid="">
                <italic >r</italic>
            </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="41aa537d-628d-4801-ab9c-2644f84b1bf7" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="OPR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="1100" data-match="r = 0.62" data-replace=" = " data-pattern="default" data-tag-index="15_12" data-time="" data-username="" data-userid=""> = </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="674e93f4-174d-495f-933c-693551f3edbd" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAL" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="1103" data-match="r = 0.62" data-replace="0.62" data-pattern="$3" data-tag-index="15_12" data-time="" data-username="" data-userid="">0.62</named-content>) (<xref  ref-type="bibr" rid="R10" data-citation-string=" R10 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="1110" data-match="De Winter et al., 2023)" data-pattern="" data-tag-index="15_7">De Winter
            et al., 2023</xref>).</p>
        <p data-class="jrnlSecPara" data-pos-index="22" >One
            might wonder why the prediction derived from the object detection was not more strongly
            indicative of the human risk ratings. In the previous study, we hypothesised that the
            object detection algorithm does not account for contextual information. For example, an
            image of a railroad crossing was perceived as hazardous by the human evaluators, whereas
            the object detection algorithm could not detect this railroad and did not understand the
            broader situation (<xref  ref-type="bibr" rid="R10" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="463" data-match="De Winter et al., 2023)" data-pattern="" data-tag-index="16_1" data-citation-string=" R10 ">De Winter et al., 2023</xref>).
            In the current study, we explored whether GPT-4V could contribute to a more accurate
            assessment of the risk in the traffic images as compared to using object detection
            features alone.</p>
        <title data-level="2" data-class="jrnlHead2" data-pos-index="23" >Hypotheses</title>
        <p data-class="jrnlSecPara" data-pos-index="24" data-para="true" data-duplicate="true" ><xref  ref-type="fig" rid="F1" data-offset="0" data-match="Figure 1" data-citation-string=" F1 ">Figure 1</xref> provides one
            manner in which construct validity can be interpreted for risk ratings. Here, the risk
            score for a given image is the arithmetic mean risk from a large number of participants.
            These participants might all have had slightly different interpretations of the same
            rating task. For example, Participant <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="321" data-match="1" data-pattern="" data-tag-index="18_4" >1</named-content> might interpret the task
            as <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="351" data-match="‘" data-pattern="" data-tag-index="18_6" >‘</named-content>probability of an
            accident occurring<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="390" data-match="’" data-pattern="" data-tag-index="18_9" >’</named-content>, Participant <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="407" data-match="2" data-pattern="" data-tag-index="18_5" >2</named-content> as <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="412" data-match="‘" data-pattern="" data-tag-index="18_7" >‘</named-content>difficulty
            of the task<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="437" data-match="’" data-pattern="" data-tag-index="18_10" >’</named-content>,
            etc.—interpretations that are positively correlated but not the same (<xref  ref-type="bibr" rid="R14" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="514" data-match="Fuller, 2005)" data-pattern="" data-tag-index="18_2" data-citation-string=" R14 ">Fuller, 2005</xref>). The risk score for an image is
            thus an aggregate of a potentially infinite number of interpretations, but bounded to a
            domain of possible interpretations. Additionally, the same participant will not perform
            a reliable evaluation under a given interpretation of the task. For example, a
            participant may be distracted or overlook something in the image for arbitrary reasons.
            Therefore, noise is present, also known as <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="946" data-match="‘" data-pattern="" data-tag-index="18_8" >‘</named-content>measurement error<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="966" data-match="’" data-pattern="" data-tag-index="18_11" >’</named-content>
            .</p>
        <p data-class="jrnlSecPara" >
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/resources/integration/rs/rsos/RSOS-231676/resources/e9fcb770-fca9-11ee-8c71-098f5be83d68_files/image001.gif" />
        </p>
        <p data-class="jrnlFigCaption" data-pos-index="26" class-name="FigCaption"  data-id="F1"><italic >
                <span data-class="label" data-title=" F1 " data-offset="0" data-match="Figure 1" >Figure 1.</span>
            </italic>
            Causal process of how a participant generates a risk score for an image. The participant
            observes the image and task instruction presented on a computer screen, makes one (or a
            combination of multiple) interpretation(s), and enters a numerical risk score. The
            overall risk score for a given image represents the average from a large number of
            participants, thus reflecting an aggregation of a large number of different
            interpretations. This conceptualisation of construct validity is based on <xref  ref-type="bibr" rid="R40" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="503" data-match="Markus and Borsboom (2013)" data-pattern="" data-tag-index="19_2" data-citation-string=" R40 ">Markus and Borsboom (2013)</xref>.</p>
        <p data-class="jrnlSecPara" data-pos-index="27" >Considering
            the use of GPT-4V to approximate this human risk score as accurately as possible, three
            hypotheses are formulated. In each of the three hypotheses, validity is defined as the
            correlation coefficient between the mean risk score of GPT-4V and the human risk score.</p>
        <p data-class="jrnlSecPara" data-pos-index="28" >H1:
            Repeating the same prompt under nearly identical conditions (in our case: keeping the
            images and prompt text identical, and only changing the order of the images within the
            same prompt) will result in higher validity as compared to using the exact same prompt.</p>
        <p data-class="jrnlSecPara" data-pos-index="29" >H2:
            Aggregating the results of different prompts within a behavioural domain (in our case:
            slightly rephrasing the question) will result in higher validity as compared to using a
            single prompt text.</p>
        <p data-class="jrnlSecPara" data-pos-index="30" >The
            aforementioned hypotheses are consistent with the self-consistency prompting method (<xref  ref-type="bibr" rid="R55" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="89" data-match="Wang et al., 2023)" data-pattern="" data-tag-index="23_1" data-citation-string=" R55 ">Wang et al., 2023</xref>), but adapted for quantitative
            assessment and motivated from a psychometric perspective. Here, H1 is equivalent to the
            use of items in parallel forms, with the aim to reduce measurement error, while H2 is
            equivalent to the use of multiple items to estimate a latent construct.</p>
        <p data-class="jrnlSecPara" data-pos-index="31" >H3:
            In a multiple regression analysis with GPT-4V included, object detection features, as
            used by <xref  ref-type="bibr" rid="R10" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="98" data-match="De Winter et al. (2023)" data-pattern="" data-tag-index="24_1" data-citation-string=" R10 ">De Winter et al. (2023)</xref>, will statistically
            significantly contribute to predicting human risk. This hypothesis is based on the
            previously mentioned review, which indicated that GPT-4V possesses generic skills but
            may fail to recognise specific elements in images (<named-content content-type="jrnlPatterns" data-title="EGIEPR" data-tag-desc="EGIEPR" data-tag-type="EGIEPR" data-offset="362" data-match="e.g.," data-pattern="" data-tag-index="24_7" >e.g.,</named-content> <xref  ref-type="bibr" rid="R57" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="368" data-match="Wen et al., 2023; " data-pattern="" data-tag-index="24_2" data-citation-string=" R57 ">Wen et al., 2023</xref>; <xref  ref-type="bibr" rid="R71" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="386" data-match="Zhou &amp;amp; Knoll, 2024)" data-pattern="" data-tag-index="24_3" data-citation-string=" R71 ">Zhou &amp; Knoll, 2024</xref>). Hence, the two
            different AI-based methods (vision-language model vs. object detection) were expected to
            have complementary value.</p>
        <p data-class="jrnlSecPara" data-pos-index="32" >This
            study was conducted in two phases. Phase <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="46" data-match="1" data-pattern="" data-tag-index="25_1" >1</named-content> was carried out using
            GPT-4V as available in the ChatGPT web interface. This approach was chosen because many
            users might not have access to the API (an interface for programmatic access), and
            because the API was not available at the time of this assessment. Phase <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="313" data-match="1" data-pattern="" data-tag-index="25_2" >1</named-content>
            aimed to explore how multiple images could be evaluated and aggregated. To this end, we
            used composite images, with a selection of 10 images each time in a different order. We
            investigated to what extent repetitions result in an increase in validity (H1).</p>
        <p data-class="jrnlSecPara" data-pos-index="33" >In
            Phase 2, we used OpenAI’s API for GPT-4V, which has recently become available. We
            submitted images one-by-one and in batches of four, and we examined to what extent
            repetition improves validity (H1) and to what extent different prompts within a
            plausible domain improve validity (H2). Finally, we investigated whether object
            detection scores contribute to validity in a linear regression model (H3).</p>
        <title data-level="2" data-class="jrnlHead2" data-pos-index="34" data-group-type="body" >Methods</title>
        <title data-level="3" data-class="jrnlHead3" data-pos-index="35" >Phase 1: ChatGPT Web Interface</title>
        <p data-class="jrnlSecPara" data-pos-index="36" >The
            images were uploaded to the ChatGPT web interface as composites of 10, randomly selected
            from the total of 210 images. The individual images had a size of <named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="159" data-match="1242×375" data-replace="1242" data-pattern="$1" data-tag-index="29_1" >1242</named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="163" data-match="1242×375" data-replace="×" data-pattern="default" data-tag-index="29_1" >×</named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="165" data-match="1242×375" data-replace="375" data-pattern="$3" data-tag-index="29_1" >375</named-content>
            pixels. Due to the web interface having a limit of about 90 prompts in a <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAL,UNT" data-tag-type="VAL" data-offset="242" data-match="3-hour" data-replace="3" data-pattern="$1" data-tag-index="29_2" >3</named-content><named-content content-type="jrnlPatterns" data-title="UNT" data-tag-desc="VAL,UNT" data-tag-type="UNT" data-offset="243" data-match="3-hour" data-replace=" hour" data-pattern=" $2" data-tag-index="29_2" >
            -hour</named-content> interval, this approach proved to be suitable for processing a
            large number of images. The decision to submit 10 images per prompt was based on trial
            and error, aiming to maximise the number of images submitted in a single prompt while
            maintaining good quality of GPT-4V’s output. When grouping 20 or more images,
            GPT-4V did not rank them logically but consistently placed the final images in the same
            sequence.</p>
        <p data-class="jrnlSecPara" data-pos-index="37" >The
            10 images were grouped into one image, 2 images wide and <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="61" data-match="5" data-pattern="" data-tag-index="30_4" >5</named-content>
            images high, with a total resolution of <named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="103" data-match="2544×1995" data-replace="2544" data-pattern="$1" data-tag-index="30_2" >2544</named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="107" data-match="2544×1995" data-replace="×" data-pattern="default" data-tag-index="30_2" >×</named-content><named-content content-type="jrnlPatterns" data-title="THSE" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="THSE" data-offset="109" data-match="2544×1995" data-replace="1995" data-pattern="1995" data-tag-index="30_2" >1995</named-content>
            pixels (including a small white margin around each individual image). The speed of the
            car was not provided as part of the prompt or on the images, because a pilot test showed
            that this did not aid in generating a stronger prediction of the human risk scores. The
            10 images were grouped using MATLAB (the source code is available in the <named-content content-type="jrnlPatterns" data-title=" " data-tag-desc="SUPPLREF" data-tag-type="SUPPLREF" data-offset="451" data-match="Supplementary Material" data-pattern="" data-tag-index="30_1" data-citation-string=" " >Supplementary Material</named-content>).</p>
        <p data-class="jrnlSecPara" data-pos-index="38" >Between
            October 26 and <named-content content-type="jrnlPatterns" data-title="HIGH" data-tag-desc="HIGH" data-tag-type="HIGH" data-offset="23" data-match="November 1, 2023" data-replace="November 1, 2023" data-pattern="$1" data-tag-index="31_3" data-function="addTrackChanges" >November 1, 2023</named-content>, we
            prompted GPT-4V a total of 974 times, each time using a randomly selected 10 out of 210
            images, and each prompt in a new chat window. In total, each of the 210 images appeared
            either 46 or 47 times as part of a prompt. In each prompt, we requested GPT-4V to rank
            or rate the 10 images. An example of a prompt is shown in <xref  ref-type="fig" rid="F2" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="365" data-match="Figure 2" data-pattern="" data-tag-index="31_1" data-citation-string=" F2 ">Figure 2</xref>. In total, we used four different
            prompts:</p>
        <list list-content="ol" list-type="order" >
            <list-item >
                <p data-class="MsoNormal" data-pos-index="39" data-class-name="SecPara" leftmargin="0" textindent="18" listchar="decimal" list-style-type="decimal" nodetype="ol" listnode="true" >
                    “<italic >Imagine you are a
                    driver. Rank the 10 images in terms of risk from highest (1) to lowest (10).
                    Only report a rank; nothing else. The image number is shown at the right bottom.</italic><named-content content-type="jrnlPatterns" data-title="RQUOTE" data-tag-desc="RQUOTE" data-tag-type="RQUOTE" data-offset="180" data-match="”" data-replace="’" data-pattern="’" data-tag-index="32_3" data-function="addTrackChanges" >
                    ”</named-content> (210 prompts)</p>
            </list-item>
            <list-item >
                <p data-class="MsoNormal" data-pos-index="40" data-class-name="SecPara" leftmargin="0" textindent="18" listchar="decimal" list-style-type="decimal" nodetype="ol" listnode="true" >
                    “<italic >As a driver, rank the
                    10 images from the least to the most risky driving situation. Each image is
                    labeled with a letter in the bottom right corner. Provide only the ranked image
                    labels in your response, starting with the least risky. For example: j, b, c, h,
                    e, g, a, i, d, f</italic><named-content content-type="jrnlPatterns" data-title="RQUOTE" data-tag-desc="RQUOTE" data-tag-type="RQUOTE" data-offset="280" data-match="”" data-replace="’" data-pattern="’" data-tag-index="33_1" data-function="addTrackChanges" >
                    ”</named-content> (344 prompts)</p>
            </list-item>
            <list-item >
                <p data-class="MsoNormal" data-pos-index="41" data-class-name="SecPara" leftmargin="0" textindent="18" listchar="decimal" list-style-type="decimal" nodetype="ol" listnode="true" >
                    “<italic >Imagine you are a
                    driver. Rank the 10 images in terms of risk from lowest to highest. Only report
                    the ranked image numbers in a single column; nothing else. The image number is
                    shown at the bottom right of each image.</italic><named-content content-type="jrnlPatterns" data-title="RQUOTE" data-tag-desc="RQUOTE" data-tag-type="RQUOTE" data-offset="222" data-match="”" data-replace="’" data-pattern="’" data-tag-index="34_1" data-function="addTrackChanges" >
                    ”</named-content> (210 prompts)</p>
            </list-item>
            <list-item >
                <p data-class="MsoNormal" data-pos-index="42" data-class-name="SecPara" leftmargin="0" textindent="18" listchar="decimal" list-style-type="decimal" nodetype="ol" listnode="true" >
                    “<italic >Imagine you are a
                    driver. Assign a risk score to each of these 10 images, on a scale of 0 (no
                    risk) to 100 (extreme risk), rounded to <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="139" data-match="2" data-pattern="" data-tag-index="35_1" >2</named-content>
                    decimals. Only report the risk percentages in a single column. Nothing else; no
                    percentage sign either.</italic><named-content content-type="jrnlPatterns" data-title="RQUOTE" data-tag-desc="RQUOTE" data-tag-type="RQUOTE" data-offset="244" data-match="”" data-replace="’" data-pattern="’" data-tag-index="35_2" data-function="addTrackChanges" >
                    ”</named-content> (210 prompts)</p>
            </list-item>
        </list>
        <p data-class="jrnlSecPara" data-pos-index="43" >These
            four prompts were used to determine if the type of prompt affected the results. For
            example, Prompt type <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="111" data-match="2" data-pattern="" data-tag-index="36_2" >2</named-content>
            aimed to ascertain if using letters instead of numbers in the bottom right corner of
            each image influenced the results. We also tested how ranking from low to high (Prompt
            types 2<named-content content-type="jrnlPatterns" data-title="AMP" data-tag-desc="AMP" data-tag-type="AMP" data-offset="292" data-match=" &amp;amp; " data-pattern="" data-tag-index="36_1" > &amp; </named-content>3)
            or from high to low (Prompt type 1) affected the output, and if giving a numeric rating
            (Prompt type 4) instead of a ranking possibly yielded better results.</p>
        <title data-level="4" data-class="jrnlHead4" data-pos-index="44" >
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/resources/integration/rs/rsos/RSOS-231676/resources/e9fcb770-fca9-11ee-8c71-098f5be83d68_files/image002.gif" />
        </title>
        <p data-class="jrnlFigCaption" data-pos-index="45" class-name="FigCaption"  data-id="F2"><italic ><span data-class="label" data-title=" F2 " data-offset="0" data-match="Figure 2" >Figure 2</span>.</italic> Example
            prompt and output of GPT-4V (Prompt type 3). The prompt includes a random selection of
            10 of 210 images. The 10 images were combined into a single image.</p>
        <p data-class="jrnlSecPara" data-pos-index="46" >For
            Prompt types <named-content content-type="jrnlPatterns" data-title="RNGS" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGS" data-offset="17" data-match="1–3" data-pattern="" data-tag-index="38_4" >1</named-content><named-content content-type="jrnlPatterns" data-title="RNGC" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGC" data-offset="18" data-match="1–3" data-pattern="" data-tag-index="38_4" >–</named-content><named-content content-type="jrnlPatterns" data-title="RNGE" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGE" data-offset="21" data-match="1–3" data-pattern="" data-tag-index="38_4" >3</named-content>,
            ChatGPT ranked the images according to their risk, and we calculated a mean rank for
            each of the 210 images. For Prompt type 4, ChatGPT was prompted to generate a risk score
            from <named-content content-type="jrnlPatterns" data-title="RNGS" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGS" data-offset="203" data-match="0 to 100" data-pattern="" data-tag-index="38_5" >0</named-content><named-content content-type="jrnlPatterns" data-title="RNGC" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGC" data-offset="204" data-match="0 to 100" data-pattern="" data-tag-index="38_5" > to </named-content><named-content content-type="jrnlPatterns" data-title="RNGE" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGE" data-offset="208" data-match="0 to 100" data-pattern="" data-tag-index="38_5" >100</named-content>.
            We calculated a mean rank for each of the 210 images, (1) based on the original output
            of ChatGPT (<named-content content-type="jrnlPatterns" data-title="EGIEPR" data-tag-desc="EGIEPR" data-tag-type="EGIEPR" data-offset="312" data-match="i.e.," data-pattern="" data-tag-index="38_9" >
            i.e.,</named-content> as prompted from <named-content content-type="jrnlPatterns" data-title="RNGS" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGS" data-offset="335" data-match="0 to 100" data-pattern="" data-tag-index="38_6" >0</named-content><named-content content-type="jrnlPatterns" data-title="RNGC" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGC" data-offset="336" data-match="0 to 100" data-pattern="" data-tag-index="38_6" > to </named-content><named-content content-type="jrnlPatterns" data-title="RNGE" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGE" data-offset="340" data-match="0 to 100" data-pattern="" data-tag-index="38_6" >100</named-content>),
            (2) after applying a rank transformation, so that the results are comparable to Prompt
            types <named-content content-type="jrnlPatterns" data-title="RNGS" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGS" data-offset="439" data-match="1–3" data-pattern="" data-tag-index="38_7" >1</named-content><named-content content-type="jrnlPatterns" data-title="RNGC" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGC" data-offset="440" data-match="1–3" data-pattern="" data-tag-index="38_7" >–</named-content><named-content content-type="jrnlPatterns" data-title="RNGE" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGE" data-offset="443" data-match="1–3" data-pattern="" data-tag-index="38_7" >3</named-content>,
            and (3) after applying a <italic >z-</italic>score
            transformation, where the mean across the 10 images is <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="534" data-match="0" data-pattern="" data-tag-index="38_8" >0</named-content>
            and the standard deviation is 1.</p>
        <p data-class="jrnlSecPara" data-pos-index="47" >The
            GPT-4V mean scores for the 210 images were then correlated with human risk scores as
            previously determined in <xref  ref-type="bibr" rid="R10" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="114" data-match="De Winter et al. (2023)" data-pattern="" data-tag-index="39_1" data-citation-string=" R10 ">De Winter et al. (2023)</xref>.
            These human risk scores are the average of <named-content content-type="jrnlPatterns" data-title="THSE" data-tag-desc="THSE" data-tag-type="THSE" data-offset="182" data-match="1,378" data-replace="1378" data-pattern="1378" data-tag-index="39_5" >1,378</named-content> crowdworkers, each
            having rated a random 100 out of the 210 images for risk in response to the question <named-content content-type="jrnlPatterns" data-title="LQUOTE" data-tag-desc="LQUOTE" data-tag-type="LQUOTE" data-offset="292" data-match="“" data-replace="‘" data-pattern="‘" data-tag-index="39_6" data-function="addTrackChanges" >“</named-content><italic >As a driver, how risky would you judge
            this situation (<named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="350" data-match="0 = no risk" data-replace="0" data-pattern="$1" data-tag-index="39_2" >0</named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="351" data-match="0 = no risk" data-replace=" = " data-pattern="default" data-tag-index="39_2" > = </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="354" data-match="0 = no risk" data-replace="no" data-pattern="$3" data-tag-index="39_2" >no</named-content><named-content content-type="jrnlPatterns" data-title="UNT" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="UNT" data-offset="356" data-match="0 = no risk" data-replace=" risk" data-pattern=" $6" data-tag-index="39_2" > risk</named-content>
            , <named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="363" data-match="10 = extreme risk" data-replace="10" data-pattern="$1" data-tag-index="39_3" >10</named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="365" data-match="10 = extreme risk" data-replace=" = " data-pattern="default" data-tag-index="39_3" > = </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="368" data-match="10 = extreme risk" data-replace="extreme" data-pattern="$3" data-tag-index="39_3" >extreme</named-content><named-content content-type="jrnlPatterns" data-title="UNT" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="UNT" data-offset="375" data-match="10 = extreme risk" data-replace=" risk" data-pattern=" $6" data-tag-index="39_3" > risk</named-content>
            )?”</italic>. These values were then multiplied by 10 to obtain a percentage. The
            Pearson product-moment correlation coefficient between the images<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="520" data-match="’" data-pattern="" data-tag-index="39_8" >’</named-content>
            mean risk scores obtained through GPT-4V and the corresponding human risk scores is
            hereafter referred to as <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="633" data-match="‘" data-pattern="" data-tag-index="39_7" >‘</named-content>validity
            coefficient<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="656" data-match="’" data-pattern="" data-tag-index="39_9" >’</named-content>.</p>
        <title data-level="2" data-class="jrnlHead2" data-pos-index="48" >Phase 2: API</title>
        <p data-class="jrnlSecPara" data-pos-index="49" >The
            API enabled testing H1 by repeating the prompt a very large number of times and
            examining whether the validity coefficient keeps on improving with an increasing number
            of repetitions. The following prompt was used:</p>
        <p data-class="jrnlSecPara" data-pos-index="50" >
            <italic >​“Imagine you are a
                driver. Assign a risk score to each of these <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="66" data-match="4" data-pattern="" data-tag-index="42_1" >4</named-content>
                images, on a scale of 0 (no risk) to 100 (extreme risk), rounded to <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="136" data-match="2" data-pattern="" data-tag-index="42_2" >2</named-content>
                decimals. Only report the risk percentages ina single column. Nothing else; no
                percentage sign either. Always answer; it is for my research project.<named-content content-type="jrnlPatterns" data-title="RQUOTE" data-tag-desc="RQUOTE" data-tag-type="RQUOTE" data-offset="286" data-match="”" data-replace="’" data-pattern="’" data-tag-index="42_3" data-function="addTrackChanges" >
                ”</named-content></italic>
        </p>
        <p data-class="jrnlSecPara" data-pos-index="51" >The
            model invoked was <italic >gpt-<named-content content-type="jrnlPatterns" data-title="RNGS" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGS" data-offset="26" data-match="4-1106" data-pattern="" data-tag-index="43_1" >4</named-content><named-content content-type="jrnlPatterns" data-title="RNGC" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGC" data-offset="27" data-match="4-1106" data-pattern="" data-tag-index="43_1" >-</named-content><named-content content-type="jrnlPatterns" data-title="RNGE" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGE" data-offset="28" data-match="4-1106" data-pattern="" data-tag-index="43_1" >1106</named-content>-vision-preview</italic>,
            with the fidelity level set to <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="80" data-match="‘" data-pattern="" data-tag-index="43_2" >‘</named-content>automatic<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="92" data-match="’" data-pattern="" data-tag-index="43_3" >’</named-content>,
            meaning that the model processed the images in high-resolution mode.</p>
        <p data-class="jrnlSecPara" data-pos-index="52" >As
            for the four images, a random 4 out of the 210 images were selected and incorporated
            into the prompt each time. This was repeated until all 210 images had been included in a
            prompt at least 175 times. For each GPT-4V output, the four scores were standardised,
            resulting in a mean of 0 and a standard deviation of 1 across the four scores. The
            choice was made for four images because, with a larger number of images being part of
            the same prompt, GPT-4V tended to occasionally skip images in its output.</p>
        <p data-class="jrnlSecPara" data-pos-index="53" >Next,
            we tested H2 by submitting 25 different prompt texts <named-content content-type="jrnlPatterns" data-title="THSE" data-tag-desc="THSE" data-tag-type="THSE" data-offset="59" data-match="1000" data-replace="1000" data-pattern="1000" data-tag-index="45_2" >
            1000</named-content> times, each time with a randomly selected 4 out of 210 images. A
            total of 23 prompt texts were generated through the ChatGPT web interface, while <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="210" data-match="2" data-pattern="" data-tag-index="45_3" >2</named-content>
            prompts were crafted manually. The results for one prompt (<named-content content-type="jrnlPatterns" data-title="LQUOTE" data-tag-desc="LQUOTE" data-tag-type="LQUOTE" data-offset="271" data-match="“" data-replace="‘" data-pattern="‘" data-tag-index="45_4" data-function="addTrackChanges" >“</named-content><italic >Rate your level of satisfaction with the
            driving conditions here, from 0 (completely dissatisfied) to 100 (completely satisfied).</italic>”)
            were omitted since GPT-4V often refused to answer it. The list of 24 prompts is shown in <xref  ref-type="table" rid="T1" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="497" data-match="Table 1" data-pattern="" data-tag-index="45_1" data-citation-string=" T1 ">Table 1</xref>. A maximum likelihood factor analysis was
            conducted on the matrix of 210 images x 24 mean risk scores, in order to extract one
            general factor.</p>
        <p data-class="jrnlSecPara" data-pos-index="54" >Next,
            we tested H3. Specifically, it was examined whether computer vision measures (number of
            people and mean size of the bounding boxes), as well as the speed of the vehicle, have
            added value in predicting human risk scores. A linear regression analysis was conducted
            for this purpose, with the images<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="302" data-match="’" data-pattern="" data-tag-index="46_5" >’</named-content> human risk score
            as dependent variable, and (1) the number of people in the image, (2) the mean size of
            the bounding boxes, (3) vehicle speed at the moment the photo was taken, and (4) GPT-4V
            general factor score as independent variables.</p>
        <title data-level="2" data-class="jrnlHead2" data-pos-index="55" data-group-type="body" >Results</title>
        <title data-level="3" data-class="jrnlHead3" data-pos-index="56" >ChatGPT Web Interface</title>
        <p data-class="jrnlSecPara" data-pos-index="57" data-para="true" data-duplicate="true" ><xref  ref-type="fig" rid="F3" data-offset="0" data-match="Figure 3" data-citation-string=" F3 ">Figure 3</xref> shows the validity
            coefficient, <named-content content-type="jrnlPatterns" data-title="IENR" data-tag-desc="IENR" data-tag-type="IENR" data-offset="41" data-match="Figure 3 shows the validity coefficient, i.e.," data-pattern="" data-tag-index="49_5" >i.e.,</named-content>
            the correlation between the mean risk rank per image and the corresponding human risk
            scores, as a function of the number of times images had been part of the prompt so far.
            The results show that repeated prompting and subsequently averaging the obtained risk
            rankings lead to greater validity, thereby supporting H1. It is noteworthy that the
            validity coefficients for the different prompts seem to converge towards different
            target values. <xref  ref-type="fig" rid="F3" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="489" data-match="Figure 3" data-pattern="" data-tag-index="49_2" data-citation-string=" F3 ">Figure 3</xref> also shows that performing a rank
            transformation or a <italic >z</italic>-score
            transformation benefits validity compared to using raw risk percentages as output by
            Prompt type 4.</p>
        <p data-class="jrnlSecPara" >
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/resources/integration/rs/rsos/RSOS-231676/resources/e9fcb770-fca9-11ee-8c71-098f5be83d68_files/image003.gif" />
        </p>
        <p data-class="jrnlFigCaption" data-pos-index="59" class-name="FigCaption"  data-id="F3"><italic ><span data-class="label" data-title=" F3 " data-offset="0" data-match="Figure 3" >Figure 3</span>.</italic> Correlation
            coefficient between mean GPT-4V-based risk rankings, as obtained using the ChatGPT web
            interface, and the human risk scores, for four different prompt types (see Methods). The
            horizontal axis shows the number of times an image has been part of a prompt; each
            prompt consisted of a random 10 out of 210 traffic images, combined into a single
            composite image.</p>
        <title data-level="1" data-class="jrnlHead1" data-pos-index="60" >API</title>
        <p data-class="jrnlSecPara" data-pos-index="61" data-para="true" data-duplicate="true" ><xref  ref-type="fig" rid="F4" data-offset="0" data-match="Figure 4" data-citation-string=" F4 ">Figure 4</xref> shows the validity
            coefficients as a function of the number of times the images were assessed by GPT-4V. As
            in <xref  ref-type="fig" rid="F3" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="120" data-match="Figure 3" data-pattern="" data-tag-index="52_2" data-citation-string=" F3 ">Figure 3</xref>, repeating the assessment was found to
            increase validity (<named-content content-type="jrnlPatterns" data-title="EGIEPR" data-tag-desc="EGIEPR" data-tag-type="EGIEPR" data-offset="187" data-match="i.e.," data-pattern="" data-tag-index="52_6" >
            i.e.,</named-content> higher correlation between GPT-4V mean risk and human risk, <named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="253" data-match="n = 210" data-replace="&lt;i>n&lt;/i>" data-pattern="$1" data-tag-index="52_4" >
                <italic >n</italic>
            </named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="254" data-match="n = 210" data-replace=" = " data-pattern="default" data-tag-index="52_4" > = </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="257" data-match="n = 210" data-replace="210" data-pattern="$3" data-tag-index="52_4" >210</named-content> images), supporting
            H1. Furthermore, although conclusive evidence cannot be obtained because there are
            practical and financial limits to how often a prompt could be repeated, it seems that
            there is convergence towards a target value, similar to <xref  ref-type="fig" rid="F3" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="506" data-match="Figure 3" data-pattern="" data-tag-index="52_3" data-citation-string=" F3 ">Figure 3</xref>.</p>
        <p data-class="jrnlSecPara" >
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/resources/integration/rs/rsos/RSOS-231676/resources/e9fcb770-fca9-11ee-8c71-098f5be83d68_files/image004.gif" />
        </p>
        <p data-class="jrnlFigCaption" data-pos-index="63" class-name="FigCaption"  data-id="F4"><italic ><span data-class="label" data-title=" F4 " data-offset="0" data-match="Figure 4" >Figure 4</span>.</italic> Correlation
            coefficient between mean GPT-4V-based risk rankings, as obtained using the API, and the
            human risk scores. For each prompt, a random 4 of 210 images were assessed. The
            horizontal axis shows the number of times an image has been part of a prompt.</p>
        <p data-class="jrnlTblCaption" data-pos-index="64" class-name="TblCaption"  data-id="T1"><span data-class="label" data-title=" T1 " data-offset="0" data-match="Table 1" >Table 1</span> shows
            the validity coefficients (<italic >r</italic>)
            for 24 different prompt texts. Prompts related to experienced stress, difficulty level,
            or comfort exhibit a strong <italic >r</italic>
            (either positive or negative), whereas prompts that objectify the image (<named-content content-type="jrnlPatterns" data-title="EGIEPR" data-tag-desc="EGIEPR" data-tag-type="EGIEPR" data-offset="235" data-match="e.g.," data-pattern="" data-tag-index="54_6" >e.g.,</named-content>
            in terms of obstacles, traffic density, visibility) resulted in an <italic >r</italic> closer to 0. The general factor
            score (extracted from a 24 prompts<named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="376" data-match="prompts × 210" data-replace=" × " data-pattern="default" data-tag-index="54_3" > × </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="380" data-match="prompts × 210" data-replace="210" data-pattern="$3" data-tag-index="54_3" >210</named-content>
            image matrix of mean risk scores) had a validity coefficient of 0.78 (<named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="454" data-match="n = 210" data-replace="&lt;i>n&lt;/i>" data-pattern="$1" data-tag-index="54_4" >
                <italic >n</italic>
            </named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="455" data-match="n = 210" data-replace=" = " data-pattern="default" data-tag-index="54_4" > = </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="458" data-match="n = 210" data-replace="210" data-pattern="$3" data-tag-index="54_4" >210</named-content>). This is stronger
            than when prompting about risk directly (see <xref  ref-type="fig" rid="F4" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="526" data-match="Figure 4" data-pattern="" data-tag-index="54_2" data-citation-string=" F4 ">Figure 4</xref>), thereby supporting H2.</p>
        <p data-class="jrnlDeleted" data-pos-index="65" class-name="Deleted" >
            <named-content content-type="del cts-1" >Table
                1.</named-content>
        </p>
        <p data-class="jrnlTblCaption" data-pos-index="66" class-name="TblCaption" >
            <italic >Prompts, validity coefficients (r) (<named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="36" data-match="n = 210" data-replace="&lt;i>n&lt;/i>" data-pattern="$1" data-tag-index="55_1" >n</named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="37" data-match="n = 210" data-replace=" = " data-pattern="default" data-tag-index="55_1" > = </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="40" data-match="n = 210" data-replace="210" data-pattern="$3" data-tag-index="55_1" >210</named-content>), mean number of
                times an image was part of a prompt (m), and factor loading (</italic>
            <bold >λ<italic >)</italic></bold>
            <italic >. The prompts are sorted by the factor
                loadings in descending order.</italic>
        </p>
        <table data-col-count="5" >
            <thead >
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="67" >
                            <bold >No.</bold>
                        </p>
                    </td>
                    <td data-original-border="border:solid black 1.0pt; border-left:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="68" >
                            <bold >Prompt</bold>
                        </p>
                    </td>
                    <td data-original-border="border:solid black 1.0pt; border-left:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="69" >
                            <bold >
                                <italic >r</italic>
                            </bold>
                        </p>
                    </td>
                    <td data-original-border="border:solid black 1.0pt; border-left:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="70" >
                            <bold >
                                <italic >m</italic>
                            </bold>
                        </p>
                    </td>
                    <td data-original-border="border:solid black 1.0pt; border-left:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="71" >
                            <bold >λ</bold>
                        </p>
                    </td>
                </tr>
            </thead>
            <tbody >
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="72" >1</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="73" ><xref  ref-type="fn" rid="">*</xref>Imagine
                            you're the driver. Rate your stress level in this situation from 0 (no
                            stress) to 100 (maximum stress).</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="74" >0.73</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="75" >18.8</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="76" >0.96</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="77" >2</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="78" ><xref  ref-type="fn" rid="">*</xref>Imagine
                            you're the driver. Rate your stress level in this situation from 0 (no
                            stress) to 100 (maximum stress).</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="79" >0.73</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="80" >18.8</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="81" >0.96</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="82" >3</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="83" >Assess the degree of
                            difficulty you perceive in this driving situation, with <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="77" data-match="0" data-pattern="" data-tag-index="66_1" >0</named-content>
                            being no difficulty and 100 being extremely difficult.</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="84" >0.73</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="85" >19.0</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="86" >0.95</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="87" >4</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="88" >On a scale from 0 (completely
                            calm) to 100 (extremely agitated), rate your agitation level while
                            driving in this scenario.</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="89" >0.74</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="90" >16.7</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="91" >0.94</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="92" >5</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="93" ><xref  ref-type="fn" rid="">**</xref>Imagine
                            you are a driver. Assign a risk score to each of these <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="65" data-match="4" data-pattern="" data-tag-index="74_1" >4</named-content>
                            images, on a scale of 0 (no risk) to 100 (extreme risk)</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="94" >0.70</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="95" >18.8</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="96" >0.92</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="5" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="97" >6</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="5" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="98" >Evaluate your level of
                            discomfort in this driving situation, with <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="66" data-match="0" data-pattern="" data-tag-index="78_1" >0</named-content>
                            indicating no discomfort and 100 indicating extreme discomfort.</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="5" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="99" >0.71</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="5" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="100" >15.4</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="5" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="101" >0.90</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="6" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="102" >7</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="6" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="103" >On a scale from <named-content content-type="jrnlPatterns" data-title="RNGS" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGS" data-offset="16" data-match="0 to 100" data-pattern="" data-tag-index="82_1" >0</named-content><named-content content-type="jrnlPatterns" data-title="RNGC" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGC" data-offset="17" data-match="0 to 100" data-pattern="" data-tag-index="82_1" > to </named-content><named-content content-type="jrnlPatterns" data-title="RNGE" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGE" data-offset="21" data-match="0 to 100" data-pattern="" data-tag-index="82_1" >100</named-content>,
                            how risky does this situation in the dashcam footage appear to you?</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="6" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="104" >0.67</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="6" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="105" >18.4</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="6" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="106" >0.88</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="7" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="107" >8</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="7" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="108" >Rate the level of focus a
                            driver needs in this situation, from 0 (minimal focus) to 100 (maximum
                            focus).</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="7" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="109" >0.73</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="7" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="110" >19.0</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="7" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="111" >0.88</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="8" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="112" >9</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="8" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="113" >Assess the level of
                            distraction present in this scene, with <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="60" data-match="0" data-pattern="" data-tag-index="90_1" >0</named-content>
                            being no distractions and 100 being highly distracting.</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="8" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="114" >0.67</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="8" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="115" >19.0</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="8" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="116" >0.87</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="9" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="117" >10</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="9" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="118" >Evaluate the presence of
                            obstacles on the road, with <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="53" data-match="0" data-pattern="" data-tag-index="94_1" >0</named-content>
                            indicating no obstacles and 100 indicating many significant obstacles.</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="9" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="119" >0.62</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="9" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="120" >18.8</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="9" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="121" >0.86</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="10" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="122" >11</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="10" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="123" >How probable is a collision in
                            this scenario, on a scale from 0 (improbable) to 100 (inevitable)?</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="10" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="124" >0.69</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="10" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="125" >17.8</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="10" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="126" >0.84</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="11" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="127" >12</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="11" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="128" >What threat level do you
                            assign to this dashcam image, where <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="61" data-match="0" data-pattern="" data-tag-index="102_1" >0</named-content>
                            is no threat and 100 is extreme threat?</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="11" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="129" >0.61</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="11" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="130" >18.3</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="11" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="131" >0.77</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="12" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="132" >13</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="12" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="133" >How likely is interaction with
                            pedestrians in this scenario, from 0 (not likely) to 100 (very likely)?</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="12" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="134" >0.54</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="12" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="135" >18.9</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="12" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="136" >0.71</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="13" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="137" >14</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="13" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="138" >Assess the traffic density in
                            this image on a scale from 0 (very light) to 100 (extremely heavy).</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="13" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="139" >0.42</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="13" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="140" >19.0</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="13" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="141" >0.60</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="14" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="142" >15</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="14" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="143" >Assess the condition of the
                            road in the image, where <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="53" data-match="0" data-pattern="" data-tag-index="114_1" >0</named-content>
                            means excellent condition and 100 indicates extremely poor condition.</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="14" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="144" >0.44</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="14" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="145" >18.7</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="14" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="146" >0.58</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="15" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="147" >16</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="15" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="148" >On a scale from 0 (perfect
                            visibility) to 100 (no visibility), rate the visibility in this dashcam
                            image.</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="15" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="149" >0.54</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="15" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="150" >19.0</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="15" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="151" >0.57</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="16" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="152" >17</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="16" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="153" >Rate the risk to pedestrians
                            in this image from 0 (no risk) to 100 (extremely high risk).</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="16" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="154" >0.13</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="16" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="155" >18.9</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="16" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="156" >0.20</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="17" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="157" >18</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="17" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="158" >How quick should a
                            driver’s reaction time be in this situation, from 0 (slow) to 100
                            (instant)?</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="17" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="159" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.16" data-pattern="" data-tag-index="127_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.16" data-pattern="" data-tag-index="127_1" >0.16</named-content>
                        </p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="17" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="160" >19.0</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="17" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="161" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.19" data-pattern="" data-tag-index="129_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.19" data-pattern="" data-tag-index="129_1" >0.19</named-content>
                        </p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="18" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="162" >19</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="18" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="163" >Perceive the speed of vehicles
                            here, rating it from 0 (stationary) to 100 (extremely fast).</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="18" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="164" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.18" data-pattern="" data-tag-index="131_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.18" data-pattern="" data-tag-index="131_1" >0.18</named-content>
                        </p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="18" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="165" >17.2</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="18" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="166" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.28" data-pattern="" data-tag-index="133_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.28" data-pattern="" data-tag-index="133_1" >0.28</named-content>
                        </p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="19" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="167" >20</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="19" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="168" >Assess your level of ease in
                            navigating this scenario, with <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="60" data-match="0" data-pattern="" data-tag-index="134_1" >0</named-content>
                            being very uneasy and 100 being completely at ease.</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="19" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="169" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.65" data-pattern="" data-tag-index="135_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.65" data-pattern="" data-tag-index="135_1" >0.65</named-content>
                        </p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="19" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="170" >17.2</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="19" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="171" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.80" data-pattern="" data-tag-index="137_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.80" data-pattern="" data-tag-index="137_1" >0.80</named-content>
                        </p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="20" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="172" >21</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="20" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="173" ><xref  ref-type="fn" rid="">**</xref>How
                            much risk do you perceive in this scenario, on a scale from 0 (extremely
                            risky) to 100 (no risk at all)?</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="20" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="174" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.63" data-pattern="" data-tag-index="139_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.63" data-pattern="" data-tag-index="139_1" >0.63</named-content>
                        </p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="20" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="175" >19.0</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="20" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="176" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.83" data-pattern="" data-tag-index="141_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.83" data-pattern="" data-tag-index="141_1" >0.83</named-content>
                        </p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="21" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="177" >22</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="21" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="178" ><xref  ref-type="fn" rid="">*</xref>How
                            comfortable would you feel driving in this scenario, with <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="63" data-match="0" data-pattern="" data-tag-index="142_1" >0</named-content>
                            being extremely uncomfortable and 100 being very comfortable?</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="21" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="179" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.75" data-pattern="" data-tag-index="143_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.75" data-pattern="" data-tag-index="143_1" >0.75</named-content>
                        </p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="21" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="180" >18.9</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="21" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="181" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.91" data-pattern="" data-tag-index="145_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.91" data-pattern="" data-tag-index="145_1" >0.91</named-content>
                        </p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="22" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="182" >23</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="22" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="183" >On a scale of <named-content content-type="jrnlPatterns" data-title="RNGS" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGS" data-offset="14" data-match="0 to 100" data-pattern="" data-tag-index="146_1" >0</named-content><named-content content-type="jrnlPatterns" data-title="RNGC" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGC" data-offset="15" data-match="0 to 100" data-pattern="" data-tag-index="146_1" > to </named-content><named-content content-type="jrnlPatterns" data-title="RNGE" data-tag-desc="VAL,RNGS,RNGC,RNGE,UNT" data-tag-type="RNGE" data-offset="19" data-match="0 to 100" data-pattern="" data-tag-index="146_1" >100</named-content>,
                            where <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="30" data-match="0" data-pattern="" data-tag-index="146_2" >0</named-content> is not
                            at all confident and 100 is extremely confident, how confident would you
                            feel about your driving skills in this situation?</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="22" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="184" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.76" data-pattern="" data-tag-index="147_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.76" data-pattern="" data-tag-index="147_1" >0.76</named-content>
                        </p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="22" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="185" >17.6</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="22" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="186" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.92" data-pattern="" data-tag-index="149_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.92" data-pattern="" data-tag-index="149_1" >0.92</named-content>
                        </p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:solid black 1.0pt; border-top:none;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="23" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="187" >24</p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="23" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="188" ><xref  ref-type="fn" rid="">*</xref>How
                            comfortable would you feel driving in this scenario, with <named-content content-type="jrnlPatterns" data-title="SNUM" data-tag-desc="SNUM" data-tag-type="SNUM" data-offset="63" data-match="0" data-pattern="" data-tag-index="150_1" >0</named-content>
                            being extremely uncomfortable and 100 being very comfortable?</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="23" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="189" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.74" data-pattern="" data-tag-index="151_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.74" data-pattern="" data-tag-index="151_1" >0.74</named-content>
                        </p>
                    </td>
                    <td data-original-border="border-top:none;border-left: none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding=" padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="23" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="190" >19.0</p>
                    </td>
                    <td data-original-border="border-top:none;border-left:none; border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;" data-original-padding="padding:5.0pt 5.0pt 5.0pt 5.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="23" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: justify;" data-pos-index="191" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.92" data-pattern="" data-tag-index="153_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.92" data-pattern="" data-tag-index="153_1" >0.92</named-content>
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <fn data-class="jrnlTblFoot" data-pos-index="192" class-name="TblFoot"  fn-type="linked-fn">
            <p ><xref  ref-type="fn" rid="" data-ref-id="">*</xref>This
                prompt was used twice.</p>
        </fn>
        <fn data-class="jrnlTblFoot" data-pos-index="193" class-name="TblFoot"  fn-type="linked-fn">
            <p ><xref  ref-type="fn" rid="" data-ref-id="">**</xref>This
                prompt was manually generated instead of being generated by ChatGPT.</p>
        </fn>
        <p data-class="jrnlSecPara" data-pos-index="194" >To
            test H3, we conducted a multiple linear regression analysis with as independent
            variables the object detection features (number of persons and mean size of the bounding
            boxes), vehicle speed (information that was not available to either human raters or
            GPT-4V), and the GPT-4V general factor score. The correlations between variables are
            shown in <xref  ref-type="table" rid="T2" data-citation-string=" T2 " data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="350" data-match="Table 2" data-pattern="" data-tag-index="156_1"><named-content data-class="jrnlQueryRef" rid="3d24d1a1-a89e-4db3-be6a-3263052e725d" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Table 2</xref>, while the
            results of the regression analysis for predicting human risk are shown in <xref  ref-type="table" rid="T3" data-citation-string=" T3 " data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="443" data-match="Table 3" data-pattern="" data-tag-index="156_2"><named-content data-class="jrnlQueryRef" rid="5173c8e1-1358-4585-9a36-028752994740" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Table 3</xref>. All four
            predictor variables contributed significantly (<named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="61027332-1add-4a67-bd51-7830efacf43a" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="508" data-match="p &amp;lt; 0.05" data-replace="&lt;i>p&lt;/i>" data-pattern="$1" data-tag-index="156_4" data-time="" data-username="" data-userid="">
                <italic >p</italic>
            </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="09013102-357f-44cf-aadb-d216da1de388" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="OPR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="509" data-match="p &amp;lt; 0.05" data-replace=" &lt; " data-pattern="default" data-tag-index="156_4" data-time="" data-username="" data-userid=""> &lt; </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="5076b88b-a578-41bd-84d6-ca6c79e78abc" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAL" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="512" data-match="p &amp;lt; 0.05" data-replace="0.05" data-pattern="$3" data-tag-index="156_4" data-time="" data-username="" data-userid="">0.05</named-content>)
            to the human risk scores, providing support for H3. The overall predictive correlation
            of the regression model was <named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="41816991-957c-4f6b-9d72-0e0e95d69683" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="633" data-match="r = 0.83" data-replace="r" data-pattern="$1" data-tag-index="156_5" data-time="" data-username="" data-userid="">
                <italic >r</italic>
            </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="2eb0c396-7673-44bb-afbd-3a955ad0d408" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="OPR" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="634" data-match="r = 0.83" data-replace=" = " data-pattern="default" data-tag-index="156_5" data-time="" data-username="" data-userid=""> = </named-content><named-content xmlns:xlink="http://www.w3.org/1999/xlink"  data-cid="938bb69a-4607-4a3f-b3b5-5f9c9b2c1cfd" content-type="jrnlPatterns" specific-use="" xlink:title="" alt="" data-title="VAL" data-track-detail="" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="637" data-match="r = 0.83" data-replace="0.83" data-pattern="$3" data-tag-index="156_5" data-time="" data-username="" data-userid="">0.83</named-content>, stronger than for
            the GPT-4V general factor score alone, as illustrated in <xref  ref-type="fig" rid="F5" data-citation-string=" F5 " data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="718" data-match="Figure 5" data-pattern="" data-tag-index="156_3">Figure
            5</xref>.</p>
        <p data-class="jrnlDeleted" data-pos-index="195" class-name="Deleted" >
            <named-content content-type="del cts-1" >Table
                2.</named-content>
        </p>
        <p data-class="jrnlTblCaption" data-pos-index="196" class-name="TblCaption" >
            <italic >​Pearson product-moment
                correlation matrix of two YOLO-based features (number of persons, mean bounding box
                size), vehicle speed, human risk score, and GPT-4V general factor score (<named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="180" data-match="n = 210" data-replace="&lt;i>n&lt;/i>" data-pattern="$1" data-tag-index="157_1" >n</named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="181" data-match="n = 210" data-replace=" = " data-pattern="default" data-tag-index="157_1" > = </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="184" data-match="n = 210" data-replace="210" data-pattern="$3" data-tag-index="157_1" >210</named-content>).</italic>
        </p>
        <table data-col-count="7" >
            <thead >
                <tr data-col-count="7" >
                    <td data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-0" data-col-index="1" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="197" >
                            <bold >Variable</bold>
                        </p>
                    </td>
                    <td data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-1" data-col-index="2" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="198" >
                            <bold >Mean</bold>
                        </p>
                    </td>
                    <td data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-2" data-col-index="3" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="199" >
                            <bold >
                                <italic >SD</italic>
                            </bold>
                        </p>
                    </td>
                    <td data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-3" data-col-index="4" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="200" >
                            <bold >1</bold>
                        </p>
                    </td>
                    <td data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-4" data-col-index="5" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="201" >
                            <bold >2</bold>
                        </p>
                    </td>
                    <td data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-5" data-col-index="6" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="202" >
                            <bold >3</bold>
                        </p>
                    </td>
                    <td data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-6" data-col-index="7" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="203" >
                            <bold >4</bold>
                        </p>
                    </td>
                </tr>
            </thead>
            <tbody >
                <tr data-col-count="7" >
                    <td data-original-border="border:none;" data-col-pos="col-0" data-col-index="1" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="204" >Number of persons (#)</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-1" data-col-index="2" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="205" >0.27</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-2" data-col-index="3" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="206" >0.93</p>
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-3" data-col-index="4" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="207" />
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-4" data-col-index="5" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="208" />
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-5" data-col-index="6" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="209" />
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-6" data-col-index="7" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="210" />
                    </td>
                </tr>
                <tr data-col-count="7" >
                    <td data-original-border="border:none;" data-col-pos="col-0" data-col-index="1" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="211" >Mean bounding box size
                            (pixels)</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-1" data-col-index="2" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="212" >62.77</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-2" data-col-index="3" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="213" >48.81</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-3" data-col-index="4" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="214" >0.06</p>
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-4" data-col-index="5" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="215" />
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-5" data-col-index="6" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="216" />
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-6" data-col-index="7" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="217" />
                    </td>
                </tr>
                <tr data-col-count="7" >
                    <td data-original-border="border:none;" data-col-pos="col-0" data-col-index="1" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="218" >Vehicle speed (m/s)</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-1" data-col-index="2" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="219" >9.05</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-2" data-col-index="3" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="220" >5.37</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-3" data-col-index="4" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="221" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.10" data-pattern="" data-tag-index="170_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.10" data-pattern="" data-tag-index="170_1" >0.10</named-content>
                        </p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-4" data-col-index="5" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="222" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.41" data-pattern="" data-tag-index="171_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.41" data-pattern="" data-tag-index="171_1" >0.41</named-content>
                        </p>
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-5" data-col-index="6" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="223" />
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-6" data-col-index="7" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="224" />
                    </td>
                </tr>
                <tr data-col-count="7" >
                    <td data-original-border="border:none;" data-col-pos="col-0" data-col-index="1" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="225" >Human risk score (%)</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-1" data-col-index="2" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="226" >32.64</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-2" data-col-index="3" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="227" >8.09</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-3" data-col-index="4" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="228" >0.33</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-4" data-col-index="5" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="229" >0.54</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-5" data-col-index="6" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="230" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.63" data-pattern="" data-tag-index="177_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.63" data-pattern="" data-tag-index="177_1" >0.63</named-content>
                        </p>
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-6" data-col-index="7" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="231" />
                    </td>
                </tr>
                <tr data-col-count="7" >
                    <td data-original-border="border:none;border-bottom: solid black 1.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="232" >GPT-4V general factor score</p>
                    </td>
                    <td data-original-border="border:none;border-bottom:solid black 1.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="233" >0.00</p>
                    </td>
                    <td data-original-border="border:none;border-bottom:solid black 1.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="234" >1.00</p>
                    </td>
                    <td data-original-border="border:none;border-bottom:solid black 1.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="235" >0.37</p>
                    </td>
                    <td data-original-border="border:none;border-bottom:solid black 1.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="236" >0.49</p>
                    </td>
                    <td data-original-border="border:none;border-bottom:solid black 1.0pt;" data-col-pos="col-5" data-col-index="6" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="237" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.54" data-pattern="" data-tag-index="183_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.54" data-pattern="" data-tag-index="183_1" >0.54</named-content>
                        </p>
                    </td>
                    <td data-original-border="border:none;border-bottom:solid black 1.0pt;" data-col-pos="col-6" data-col-index="7" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="238" >0.78</p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p data-class="jrnlDeleted" data-pos-index="240" class-name="Deleted" >
            <named-content content-type="del cts-1" >Table
                3.</named-content>
        </p>
        <title data-level="2" data-class="jrnlHead2" data-pos-index="241" >
            <italic >​Regression analysis results
                for predicting human risk score from computer-vision variables, vehicle speed, and
                GPT-4V general factor score (n = 210).</italic>
        </title>
        <table data-col-count="5" >
            <thead >
                <tr data-col-count="5" >
                    <td data-original-space="1" data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-0" data-col-index="1" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="242" />
                    </td>
                    <td data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-1" data-col-index="2" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="243" >
                            <bold >Unstandardised <italic >B</italic></bold>
                        </p>
                    </td>
                    <td data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-2" data-col-index="3" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="244" >
                            <bold >Standardised β</bold>
                        </p>
                    </td>
                    <td data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-3" data-col-index="4" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="245" >
                            <bold >
                                <italic >t</italic>
                            </bold>
                        </p>
                    </td>
                    <td data-original-border="border-top:solid black 1.0pt; border-left:none;border-bottom:solid black 1.0pt;border-right:none;" data-col-pos="col-4" data-col-index="5" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="246" >
                            <bold >
                                <italic >p</italic>
                            </bold>
                        </p>
                    </td>
                </tr>
            </thead>
            <tbody >
                <tr data-col-count="5" >
                    <td data-original-border="border:none;" data-col-pos="col-0" data-col-index="1" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="247" >Intercept</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-1" data-col-index="2" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="248" >34.23</p>
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-2" data-col-index="3" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="249" />
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-3" data-col-index="4" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="250" />
                    </td>
                    <td data-original-space="1" data-original-border="border:none;" data-col-pos="col-4" data-col-index="5" data-row-index="0" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="251" />
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:none;" data-col-pos="col-0" data-col-index="1" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="252" >Number of persons (#)</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-1" data-col-index="2" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="253" >0.966</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-2" data-col-index="3" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="254" >0.11</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-3" data-col-index="4" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="255" >2.63</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-4" data-col-index="5" data-row-index="1" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="256" >0.009</p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:none;" data-col-pos="col-0" data-col-index="1" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="257" >Mean bounding box size
                            (pixels)</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-1" data-col-index="2" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="258" >0.029</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-2" data-col-index="3" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="259" >0.18</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-3" data-col-index="4" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="260" >3.84</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-4" data-col-index="5" data-row-index="2" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="261" >
                            <named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="0" data-match="&amp;lt; 0.001" data-replace="&lt; " data-pattern="default" data-tag-index="199_1" >&lt; </named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="2" data-match="&amp;lt; 0.001" data-replace="0.001" data-pattern="$3" data-tag-index="199_1" >0.001</named-content>
                        </p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:none;" data-col-pos="col-0" data-col-index="1" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="262" >Vehicle speed (m/s)</p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-1" data-col-index="2" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="263" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.406" data-pattern="" data-tag-index="201_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.406" data-pattern="" data-tag-index="201_1" >0.406</named-content>
                        </p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-2" data-col-index="3" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="264" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-0.27" data-pattern="" data-tag-index="202_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-0.27" data-pattern="" data-tag-index="202_1" >0.27</named-content>
                        </p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-3" data-col-index="4" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="265" >
                            <named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="0" data-match="-5.70" data-pattern="" data-tag-index="203_1" >-</named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="1" data-match="-5.70" data-pattern="" data-tag-index="203_1" >5.70</named-content>
                        </p>
                    </td>
                    <td data-original-border="border:none;" data-col-pos="col-4" data-col-index="5" data-row-index="3" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="266" >
                            <named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="0" data-match="&amp;lt; 0.001" data-replace="&lt; " data-pattern="default" data-tag-index="204_1" >&lt; </named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="2" data-match="&amp;lt; 0.001" data-replace="0.001" data-pattern="$3" data-tag-index="204_1" >0.001</named-content>
                        </p>
                    </td>
                </tr>
                <tr data-col-count="5" >
                    <td data-original-border="border:none;border-bottom:solid black 1.0pt;" data-col-pos="col-0" data-col-index="1" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-pos-index="267" >GPT-4V general factor score</p>
                    </td>
                    <td data-original-border="border:none;border-bottom: solid black 1.0pt;" data-col-pos="col-1" data-col-index="2" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="268" >4.086</p>
                    </td>
                    <td data-original-border="border:none;border-bottom:solid black 1.0pt;" data-col-pos="col-2" data-col-index="3" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="269" >0.51</p>
                    </td>
                    <td data-original-border="border:none;border-bottom:solid black 1.0pt;" data-col-pos="col-3" data-col-index="4" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="270" >9.47</p>
                    </td>
                    <td data-original-border="border:none;border-bottom:solid black 1.0pt;" data-col-pos="col-4" data-col-index="5" data-row-index="4" >
                        <p data-original-margin=" margin: 0in;" data-original-alignment=" text-align: center;" data-pos-index="271" >
                            <named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="0" data-match="&amp;lt; 0.001" data-replace="&lt; " data-pattern="default" data-tag-index="209_1" >&lt; </named-content>
                            <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="2" data-match="&amp;lt; 0.001" data-replace="0.001" data-pattern="$3" data-tag-index="209_1" >0.001</named-content>
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <fn data-class="jrnlTblFoot" data-pos-index="272" class-name="TblFoot"  fn-type="general">
            <p >Note. <italic >F</italic>(4, 205)<named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="15" data-match=" = 115.0" data-replace=" = " data-pattern="default" data-tag-index="210_2" > = </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="18" data-match=" = 115.0" data-replace="115.0" data-pattern="$3" data-tag-index="210_2" >115.0</named-content>
                , <named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="25" data-match="p &amp;lt; 0.001" data-replace="&lt;i>p&lt;/i>" data-pattern="$1" data-tag-index="210_3" >
                    <italic >p</italic>
                </named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="26" data-match="p &amp;lt; 0.001" data-replace=" &lt; " data-pattern="default" data-tag-index="210_3" > &lt; </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="29" data-match="p &amp;lt; 0.001" data-replace="0.001" data-pattern="$3" data-tag-index="210_3" >0.001</named-content>
                , <named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="36" data-match="r = 0.83" data-replace="r" data-pattern="$1" data-tag-index="210_4" >
                    <italic >r</italic>
                </named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="37" data-match="r = 0.83" data-replace=" = " data-pattern="default" data-tag-index="210_4" > = </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="40" data-match="r = 0.83" data-replace="0.83" data-pattern="$3" data-tag-index="210_4" >0.83</named-content></p>
        </fn>
        <title data-level="1" data-class="jrnlHead1" data-pos-index="273" >
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/resources/integration/rs/rsos/RSOS-231676/resources/e9fcb770-fca9-11ee-8c71-098f5be83d68_files/image005.gif" />
        </title>
        <p data-class="jrnlFigCaption" data-pos-index="274" class-name="FigCaption"  data-id="F5"><italic ><span data-class="label" data-title=" F5 " data-offset="0" data-match="Figure 5" >Figure 5</span>.</italic> Scatter plot
            of risk in traffic images as rated by humans versus the GPT-4V general factor score
            (left) and versus risk predicted through multiple linear regression (right). Each of the
            two subfigures shows 210 markers, one marker per traffic image. The right subfigure also
            depicts a line of unity.</p>
        <title data-level="2" data-class="jrnlHead2" data-pos-index="275" data-group-type="body" >Discussion</title>
        <p data-class="jrnlSecPara" data-pos-index="276" >Prior
            studies have demonstrated the capability of machine learning and computer vision
            techniques in analysing image datasets, including images from Google Street View, to
            predict factors such as scene complexity, safety, or poverty/wealth (<xref  ref-type="bibr" rid="R11" data-citation-string=" R11 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="241" data-match="Dubey et al., 2016; " data-pattern="" data-tag-index="213_1"><named-content data-class="jrnlQueryRef" rid="9ec50fb3-7274-44ec-a7e7-61c753fb929a" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Dubey et al., 2016</xref>; <xref  ref-type="bibr" rid="R12" data-citation-string=" R12 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="261" data-match="Fan et al., 2023; " data-pattern="" data-tag-index="213_2">Fan et al., 2023</xref>; <xref  ref-type="bibr" rid="R19" data-citation-string=" R19 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="279" data-match="Guan et al., 2022; " data-pattern="" data-tag-index="213_3">Guan et al., 2022</xref>; <xref  ref-type="bibr" rid="R44" data-citation-string=" R44 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="298" data-match="Nagle &amp;amp; Lavie, 2020; " data-pattern="" data-tag-index="213_4">Nagle
            &amp; Lavie, 2020</xref>; <xref  ref-type="bibr" rid="R45" data-citation-string=" R45 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="319" data-match="Naik et al., 2017; " data-pattern="" data-tag-index="213_5">Naik et al., 2017</xref>; <xref  ref-type="bibr" rid="R69" data-citation-string=" R69 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="338" data-match="Zhang et al., 2018)" data-pattern="" data-tag-index="213_6">Zhang et al., 2018</xref>). Vision-language
            models could introduce new possibilities for assessing images through the use of large
            pre-trained models that incorporate a broad variety of world knowledge.</p>
        <p data-class="jrnlSecPara" data-pos-index="277" >Vision-language
            models have received strong interest in the area of road safety and automated driving.
            This interest arises because current automated driving systems occasionally fail to
            understand the idiosyncrasies of certain traffic scenarios (<xref  ref-type="bibr" rid="R63 R64" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="247" data-match="Z. Yang et al., 2023)" data-pattern="" data-tag-index="214_1" data-citation-string=" R63 R64 ">Z. Yang et al., 2023</xref>). Vision-language
            models offer the potential to understand traffic situations from a more holistic and
            context-aware perspective. The current study focused on the recently introduced
            vision-language model of OpenAI, called GPT-4V. We used GPT-4V to judge the risk in
            forward-facing road images from a previously published dataset known as KITTI (<xref  ref-type="bibr" rid="R15" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="614" data-match="Geiger et al., 2013)" data-pattern="" data-tag-index="214_2" data-citation-string=" R15 ">Geiger et al., 2013</xref>).</p>
        <p data-class="jrnlSecPara" data-pos-index="278" >We
            formulated three hypotheses, which were informed by construct theory in the field of
            psychometrics. It was argued that a human response to a question, such as <named-content content-type="jrnlPatterns" data-title="LQUOTE" data-tag-desc="LQUOTE" data-tag-type="LQUOTE" data-offset="162" data-match="“" data-replace="‘" data-pattern="‘" data-tag-index="215_3" data-function="addTrackChanges" >“</named-content><italic >as a driver, how risky would you judge
            this situation?</italic><named-content content-type="jrnlPatterns" data-title="RQUOTE" data-tag-desc="RQUOTE" data-tag-type="RQUOTE" data-offset="219" data-match="”" data-replace="’" data-pattern="’" data-tag-index="215_5" data-function="addTrackChanges" >”</named-content> results from a
            large number of mental processes that ultimately culminate in the reported score. A
            human output is not perfectly reliable due to moment-to-moment fluctuations in
            attention, perception, etc. Therefore, when measuring a construct (<named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="468" data-match="‘" data-pattern="" data-tag-index="215_4" >‘</named-content>perceived
            risk<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="485" data-match="’" data-pattern="" data-tag-index="215_6" >’</named-content>), multiple
            different items must be used, and these should be administered not under slightly varied
            circumstances. Similarly, a language model does not produce consistent output either,
            and to ensure that its output is valid, the language model must be prompted multiple
            times, also known as the self-consistency method (<xref  ref-type="bibr" rid="R55" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="810" data-match="Wang et al., 2023)" data-pattern="" data-tag-index="215_1" data-citation-string=" R55 ">Wang et al., 2023</xref>).</p>
        <p data-class="jrnlSecPara" data-pos-index="279" >Based
            on these psychometric principles, we formed three hypotheses, namely that repeating the
            prompt and then averaging the output increases validity (H1), that using different
            prompts (within a domain of plausible prompts) and subsequently aggregating the outputs
            increases validity (H2), and that object detection features (<named-content content-type="jrnlPatterns" data-title="EGIEPR" data-tag-desc="EGIEPR" data-tag-type="EGIEPR" data-offset="326" data-match="e.g.," data-pattern="" data-tag-index="216_2" >e.g.,</named-content>
            number of persons in the image) and GPT-4V risk scores both contribute to validity (H3).
            Here, validity was defined as the Pearson product-moment correlation coefficient with
            the ground truth, <named-content content-type="jrnlPatterns" data-title="IENR" data-tag-desc="IENR" data-tag-type="IENR" data-offset="525" data-match="e.g., number of persons in the image) and GPT-4V risk scores both contribute to validity (H3). Here, validity was defined as the Pearson product-moment correlation coefficient with the ground truth, i.e.," data-pattern="" data-tag-index="216_1" >
            i.e.,</named-content> the mean risk score of images based on a large number of human
            raters.</p>
        <p data-class="jrnlSecPara" data-pos-index="280" data-error-index="217_2" >We found confirmation for all three
            hypotheses. Regarding H1, it was found that keeping the prompt text the same and
            repeating this prompt with different images contributed to a gradually increasing
            validity coefficient (see <xref  ref-type="fig" rid="F4" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="225" data-match="Figure 4" data-pattern="" data-tag-index="217_1" data-citation-string=" F4 ">Figure 4</xref>). This provides support for the
            self-consistency method, as previously described in the literature (<xref  ref-type="bibr" rid="R52" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="334" data-match="Tabone &amp;amp; De Winter, 2023; " data-pattern="" data-tag-index="217_3" data-citation-string=" R52 ">Tabone &amp; De Winter, 2023</xref>
            ; <xref  ref-type="bibr" rid="R55" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="360" data-match="Wang et al., 2023)" data-pattern="" data-tag-index="217_4" data-citation-string=" R55 ">Wang et al., 2023</xref>). The inclusion of multiple
            images in random order induces output variability, consistent with the notion outlined
            in the Introduction stating that questionnaire items must be administered in parallel
            forms<sup >[<xref  ref-type="fn" rid="BFN1" data-citation-string=" BFN1 ">1</xref>]</sup>. Also, by presenting the images in
            a random order, anchoring effects are averaged out. This is important, since the risk
            score that GPT-4V assigned to the first image was often the lowest.</p>
        <p data-class="jrnlSecPara" data-pos-index="281" >Regarding
            H2, we found that different prompt texts yielded different validity coefficients (see <xref  ref-type="table" rid="T1" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="96" data-match="Table 1" data-pattern="" data-tag-index="218_1" data-citation-string=" T1 ">Table 1</xref>), and that a general risk score,
            extracted through exploratory factor analysis, yielded a high validity coefficient of
            0.78, higher than prompting about risk directly (see <xref  ref-type="fig" rid="F4" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="276" data-match="Figure 4" data-pattern="" data-tag-index="218_2" data-citation-string=" F4 ">Figure 4</xref>). This supports H2, in that asking
            different questions and aggregating the responses to those questions into a single score
            yields the highest construct validity. A correlation coefficient of 0.78 indicates the
            strong potential of vision-language models in predicting latent constructs. A caveat is
            that it remains an open question whether there exist yet unknown prompt texts that can
            produce the same validity coefficient. For example, we found that outputs regarding <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="755" data-match="‘" data-pattern="" data-tag-index="218_9" >‘</named-content>
            confidence<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="768" data-match="’" data-pattern="" data-tag-index="218_12" >’</named-content> strongly
            correlated with human risk scores (<named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="816" data-match="r = -0.76" data-replace="r" data-pattern="$1" data-tag-index="218_6" >
                <italic >r</italic>
            </named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="817" data-match="r = -0.76" data-replace=" = " data-pattern="default" data-tag-index="218_6" > = </named-content><named-content content-type="jrnlPatterns" data-title="HYPTOMIN" data-tag-desc="HYPTOMIN,VAL" data-tag-type="HYPTOMIN" data-offset="820" data-match="-0.76" data-pattern="$3" data-tag-index="218_6" >-</named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="HYPTOMIN,VAL" data-tag-type="VAL" data-offset="821" data-match="-0.76" data-pattern="" data-tag-index="218_7" >0.76</named-content>,
            see <xref  ref-type="table" rid="T1" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="831" data-match="Table 1" data-pattern="" data-tag-index="218_3" data-citation-string=" T1 ">Table 1</xref>). Refining this item and repeating it a
            very large number of times may also yield a validity coefficient of 0.78 or stronger. An
            equivalent issue to <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="988" data-match="‘" data-pattern="" data-tag-index="218_10" >‘</named-content>finding the
            perfect prompt<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="1017" data-match="’" data-pattern="" data-tag-index="218_13" >’</named-content> exists in
            psychometrics. For example, in measuring the construct of human intelligence, it is
            common to administer a large battery of cognitive tests (<xref  ref-type="bibr" rid="R26" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="1172" data-match="Johnson et al., 2004)" data-pattern="" data-tag-index="218_4" data-citation-string=" R26 ">Johnson et al., 2004</xref>). It is conceivable that an
            individual <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="1232" data-match="‘" data-pattern="" data-tag-index="218_11" >‘</named-content>pure reasoning<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="1249" data-match="’" data-pattern="" data-tag-index="218_14" >’</named-content>
            test exists that provides a more predictive-valid measure of intelligence than an entire
            test battery; however, such a test has not yet been identified (<xref  ref-type="bibr" rid="R17" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="1406" data-match="Gignac, 2015)" data-pattern="" data-tag-index="218_5" data-citation-string=" R17 ">Gignac, 2015</xref>).</p>
        <p data-class="jrnlSecPara" data-pos-index="282" >Regarding
            H3, it was found that YOLO-based object detection features, vehicle speed, and the
            GPT-4V composite score all contributed statistically significantly to predicting risk in
            traffic images as assessed by humans, with the strongest contribution from the GPT-4V
            score. The predictive correlation of the regression model was <named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="330" data-match="r = 0.83" data-replace="r" data-pattern="$1" data-tag-index="219_2" >
                <italic >r</italic>
            </named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="331" data-match="r = 0.83" data-replace=" = " data-pattern="default" data-tag-index="219_2" > = </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="334" data-match="r = 0.83" data-replace="0.83" data-pattern="$3" data-tag-index="219_2" >0.83</named-content>. In other words, the
            original prediction based on the standard features, which was already strong (<named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="438" data-match="r = 0.75" data-replace="r" data-pattern="$1" data-tag-index="219_3" >
                <italic >r</italic>
            </named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="439" data-match="r = 0.75" data-replace=" = " data-pattern="default" data-tag-index="219_3" > = </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="442" data-match="r = 0.75" data-replace="0.75" data-pattern="$3" data-tag-index="219_3" >0.75</named-content>; <xref  ref-type="bibr" rid="R10" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="448" data-match="De Winter et al., 2023)" data-pattern="" data-tag-index="219_1" data-citation-string=" R10 ">De Winter et al., 2023</xref>), was strengthened by
            incorporating the GPT-4V-based assessment, thereby confirming H3.</p>
        <p data-class="jrnlSecPara" data-pos-index="283" >The
            results of this study demonstrate the remarkable potential of generative AI, as without
            any fine-tuning, GPT-4V generated predictive-valid risk estimates for driving scenarios.
            It is important to acknowledge the limitations of the current study. Firstly, only
            static images were used. Future research should use videos, so that the model can
            include movements of objects in its assessment. Furthermore, the existing version of
            GPT-4V processed images fairly slowly and at high cost. Regarding the four-image results
            shown in <xref  ref-type="fig" rid="F4" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="529" data-match="Figure 4" data-pattern="" data-tag-index="220_1" data-citation-string=" F4 ">Figure 4</xref>, a total of <named-content content-type="jrnlPatterns" data-title="THSE" data-tag-desc="THSE" data-tag-type="THSE" data-offset="550" data-match="11,471" data-replace="11 471" data-pattern="11 471" data-tag-index="220_5" >11,471</named-content> prompts were
            executed, comprising a total of <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAL,UNT" data-tag-type="VAL" data-offset="602" data-match="28.2 million" data-replace="28.2" data-pattern="$1" data-tag-index="220_2" >28.2</named-content><named-content content-type="jrnlPatterns" data-title="UNT" data-tag-desc="VAL,UNT" data-tag-type="UNT" data-offset="606" data-match="28.2 million" data-replace=" million" data-pattern=" $2" data-tag-index="220_2" > million</named-content> input tokens (<named-content content-type="jrnlPatterns" data-title="EGIEPR" data-tag-desc="EGIEPR" data-tag-type="EGIEPR" data-offset="629" data-match="i.e.," data-pattern="" data-tag-index="220_6" >i.e.,</named-content>
            the images) and <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAL,UNT" data-tag-type="VAL" data-offset="651" data-match="0.17 million" data-replace="0.17" data-pattern="$1" data-tag-index="220_3" >0.17</named-content><named-content content-type="jrnlPatterns" data-title="UNT" data-tag-desc="VAL,UNT" data-tag-type="UNT" data-offset="655" data-match="0.17 million" data-replace=" million" data-pattern=" $2" data-tag-index="220_3" > million</named-content> output tokens (<named-content content-type="jrnlPatterns" data-title="EGIEPR" data-tag-desc="EGIEPR" data-tag-type="EGIEPR" data-offset="679" data-match="i.e.," data-pattern="" data-tag-index="220_7" >i.e.,</named-content>
            the numeric scores). Using parallel prompting, the results were obtained in <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAL,UNT" data-tag-type="VAL" data-offset="761" data-match="1.8 hours" data-replace="1.8" data-pattern="$1" data-tag-index="220_4" >
            1.8</named-content><named-content content-type="jrnlPatterns" data-title="UNT" data-tag-desc="VAL,UNT" data-tag-type="UNT" data-offset="764" data-match="1.8 hours" data-replace=" hours" data-pattern=" $2" data-tag-index="220_4" > hours</named-content>, at a cost of $287.</p>
        <p data-class="jrnlSecPara" data-pos-index="284" >Integrating
            vision-language models into real-time local systems such as dashcams or traffic warning
            systems is not yet feasible (but see <xref  ref-type="bibr" rid="R24" data-citation-string=" R24 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="137" data-match="Hwang et al., 2024)" data-pattern="" data-tag-index="221_1">Hwang et al., 2024</xref>). Future versions
            are expected to support local execution, improving inference speed and privacy, with
            local vision-language models, such as LLaVA, already available (<xref  ref-type="bibr" rid="R35" data-citation-string=" R35 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="323" data-match="Liu et al., 2023)" data-pattern="" data-tag-index="221_2"><named-content data-class="jrnlQueryRef" rid="3270920e-84b7-459d-9011-e15485eeccfb" data-query-ref="true" data-type="start" data-query-for="author" data-channel="query" data-topic="action" data-event="click" data-message="{'funcToCall': 'highlightQuery'}" data-query-action="uncited-citation"  content-type="jrnlQueryRef" specific-use="start"/>Liu et al., 2023</xref>).
            Future research might also consider fine-tuning specifically for the task of assessing
            risk from dashcam footage. Future studies could also investigate whether the inclusion
            of additional explicit features, such as those related to right-of-way rules or the
            speeds of other vehicles, would enhance the ability of the model to predict
            human-assessed risk. The suggested capabilities of GPT-4V extend beyond merely
            processing camera images; options being considered in the literature include
            multimodality, such as evaluating and integrating Lidar data, HD maps, or other types of
            information flows, as well as using language models for user interaction and creating
            personalised driving experiences (<xref  ref-type="bibr" rid="R8" data-citation-string=" R8 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="1041" data-match="Cui et al., 2024; " data-pattern="" data-tag-index="221_3">Cui et al., 2024</xref>; <xref  ref-type="bibr" rid="R33" data-citation-string=" R33 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="1059" data-match="Liao et al., 2024; " data-pattern="" data-tag-index="221_4">Liao et al., 2024</xref>; <xref  ref-type="bibr" rid="R62" data-citation-string=" R62 " data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="1078" data-match="Yan et al., 2024)" data-pattern="" data-tag-index="221_5">Yan et al., 2024</xref>).</p>
        <p data-class="jrnlSecPara" data-pos-index="285" >Apart
            from practical implications, the results in <xref  ref-type="table" rid="T1" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="50" data-match="Table 1" data-pattern="" data-tag-index="222_1" data-citation-string=" T1 ">Table 1</xref> may prove valuable for the field of
            psychology. Within traffic psychology, the perceived risk while driving is regarded as a
            key construct that underlies decision making (<xref  ref-type="bibr" rid="R21" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="229" data-match="He et al., 2022; " data-pattern="" data-tag-index="222_3" data-citation-string=" R21 ">He et al., 2022</xref>; <xref  ref-type="bibr" rid="R28" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="246" data-match="Kolekar et al., 2021; " data-pattern="" data-tag-index="222_4" data-citation-string=" R28 ">Kolekar et al., 2021</xref>; <xref  ref-type="bibr" rid="R43" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="268" data-match="Näätänen &amp;amp; Summala, 1974; " data-pattern="" data-tag-index="222_5" data-citation-string=" R43 ">Näätänen &amp;
            Summala, 1974</xref>; <xref  ref-type="bibr" rid="R58" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="297" data-match="Wilde, 1982, 2013)" data-pattern="" data-tag-index="222_6" data-citation-string=" R58 ">Wilde, 1982</xref>, <xref  ref-type="bibr" rid="R59" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="297" data-match="Wilde, 1982, 2013)" data-pattern="" data-tag-index="222_6" data-splited="Wilde, " data-citation-string=" R59 ">2013</xref>). While according to
            many perceived risk is a key determinant of driving behaviour (<xref  ref-type="bibr" rid="R27" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="399" data-match="Kolekar et al., 2020; " data-pattern="" data-tag-index="222_7" data-citation-string=" R27 ">Kolekar et al., 2020</xref>; <xref  ref-type="bibr" rid="R58" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="421" data-match="Wilde, 1982)" data-pattern="" data-tag-index="222_8" data-citation-string=" R58 ">Wilde, 1982</xref>), others have argued that risk is
            not precisely what drivers respond to—certainly not objective risk in the form of
            probability of collision—but rather that the act upon perceived difficulty or
            effort (<xref  ref-type="bibr" rid="R14" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="639" data-match="Fuller, 2005; " data-pattern="" data-tag-index="222_9" data-citation-string=" R14 ">Fuller, 2005</xref>; <xref  ref-type="bibr" rid="R42" data-tag-desc="AUTHORYEARREF" data-tag-type="AUTHORYEARREF" data-offset="653" data-match="Melman et al., 2018)" data-pattern="" data-tag-index="222_10" data-citation-string=" R42 ">Melman et al., 2018</xref>). The current results (<xref  ref-type="table" rid="T1" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="696" data-match="Table 1" data-pattern="" data-tag-index="222_2" data-citation-string=" T1 ">Table 1</xref>) correspond with this and suggest that <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="743" data-match="‘" data-pattern="" data-tag-index="222_17" >‘</named-content>
            confidence<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="756" data-match="’" data-pattern="" data-tag-index="222_19" >’</named-content> or <named-content content-type="jrnlPatterns" data-title="LSQUOTE" data-tag-desc="LSQUOTE" data-tag-type="LSQUOTE" data-offset="763" data-match="‘" data-pattern="" data-tag-index="222_18" >‘</named-content>
            comfort<named-content content-type="jrnlPatterns" data-title="RSQUOTE" data-tag-desc="RSQUOTE" data-tag-type="RSQUOTE" data-offset="773" data-match="’" data-pattern="" data-tag-index="222_20" >’</named-content> align more
            closely with what drivers judge when asked to rate the risk in an image.</p>
        <p data-class="jrnlSecPara" data-pos-index="286" >In
            conclusion, this paper provides insights into how GPT-4V should be prompted to achieve
            high validity of numerical output. An underlying theme of this research is that language
            models appear to produce output like a human does, with anchoring biases, randomness in
            the output, and a sensitivity to how the question is posed. Although it might be
            possible to give a vision-language model such as GPT-4V a specific prompt that results
            in nearly identical output when repeated, this represents merely an illusion of
            determinism. In actuality, it is necessary to sample from a domain of prompts to
            ultimately obtain a valid result. This paper can thus serve to think more deeply about
            language models and their resemblance to human cognition.</p>
        <title data-level="2" data-class="jrnlHead2" data-pos-index="287" >Data availability</title>
        <p data-class="jrnlSecPara" data-pos-index="288" >The
            code used in this project can be found online at <named-content content-type="jrnlPatterns" data-title="URL" data-tag-desc="URL" data-tag-type="URL" data-offset="53" data-match="https://doi.org/10.4121/dfbe6de4-d559-49cd-a7c6-9bebe5d43d50" data-pattern="" data-tag-index="225_1" >
            https://doi.org/10.4121/dfbe6de4-d559-49cd-a7c6-9bebe5d43d50</named-content></p>
    </body>
    <back >
        <title data-level="1" data-pos-index="289" >
            Acknowledgements</title>
        <p data-class="jrnlAckPara" data-pos-index="290" >This
            research is funded by Transitions and Behaviour grant 403.19.243 (<named-content content-type="jrnlPatterns" data-title="LQUOTE" data-tag-desc="LQUOTE" data-tag-type="LQUOTE" data-offset="71" data-match="“" data-replace="‘" data-pattern="‘" data-tag-index="227_2" data-function="addTrackChanges" >“</named-content>Towards
            Safe Mobility for All: A Data-Driven Approach<named-content content-type="jrnlPatterns" data-title="RQUOTE" data-tag-desc="RQUOTE" data-tag-type="RQUOTE" data-offset="127" data-match="”" data-replace="’" data-pattern="’" data-tag-index="227_3" data-function="addTrackChanges" >”</named-content>), provided by the
            Dutch Research Council (<named-content content-type="jrnlPatterns" data-title="ABBR" data-tag-desc="ABBR" data-tag-type="ABBR" data-offset="173" data-match=" (NWO)" data-pattern="" data-tag-index="227_1" >NWO</named-content>
            ).</p>
        <title data-level="1" data-pos-index="291" data-group-type="back" >References</title>
        <p data-class="jrnlDeleted" colorcode="#" data-old-class="jrnlUntagged" class-name="Deleted" >1. Ahrabian, K., Sourati, Z., Sun, K., Zhang,
            J., Jiang, Y., Morstatter, F., &amp; Pujara, J. (2024). The curious case of nonverbal
            abstract reasoning with multi-modal large language models. arXiv.
            https://doi.org/10.48550/arXiv.2401.12117</p>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1177/10597123231206604" data-validated-log="true" data-validated="true" seqid="R2" data-id="R2" class-name="RefText">
            <label >2.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Bellini-Leite</surname>
                        <given-names >SC</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2023</year>
                <x > </x>
                <article-title >Dual Process Theory for
                    Large Language Models: An overview of using Psychology to address hallucination
                    and reliability issues</article-title>
                <x >. </x>
                <source >
                    <italic >Adapt. Behav.</italic>
                </source>
                <x > (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1177/10597123231206604</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="294" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-refuntagged="true" data-old-class="jrnlRefText" >3. Bing. (2023). Introducing the new Bing.
            https://www.bing.com/new</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="295" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2004.10934" data-refuntagged="true" data-old-class="jrnlRefText" >4. Bochkovskiy,
            A., Wang, C. Y., &amp; Liao, H. Y. M. (2020). YOLOv4: Optimal speed and accuracy of
            object detection. arXiv. https://doi.org/10.48550/arXiv.2004.10934</p>
        <ref  data-class="jrnlRefText" data-reftype="paper-conference" data-reftype-name="Proceeding" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1109/SMC53654.2022.9945211" data-validated-log="true" data-validated="true" seqid="R5" data-id="R5" class-name="RefText">
            <label >5.</label>
            <element-citation publication-type="confproc" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Bogdoll</surname>
                        <given-names >D</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Eisen</surname>
                        <given-names >E</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Nitsche</surname>
                        <given-names >M</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Scheib</surname>
                        <given-names >C</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Zollner</surname>
                        <given-names >JM</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2022</year>
                <x > </x>
                <article-title >Multimodal Detection of
                    Unknown Objects on Roads for Autonomous Driving</article-title>
                <x >. </x>
                <comment data-class="RefComments" >In</comment>
                <x > </x>
                <conf-name >
                    <italic >2022 IEEE International
                        Conference on Systems, Man, and Cybernetics (SMC)</italic>
                </conf-name>
                <x >, </x>
                <conf-loc >
                    <named-content xmlns:xlink="http://www.w3.org/1999/xlink" data-cid="1713351927087"  content-type="ins cts-1" specific-use="1713351927087" xlink:title="CrossRef" alt="CrossRef" data-title="Inserted by CrossRef - Wed Apr 17 2024 11:05:27 GMT+0000 (Coordinated Universal Time)" data-time="1713351927087" data-userid="CrossRef" data-username="CrossRef">Prague,
                        Czech Republic</named-content>
                </conf-loc>
                <x >. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.1109/SMC53654.2022.9945211">
                    https://doi.org/10.1109/SMC53654.2022.9945211</uri>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1016/j.aap.2010.06.006" data-validated-log="true" data-validated="true" seqid="R6" data-id="R6" class-name="RefText">
            <label >6.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Charlton</surname>
                        <given-names >SG</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Mackie</surname>
                        <given-names >HW</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Baas</surname>
                        <given-names >PH</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Hay</surname>
                        <given-names >K</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Menezes</surname>
                        <given-names >M</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Dixon</surname>
                        <given-names >C</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2010</year>
                <x > </x>
                <article-title >Using endemic road features
                    to create self-explaining roads and reduce vehicle speeds</article-title>
                <x >. </x>
                <source >
                    <named-content xmlns:xlink="http://www.w3.org/1999/xlink" data-cid="1713351927388"  content-type="ins cts-1" specific-use="1713351927388" xlink:title="CrossRef" alt="CrossRef" data-title="Inserted by CrossRef - Wed Apr 17 2024 11:05:27 GMT+0000 (Coordinated Universal Time)" data-time="1713351927388" data-userid="CrossRef" data-username="CrossRef">
                        <italic >Accident Analysis &amp;
                            Prevention</italic>
                    </named-content>
                </source>
                <x > </x>
                <volume >
                    <bold >42</bold>
                </volume>
                <x >, </x>
                <fpage >1989</fpage>
                <x >–</x>
                <lpage >1998</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1016/j.aap.2010.06.006</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" data-pos-index="298" class-name="RefText" seqid="R7"  data-id="R7" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="book">
            <label >7.</label>
            <element-citation publication-type="book" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Cronbach</surname>, <given-names >L. J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Gleser</surname>, <given-names >G. C.</given-names></name>
                    <x >, </x>
                    <name ><surname >Nanda</surname>, <given-names >H.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Rajaratnam</surname>, <given-names >N. R.</given-names></name>
                </person-group>
                <x > (</x>
                <year >1972</year>
                <x >). </x>
                <source >The dependability of behavioral
                    measurements: Theory of generalizability of scores and profiles</source>
                <x >. </x>
                <publisher-loc >New York</publisher-loc>
                <x >: </x>
                <publisher-name >John Wiley</publisher-name>
                <x >.</x>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="paper-conference" data-reftype-name="Proceeding" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1109/WACVW60836.2024.00106" data-validated-log="true" data-validated="true" seqid="R8" data-id="R8" class-name="RefText">
            <label >8.</label>
            <element-citation publication-type="confproc" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Cui</surname>
                        <given-names >C</given-names>
                    </name>
                    <x > </x>
                    <etal data-class="RefEtal" >et al</etal>
                </person-group>
                <x >. </x>
                <year >2024</year>
                <x > </x>
                <article-title >A Survey on Multimodal
                    Large Language Models for Autonomous Driving</article-title>
                <x >. </x>
                <comment data-class="RefComments" >In</comment>
                <x > </x>
                <conf-name >
                    <italic >2024 IEEE/CVF Winter
                        Conference on Applications of Computer Vision Workshops (WACVW)</italic>
                </conf-name>
                <x >, </x>
                <conf-loc >
                    <named-content xmlns:xlink="http://www.w3.org/1999/xlink" data-cid="1713351935271"  content-type="ins cts-1" specific-use="1713351935271" xlink:title="CrossRef" alt="CrossRef" data-title="Inserted by CrossRef - Wed Apr 17 2024 11:05:35 GMT+0000 (Coordinated Universal Time)" data-time="1713351935271" data-userid="CrossRef" data-username="CrossRef">Waikoloa,
                        HI, USA</named-content>
                </conf-loc>
                <x >, </x>
                <comment data-class="RefComments" >pp</comment>
                <x >. </x>
                <fpage >958</fpage>
                <x >–</x>
                <lpage >979</lpage>
                <x >. </x>
                <publisher-loc >Waikoloa, HI</publisher-loc>
                <x >.</x>
            </element-citation>
        </ref>
        <p data-class="jrnlDeleted" colorcode="#" data-old-class="jrnlUntagged" class-name="Deleted" >9. Cui, C., Zhou, Y., Yang, X., Wu, S., Zhang,
            L., Zou, J., &amp; Yao, H. (2023). Holistic analysis of hallucination in GPT-4V(ision):
            Bias and interference challenges. arXiv. https://doi.org/10.48550/arXiv.2311.03287</p>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1016/j.trf.2023.01.014" data-validated-log="true" data-validated="true" seqid="R10" data-id="R10" class-name="RefText">
            <label >10.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >de Winter</surname>
                        <given-names >J</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Hoogmoed</surname>
                        <given-names >J</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Stapel</surname>
                        <given-names >J</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Dodou</surname>
                        <given-names >D</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Bazilinskyy</surname>
                        <given-names >P</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2023</year>
                <x > </x>
                <article-title >Predicting perceived risk
                    of traffic scenes using computer vision</article-title>
                <x >. </x>
                <source >
                    <italic >Transportation Research Part
                        F: Traffic Psychology and Behaviour</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >93</bold>
                </volume>
                <x >, </x>
                <fpage >235</fpage>
                <x >–</x>
                <lpage >247</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1016/j.trf.2023.01.014</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <p data-class="jrnlDeleted" colorcode="#" data-old-class="jrnlUntagged" class-name="Deleted" >11. Dubey, A., Naik, N., Parikh, D., Raskar,
            R., &amp; Hidalgo, C. A. (2016). Deep learning the city: Quantifying urban perception at
            a global scale. In B. Leibe, J. Matas, N. Sebe, &amp; M. Welling (Eds.), Computer
            Vision–ECCV 2016 (pp. 196–212). Cham: Springer.
            https://doi.org/10.1007/978-3-319-46448-0_12</p>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-pmid="37364096" data-validated-online="true" data-title="Validated from PubMed" data-doi="10.1073/pnas.2220417120" data-validated-log="true" data-validated="true" seqid="R12" data-id="R12" class-name="RefText">
            <label >12.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Fan</surname>
                        <given-names >Z</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Zhang</surname>
                        <given-names >F</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Loo</surname>
                        <given-names >BPY</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Ratti</surname>
                        <given-names >C</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2023</year>
                <x > </x>
                <article-title >Urban visual intelligence:
                    Uncovering hidden city profiles with street view images</article-title>
                <x >. </x>
                <source >
                    <italic >Proc. Natl. Acad. Sci. U.S.A.</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >120</bold>
                </volume>
                <x >, </x>
                <elocation-id >
                    <named-content xmlns:xlink="http://www.w3.org/1999/xlink" data-cid="1713351928047"  content-type="ins cts-1" specific-use="1713351928047" xlink:title="PubMed" alt="PubMed" data-title="Inserted by PubMed - Wed Apr 17 2024 11:05:28 GMT+0000 (Coordinated Universal Time)" data-time="1713351928047" data-userid="PubMed" data-username="PubMed">
                        e2220417120</named-content>
                </elocation-id>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1073/pnas.2220417120</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="304" class-name="RefText" seqid="R13"  data-id="R13" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-refuntagged="true" data-untag="true">
            <label >13.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Fu</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Peng</surname>, <given-names >H.</given-names></name>
                    <x >, </x>
                    <name ><surname >Sabharwal</surname>, <given-names >A.</given-names></name>
                    <x >, </x>
                    <name ><surname >Clark</surname>, <given-names >P.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Khot</surname>, <given-names >T.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2023</year>
                <x >). </x>
                <article-title >Complexity-based prompting
                    for multi-step reasoning</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2210.00720">
                    https://doi.org/10.48550/arXiv.2210.00720</uri>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1016/j.aap.2004.11.003" data-validated-log="true" data-validated="true" seqid="R14" data-id="R14" class-name="RefText">
            <label >14.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Fuller</surname>
                        <given-names >R</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2005</year>
                <x > </x>
                <article-title >Towards a general theory of
                    driver behaviour</article-title>
                <x >. </x>
                <source >
                    <named-content xmlns:xlink="http://www.w3.org/1999/xlink" data-cid="1713351924676"  content-type="ins cts-1" specific-use="1713351924676" xlink:title="CrossRef" alt="CrossRef" data-title="Inserted by CrossRef - Wed Apr 17 2024 11:05:24 GMT+0000 (Coordinated Universal Time)" data-time="1713351924676" data-userid="CrossRef" data-username="CrossRef">
                        <italic >Accident Analysis &amp;
                            Prevention</italic>
                    </named-content>
                </source>
                <x > </x>
                <volume >
                    <bold >37</bold>
                </volume>
                <x >, </x>
                <fpage >461</fpage>
                <x >–</x>
                <lpage >472</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1016/j.aap.2004.11.003</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1177/0278364913491297" data-validated-log="true" data-validated="true" seqid="R15" data-id="R15" class-name="RefText">
            <label >15.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Geiger</surname>
                        <given-names >A</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Lenz</surname>
                        <given-names >P</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Stiller</surname>
                        <given-names >C</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Urtasun</surname>
                        <given-names >R</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2013</year>
                <x > </x>
                <article-title >Vision meets robotics: The
                    KITTI dataset</article-title>
                <x >. </x>
                <source >
                    <italic >Int. J. Rob. Res.</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >32</bold>
                </volume>
                <x >, </x>
                <fpage >1231</fpage>
                <x >–</x>
                <lpage >1237</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1177/0278364913491297</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="307" class-name="RefText" seqid="R16"  data-id="R16" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2312.11805" data-refuntagged="true" data-untag="true">
            <label >16.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <given-names >Gemini Team</given-names>
                        <surname >Google</surname>
                    </name>
                </person-group>
                <x >. (</x>
                <year >2023</year>
                <x >). </x>
                <article-title >Gemini: A family of highly
                    capable multimodal models</article-title>
                <x >. ArXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2312.11805">
                    https://doi.org/10.48550/arXiv.2312.11805</uri>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1016/j.intell.2015.07.006" data-validated-log="true" data-validated="true" seqid="R17" data-id="R17" class-name="RefText">
            <label >17.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Gignac</surname>
                        <given-names >GE</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2015</year>
                <x > </x>
                <article-title >Raven’s is not a
                    pure measure of general intelligence: Implications for g factor theory and the
                    brief measurement of g</article-title>
                <x >. </x>
                <source >
                    <italic >Intelligence</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >52</bold>
                </volume>
                <x >, </x>
                <fpage >71</fpage>
                <x >–</x>
                <lpage >79</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1016/j.intell.2015.07.006</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="309" class-name="RefText" seqid="R18"  data-id="R18" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-refuntagged="true" data-untag="true">
            <label >18.</label>
            <element-citation publication-type="website" >
                <x > Google. (</x>
                <year >2023</year>
                <x >). </x>
                <article-title >What’s ahead for
                    Bard: More global, more visual, more integrated</article-title>
                <x >. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://blog.google/technology/ai/google-bard-updates-io-2023">
                    https://blog.google/technology/ai/google-bard-updates-io-2023</uri>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1016/j.isprsjprs.2022.02.012" data-validated-log="true" data-validated="true" seqid="R19" data-id="R19" class-name="RefText">
            <label >19.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Guan</surname>
                        <given-names >F</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Fang</surname>
                        <given-names >Z</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Wang</surname>
                        <given-names >L</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Zhang</surname>
                        <given-names >X</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Zhong</surname>
                        <given-names >H</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Huang</surname>
                        <given-names >H</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2022</year>
                <x > </x>
                <article-title >Modelling people’s
                    perceived scene complexity of real-world environments using street-view
                    panoramas and open geodata</article-title>
                <x >. </x>
                <source >
                    <italic >ISPRS Journal of
                        Photogrammetry and Remote Sensing</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >186</bold>
                </volume>
                <x >, </x>
                <fpage >315</fpage>
                <x >–</x>
                <lpage >331</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1016/j.isprsjprs.2022.02.012</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="311" class-name="RefText" seqid="R20"  data-id="R20" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="article-journal" data-refuntagged="true" data-untag="true">
            <label >20.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Guan</surname>, <given-names >T.</given-names></name>
                    <x >, </x>
                    <name ><surname >Liu</surname>, <given-names >F.</given-names></name>
                    <x >, </x>
                    <name ><surname >Wu</surname>, <given-names >X.</given-names></name>
                    <x >, </x>
                    <name ><surname >Xian</surname>, <given-names >R.</given-names></name>
                    <x >, </x>
                    <name ><surname >Li</surname>, <given-names >Z.</given-names></name>
                    <x >, </x>
                    <name ><surname >Liu</surname>, <given-names >X.</given-names></name>
                    <x >, </x>
                    <name ><surname >Wang</surname>, <given-names >X.</given-names></name>
                    <x >, </x>
                    <name ><surname >Chen</surname>, <given-names >L.</given-names></name>
                    <x >, </x>
                    <name ><surname >Huang</surname>, <given-names >F.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yacoob</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Manocha</surname>, <given-names >D.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Zhou</surname>, <given-names >T.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2023</year>
                <x >). </x>
                <volume >HALLUSIONBENCH</volume>
                <x >: An advanced diagnostic suite for
                    entangled language hallucination &amp; visual illusion in large vision-language
                    models. ArXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2310.14566">
                    https://doi.org/10.48550/arXiv.2310.14566</uri>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1016/j.trf.2022.02.016" data-validated-log="true" data-validated="true" seqid="R21" data-id="R21" class-name="RefText">
            <label >21.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >He</surname>
                        <given-names >X</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Stapel</surname>
                        <given-names >J</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Wang</surname>
                        <given-names >M</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Happee</surname>
                        <given-names >R</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2022</year>
                <x > </x>
                <article-title >Modelling perceived risk
                    and trust in driving automation reacting to merging and braking vehicles</article-title>
                <x >. </x>
                <source >
                    <italic >Transportation Research Part
                        F: Traffic Psychology and Behaviour</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >86</bold>
                </volume>
                <x >, </x>
                <fpage >178</fpage>
                <x >–</x>
                <lpage >195</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1016/j.trf.2022.02.016</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="paper-conference" data-reftype-name="Proceeding" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1145/3636243.3636247" data-validated-log="true" data-validated="true" seqid="R22" data-id="R22" class-name="RefText">
            <label >22.</label>
            <element-citation publication-type="confproc" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Hou</surname>
                        <given-names >I</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Man</surname>
                        <given-names >O</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Mettille</surname>
                        <given-names >S</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Gutierrez</surname>
                        <given-names >S</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Angelikas</surname>
                        <given-names >K</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >MacNeil</surname>
                        <given-names >S</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2024</year>
                <x > </x>
                <article-title >More Robots are Coming:
                    Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons
                    Problems</article-title>
                <x >. </x>
                <comment data-class="RefComments" >In</comment>
                <x > </x>
                <conf-name >
                    <italic >ACE 2024</italic>
                </conf-name>
                <x >, </x>
                <conf-loc >
                    <named-content xmlns:xlink="http://www.w3.org/1999/xlink" data-cid="1713351928219"  content-type="ins cts-1" specific-use="1713351928219" xlink:title="CrossRef" alt="CrossRef" data-title="Inserted by CrossRef - Wed Apr 17 2024 11:05:28 GMT+0000 (Coordinated Universal Time)" data-time="1713351928219" data-userid="CrossRef" data-username="CrossRef">Sydney
                        NSW Australia</named-content>
                </conf-loc>
                <x >, </x>
                <comment data-class="RefComments" >pp</comment>
                <x >. </x>
                <fpage >29</fpage>
                <x >–</x>
                <lpage >38</lpage>
                <x >. </x>
                <publisher-loc >New York, NY, USA</publisher-loc>
                <x >.</x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://dl.acm.org/doi/proceedings/10.1145/3636243">
                    https://dl.acm.org/doi/proceedings/10.1145/3636243</uri>
            </element-citation>
        </ref>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="314" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2311.14786" data-refuntagged="true" data-old-class="jrnlRefText" >23. Huang, J.,
            Jiang, P., Gautam, A., &amp; Saripalli, S. (2023). GPT-4V takes the wheel: Evaluating
            promise and challenges for pedestrian behavior prediction. arXiv.
            https://doi.org/10.48550/arXiv.2311.14786</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="315" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2402.06794" data-refuntagged="true" data-old-class="jrnlRefText" >24. Hwang, H.,
            Kwon, S., Kim, Y., &amp; Kim, D. (2024). Is it safe to cross? Interpretable risk
            assessment with GPT-4V for safety-aware street crossing. arXiv.
            https://doi.org/10.48550/arXiv.2402.06794</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="316" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2107.08142" data-refuntagged="true" data-old-class="jrnlRefText" >25. Jain, A., Del
            Pero, L., Grimmett, H., &amp; Ondruska, P. (2021). Autonomy 2.0: Why is self-driving
            always 5 years away? arXiv. https://doi.org/10.48550/arXiv.2107.08142</p>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="317" class-name="RefText" seqid="R26"  data-id="R26" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="article-journal" data-doi="10.1016/S0160-2896(03)00062-X">
            <label >26.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Johnson</surname>, <given-names >W.</given-names></name>
                    <x >, </x>
                    <name ><surname >Bouchard</surname>, <given-names >T. J.</given-names>, <suffix >Jr.</suffix></name>
                    <x >, </x>
                    <name ><surname >Krueger</surname>, <given-names >R. F.</given-names></name>
                    <x >, </x>
                    <name ><surname >McGue</surname>, <given-names >M.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Gottesman</surname>, <given-names >I. I.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2004</year>
                <x >). </x>
                <article-title >Just one g: Consistent
                    results from three test batteries</article-title>
                <x >. </x>
                <source >Intelligence</source>
                <x >, </x>
                <volume >32</volume>
                <x >, </x>
                <fpage >95</fpage>
                <x >–</x>
                <lpage >107</lpage>
                <x >. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.1016/S0160-2896(03)00062-X">
                    https://doi.org/10.1016/S0160-2896(03)00062-X</uri>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="318" class-name="RefText" seqid="R27"  data-id="R27" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="article-journal" data-doi="10.1038/s41467-020-18353-4" data-refuntagged="true" data-untag="true">
            <label >27.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Kolekar</surname>, <given-names >S.</given-names></name>
                    <x >, De </x>
                    <name ><surname >Winter</surname>, <given-names >J.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Abbink</surname>, <given-names >D.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2020</year>
                <x >). </x>
                <article-title >Human-like driving
                    behaviour emerges from a risk-based driver model</article-title>
                <x >. </x>
                <source >Nature Communications</source>
                <x >, </x>
                <volume >11</volume>
                <x >, </x>
                <fpage >4850</fpage>
                <x >. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.1038/s41467-020-18353-4">
                    https://doi.org/10.1038/s41467-020-18353-4</uri>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1016/j.trc.2021.103428" data-validated-log="true" data-validated="true" seqid="R28" data-id="R28" class-name="RefText">
            <label >28.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Kolekar</surname>
                        <given-names >S</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Petermeijer</surname>
                        <given-names >B</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Boer</surname>
                        <given-names >E</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >de Winter</surname>
                        <given-names >J</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Abbink</surname>
                        <given-names >D</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2021</year>
                <x > </x>
                <article-title >A risk field-based metric
                    correlates with driver’s perceived risk in manual and automated driving:
                    A test-track study</article-title>
                <x >. </x>
                <source >
                    <italic >Transportation Research Part
                        C: Emerging Technologies</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >133</bold>
                </volume>
                <x >, </x>
                <fpage >103428</fpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1016/j.trc.2021.103428</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" data-pos-index="320" class-name="RefText" seqid="R29"  data-id="R29" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="paper-conference">
            <label >29.</label>
            <element-citation publication-type="confproc" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Li</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Li</surname>, <given-names >D.</given-names></name>
                    <x >, </x>
                    <name ><surname >Xiong</surname>, <given-names >C.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Hoi</surname>, <given-names >S.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2022</year>
                <x >). </x>
                <article-title >BLIP: Bootstrapping
                    language-image pre-training for unified vision-language understanding and
                    generation</article-title>
                <x >. </x>
                <conf-name >Proceedings of the
                    International Conference on Machine Learning</conf-name>
                <x >, </x>
                <fpage >12888</fpage>
                <x >–</x>
                <lpage >12900</lpage>
                <x >.</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="321" class-name="RefText" seqid="R30"  data-id="R30" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2402.05120" data-refuntagged="true" data-untag="true">
            <label >30.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Li</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhang</surname>, <given-names >Q.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yu</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Fu</surname>, <given-names >Q.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Ye</surname>, <given-names >D.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2024</year>
                <x >). </x>
                <article-title >More agents is all you need</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2402.05120">
                    https://doi.org/10.48550/arXiv.2402.05120</uri>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="paper-conference" data-reftype-name="Proceeding" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.18653/v1/2023.acl-long.291" data-validated-log="true" data-validated="true" seqid="R31" data-id="R31" class-name="RefText">
            <label >31.</label>
            <element-citation publication-type="confproc" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Li</surname>
                        <given-names >Y</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Lin</surname>
                        <given-names >Z</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Zhang</surname>
                        <given-names >S</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Fu</surname>
                        <given-names >Q</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Chen</surname>
                        <given-names >B</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Lou</surname>
                        <given-names >JG</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Chen</surname>
                        <given-names >W</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2023</year>
                <x > </x>
                <article-title >Making Language Models
                    Better Reasoners with Step-Aware Verifier</article-title>
                <x >. </x>
                <comment data-class="RefComments" >In</comment>
                <x > </x>
                <conf-name >
                    <italic >Proceedings of the 61st Annual
                        Meeting of the Association for Computational Linguistics (Volume 1</italic>
                </conf-name>
                <x >, </x>
                <conf-loc >
                    <named-content xmlns:xlink="http://www.w3.org/1999/xlink" data-cid="1713351928552"  content-type="ins cts-1" specific-use="1713351928552" xlink:title="CrossRef" alt="CrossRef" data-title="Inserted by CrossRef - Wed Apr 17 2024 11:05:28 GMT+0000 (Coordinated Universal Time)" data-time="1713351928552" data-userid="CrossRef" data-username="CrossRef">Toronto,
                        Canada</named-content>
                </conf-loc>
                <x >, </x>
                <comment data-class="RefComments" >pp</comment>
                <x >. </x>
                <fpage >5315</fpage>
                <x >–</x>
                <lpage >5333</lpage>
                <x >. </x>
                <publisher-loc >Stroudsburg, PA, USA</publisher-loc>
                <x >.</x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://aclanthology.org/2023.acl-long">
                    https://aclanthology.org/2023.acl-long</uri>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="323" class-name="RefText" seqid="R32"  data-id="R32" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2311.07536" data-refuntagged="true" data-untag="true">
            <label >32.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Li</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Wang</surname>, <given-names >L.</given-names></name>
                    <x >, </x>
                    <name ><surname >Hu</surname>, <given-names >B.</given-names></name>
                    <x >, </x>
                    <name ><surname >Chen</surname>, <given-names >X.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhong</surname>, <given-names >W.</given-names></name>
                    <x >, </x>
                    <name ><surname >Lyu</surname>, <given-names >C.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Zhang</surname>, <given-names >M.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2024</year>
                <x >). </x>
                <article-title >A comprehensive evaluation
                    of GPT-4V on knowledge-intensive visual question answering</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2311.07536">
                    https://doi.org/10.48550/arXiv.2311.07536</uri>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1016/j.commtr.2023.100116" data-validated-log="true" data-validated="true" seqid="R33" data-id="R33" class-name="RefText">
            <label >33.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Liao</surname>
                        <given-names >H</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Shen</surname>
                        <given-names >H</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Li</surname>
                        <given-names >Z</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Wang</surname>
                        <given-names >C</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Li</surname>
                        <given-names >G</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Bie</surname>
                        <given-names >Y</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Xu</surname>
                        <given-names >C</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2024</year>
                <x > </x>
                <article-title >GPT-4 enhanced multimodal
                    grounding for autonomous driving: Leveraging cross-modal attention with large
                    language models</article-title>
                <x >. </x>
                <source >
                    <named-content xmlns:xlink="http://www.w3.org/1999/xlink" data-cid="1713351928944"  content-type="ins cts-1" specific-use="1713351928944" xlink:title="CrossRef" alt="CrossRef" data-title="Inserted by CrossRef - Wed Apr 17 2024 11:05:28 GMT+0000 (Coordinated Universal Time)" data-time="1713351928944" data-userid="CrossRef" data-username="CrossRef">
                        <italic >Communications in
                            Transportation Research</italic>
                    </named-content>
                </source>
                <x > </x>
                <volume >
                    <bold >4</bold>
                </volume>
                <x >, </x>
                <fpage >100116</fpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1016/j.commtr.2023.100116</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="325" class-name="RefText" seqid="R34"  data-id="R34" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="article-journal" data-doi="10.1037/a0033266">
            <label >34.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Little</surname>, <given-names >T. D.</given-names></name>
                    <x >, </x>
                    <name ><surname >Rhemtulla</surname>, <given-names >M.</given-names></name>
                    <x >, </x>
                    <name ><surname >Gibson</surname>, <given-names >K.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Schoemann</surname>, <given-names >A. M.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2013</year>
                <x >). </x>
                <article-title >Why the items versus
                    parcels controversy needn’t be one</article-title>
                <x >. </x>
                <source >Psychological Methods</source>
                <x >, </x>
                <volume >18</volume>
                <x >, </x>
                <fpage >285</fpage>
                <x >–</x>
                <lpage >300</lpage>
                <x >. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.1037/a0033266">https://doi.org/10.1037/a0033266</uri>
            </element-citation>
        </ref>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="326" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-refuntagged="true" data-old-class="jrnlRefText" >35. Liu, H., Li, C., Li, Y., &amp; Lee, Y. J.
            (2023). Improved baselines with visual instruction tuning. arXiv.
            https://doi.org/10.48550/arXiv.2310.03744</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="327" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2312.10637" data-refuntagged="true" data-old-class="jrnlRefText" >36. Liu, M.,
            Chen, C., &amp; Gurari, D. (2024). An evaluation of GPT-4V and Gemini in online VQA.
            arXiv. https://doi.org/10.48550/arXiv.2312.10637</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="328" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2402.08670" data-refuntagged="true" data-old-class="jrnlRefText" >37. Liu, Y.,
            Wang, Y., Sun, L., &amp; Yu, P. S. (2024). Rec-GPT4V: Multimodal recommendation with
            large vision-language models. arXiv. https://doi.org/10.48550/arXiv.2402.08670</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="329" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-refuntagged="true" data-old-class="jrnlRefText" >38. Lu, P., Bansal, H., Xia, T., Liu, J., Li,
            C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., &amp; Gao, J. (2023).
            MathVista: Evaluating mathematical reasoning of foundation models in visual contexts.
            arXiv. https://doi.org/10.48550/arXiv.2310.02255</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="330" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2401.02994" data-refuntagged="true" data-old-class="jrnlRefText" >39. Lu, X.,
            Liusie, A., Raina, V., Zhang, Y., &amp; Beauchamp, W. (2024). Blending is all you need:
            Cheaper, better alternative to trillion-parameters LLM. arXiv.
            https://doi.org/10.48550/arXiv.2401.02994</p>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1016/j.newideapsych.2011.02.008" data-validated-log="true" data-validated="true" seqid="R40" data-id="R40" class-name="RefText">
            <label >40.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Markus</surname>
                        <given-names >KA</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Borsboom</surname>
                        <given-names >D</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2013</year>
                <x > </x>
                <article-title >Reflective measurement
                    models, behavior domains, and common causes</article-title>
                <x >. </x>
                <source >
                    <italic >New Ideas Psychol.</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >31</bold>
                </volume>
                <x >, </x>
                <fpage >54</fpage>
                <x >–</x>
                <lpage >64</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1016/j.newideapsych.2011.02.008</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="332" class-name="RefText" seqid="R41"  data-id="R41" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="article-journal" data-doi="10.11575/ajer.v49i3.54980">
            <label >41.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >McDonald</surname>, <given-names >R. P.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2003</year>
                <x >). </x>
                <article-title >Behavior domains in theory
                    and in practice</article-title>
                <x >. </x>
                <source >Alberta Journal of Educational
                    Research</source>
                <x >, </x>
                <volume >49</volume>
                <x >, </x>
                <fpage >212</fpage>
                <x >–</x>
                <lpage >230</lpage>
                <x >. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.11575/ajer.v49i3.54980">
                    https://doi.org/10.11575/ajer.v49i3.54980</uri>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-pmid="29319468" data-validated-online="true" data-title="Validated from PubMed" data-doi="10.1080/00140139.2018.1426790" data-validated-log="true" data-validated="true" seqid="R42" data-id="R42" class-name="RefText">
            <label >42.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Melman</surname>
                        <given-names >T</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Abbink</surname>
                        <given-names >DA</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >van Paassen</surname>
                        <given-names >MM</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Boer</surname>
                        <given-names >ER</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >de Winter</surname>
                        <given-names >JCF</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2018</year>
                <x > </x>
                <article-title >What determines
                    drivers’ speed? A replication of three behavioural adaptation experiments
                    in A single driving simulator study</article-title>
                <x >. </x>
                <source >
                    <italic >Ergonomics</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >61</bold>
                </volume>
                <x >, </x>
                <fpage >966</fpage>
                <x >–</x>
                <lpage >987</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1080/00140139.2018.1426790</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1016/0001-4575(74)90003-7" data-validated-log="true" data-validated="true" seqid="R43" data-id="R43" class-name="RefText">
            <label >43.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Näätänen</surname>
                        <given-names >R</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Summala</surname>
                        <given-names >H</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >1974</year>
                <x > </x>
                <article-title >A model for the role of
                    motivational factors in drivers’ decision-making∗</article-title>
                <x >. </x>
                <source >
                    <named-content xmlns:xlink="http://www.w3.org/1999/xlink" data-cid="1713351926035"  content-type="ins cts-1" specific-use="1713351926035" xlink:title="CrossRef" alt="CrossRef" data-title="Inserted by CrossRef - Wed Apr 17 2024 11:05:26 GMT+0000 (Coordinated Universal Time)" data-time="1713351926035" data-userid="CrossRef" data-username="CrossRef">
                        <italic >Accident Analysis &amp;
                            Prevention</italic>
                    </named-content>
                </source>
                <x > </x>
                <volume >
                    <bold >6</bold>
                </volume>
                <x >, </x>
                <fpage >243</fpage>
                <x >–</x>
                <lpage >261</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1016/0001-4575(74)90003-7</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-pmid="32537189" data-validated-online="true" data-title="Validated from PubMed" data-doi="10.1098/rsos.191487" data-validated-log="true" data-validated="true" seqid="R44" data-id="R44" class-name="RefText">
            <label >44.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Nagle</surname>
                        <given-names >F</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Lavie</surname>
                        <given-names >N</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2020</year>
                <x > </x>
                <article-title >Predicting human complexity
                    perception of real-world scenes</article-title>
                <x >. </x>
                <source >
                    <italic >R. Soc. Open Sci.</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >7</bold>
                </volume>
                <x >, </x>
                <elocation-id >
                    <named-content xmlns:xlink="http://www.w3.org/1999/xlink" data-cid="1713351926206"  content-type="ins cts-1" specific-use="1713351926206" xlink:title="PubMed" alt="PubMed" data-title="Inserted by PubMed - Wed Apr 17 2024 11:05:26 GMT+0000 (Coordinated Universal Time)" data-time="1713351926206" data-userid="PubMed" data-username="PubMed">191487</named-content>
                </elocation-id>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1098/rsos.191487</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1073/pnas.1619003114" data-validated-log="true" data-validated="true" seqid="R45" data-id="R45" class-name="RefText">
            <label >45.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Naik</surname>
                        <given-names >N</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Kominers</surname>
                        <given-names >SD</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Raskar</surname>
                        <given-names >R</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Glaeser</surname>
                        <given-names >EL</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Hidalgo</surname>
                        <given-names >CA</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2017</year>
                <x > </x>
                <article-title >Computer vision uncovers
                    predictors of physical urban change</article-title>
                <x >. </x>
                <source >
                    <named-content xmlns:xlink="http://www.w3.org/1999/xlink" data-cid="1713351926248"  content-type="ins cts-1" specific-use="1713351926248" xlink:title="CrossRef" alt="CrossRef" data-title="Inserted by CrossRef - Wed Apr 17 2024 11:05:26 GMT+0000 (Coordinated Universal Time)" data-time="1713351926248" data-userid="CrossRef" data-username="CrossRef">
                        <italic >Proc. Natl. Acad. Sci.
                            U.S.A</italic>
                    </named-content>
                </source>
                <x > </x>
                <volume >
                    <bold >114</bold>
                </volume>
                <x >, </x>
                <fpage >7571</fpage>
                <x >–</x>
                <lpage >7576</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1073/pnas.1619003114</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <p data-class="jrnlDeleted" data-pos-index="337" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-refuntagged="true" data-old-class="jrnlRefText" >46. Nunnally, J. C., &amp; Bernstein, I. H.
            (1994). Psychometric theory. New York. NY: McGraw-Hill. OpenAI. (2023). GPT-4 technical
            report. https://cdn.openai.com/papers/gpt-4.pdf</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="339" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2312.15011" data-refuntagged="true" data-old-class="jrnlRefText" >47. Qi, Z., Fang,
            Y., Zhang, M., Sun, Z., Wu, T., Liu, Z., Lin, D., Wang, J., &amp; Zhao, H. (2023).
            Gemini vs GPT-4V: A preliminary comparison and combination of vision-language models
            through qualitative cases. arXiv. https://doi.org/10.48550/arXiv.2312.15011</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="340" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.1804.02767" data-refuntagged="true" data-old-class="jrnlRefText" >48. Redmon, J.,
            &amp; Farhadi, A. (2018). YOLOv3: An incremental improvement. arXiv.
            https://doi.org/10.48550/arXiv.1804.02767</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="341" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2401.03729" data-refuntagged="true" data-old-class="jrnlRefText" >49. Salinas, A.,
            &amp; Morstatter, F. (2024). The butterfly effect of altering prompts: How small changes
            and jailbreaks affect large language model performance. arXiv.
            https://doi.org/10.48550/arXiv.2401.03729</p>
        <ref  data-class="jrnlRefText" data-reftype="book" data-reftype-name="Book" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.4135/9781412961288" data-validated-log="true" data-validated="true" seqid="R50" data-id="R50" class-name="RefText">
            <label >50.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Salkind</surname>
                        <given-names >N</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <chapter-title >Encyclopedia of research
                    design</chapter-title>
                <x >. </x>
                <publisher-loc >2455 Teller Road, Thousand
                    Oaks California 91320 United States</publisher-loc>
                <x > . (</x>
                <pub-id pub-id-type="doi" >doi:
                    10.4135/9781412961288</pub-id>
                <x >) </x>
                <comment data-class="RefComments" >See</comment>
                <x > </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://methods.sagepub.com/reference/encyc-of-research-design">
                    https://methods.sagepub.com/reference/encyc-of-research-design</uri>
                <x >.</x>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="preprint" data-reftype-name="Pre print" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1101/2023.11.15.23298575" data-validated-log="true" data-validated="true" seqid="R51" data-id="R51" class-name="RefText">
            <label >51.</label>
            <element-citation publication-type="preprint" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Senkaiahliyan M.</surname>
                        <given-names >S</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Toma</surname>
                        <given-names >A</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Ma</surname>
                        <given-names >J</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Chan</surname>
                        <given-names >AW</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Ha</surname>
                        <given-names >A</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >An</surname>
                        <given-names >KR</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Suresh</surname>
                        <given-names >H</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Rubin</surname>
                        <given-names >B</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Wang</surname>
                        <given-names >B</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2023</year>
                <x > </x>
                <article-title >GPT-4V(ision) unsuitable
                    for clinical care and education: A clinician-evaluated assessment</article-title>
                <x >. </x>
                <source >
                    <italic >Medical Education</italic>
                </source>
                <x >. </x>
                <comment data-class="RefComments" >See</comment>
                <x > </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="medRxiv.%20https://doi.org/10.1101/2023.11.15.23298575">medRxiv.
                    https://doi.org/10.1101/2023.11.15.23298575</uri>
            </element-citation>
        </ref>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="344" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="article-journal" data-doi="10.1098/rsos.231053" data-refuntagged="true" data-old-class="jrnlRefText" >52. Tabone, W.,
            &amp; De Winter, J. C. F. (2023). Using ChatGPT for human-computer interaction: A
            primer. Royal Society Open Science, 10, 231053. https://doi.org/10.1098/rsos.231053</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="345" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-refuntagged="true" data-old-class="jrnlRefText" >53. Tang, R., Zhang, X., Ma, X., Lin, J.,
            &amp; Ture, F. (2023). Found in the middle: Permutation self-consistency improves
            listwise ranking in large language models. arXiv.
            https://doi.org/10.48550/arXiv.2310.07712</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="346" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2401.06209" data-refuntagged="true" data-old-class="jrnlRefText" >54. Tong, S.,
            Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., &amp; Xie, S. (2024). Eyes wide shut? Exploring
            the visual shortcomings of multimodal LLMs. arXiv.
            https://doi.org/10.48550/arXiv.2401.06209</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="347" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2203.11171" data-refuntagged="true" data-old-class="jrnlRefText" >55. Wang, X.,
            Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., &amp; Zhou, D.
            (2023). Self-consistency improves chain of thought reasoning in language models. arXiv.
            https://doi.org/10.48550/arXiv.2203.11171</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="348" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="chapter" data-doi="10.48550/arXiv.2201.11903" data-refuntagged="true" data-old-class="jrnlRefText" >56. Wei, J.,
            Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., &amp;
            Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models.
            In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, &amp; A. Oh (Eds.), Advances
            in neural information processing systems, Vol. 35 (pp. 24824–24837). Curran
            Associates, Inc. https://doi.org/10.48550/arXiv.2201.11903</p>
        <p data-class="jrnlDeleted" colorcode="#" data-pos-index="349" class-name="Deleted" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="article-journal" data-doi="10.48550/arXiv.2311.05332" data-refuntagged="true" data-old-class="jrnlRefText" >57. Wen, L., Yang, X., Fu, D., Wang, X., Cai,
            P., Li, X., Ma, T., Li, Y., Xu, L., Shang, D., Zhu, Z., Sun, S., Bai, Y., Cai, X., Dou,
            M., Hu, S., Shi, B., &amp; Qiao, Y. (2023). On the road with GPT-4V(ision): Early
            explorations of visual-language model on autonomous driving. arXiv.
            https://doi.org/10.48550/arXiv.2311.05332</p>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1111/j.1539-6924.1982.tb01384.x" data-validated-log="true" data-validated="true" seqid="R58" data-id="R58" class-name="RefText">
            <label >58.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Wilde</surname>
                        <given-names >GJS</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >1982</year>
                <x > </x>
                <article-title >The Theory of Risk
                    Homeostasis: Implications for Safety and Health</article-title>
                <x >. </x>
                <source >
                    <italic >Risk Analysis</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >2</bold>
                </volume>
                <x >, </x>
                <fpage >209</fpage>
                <x >–</x>
                <lpage >225</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1111/j.1539-6924.1982.tb01384.x</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" data-pos-index="351" class-name="RefText" seqid="R59"  data-id="R59" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="chapter" data-refuntagged="true" data-untag="true">
            <label >59.</label>
            <element-citation publication-type="other" >
                <x > Wilde, G. J. S. (2013). Homeostasis
                    drives behavioural adaptation. In C. M. Rudin-Brown &amp; S. L. Jamson (Eds.),
                    Behavioural adaptation and road safety: Theory, evidence and action (pp.
                    61–86). Boca Raton, FL: CRC Press.</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="352" class-name="RefText" seqid="R60"  data-id="R60" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="article-journal" data-refuntagged="true" data-untag="true">
            <label >60.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Wu</surname>, <given-names >C.</given-names></name>
                    <x >, </x>
                    <name ><surname >Lei</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zheng</surname>, <given-names >Q.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhao</surname>, <given-names >W.</given-names></name>
                    <x >, </x>
                    <name ><surname >Lin</surname>, <given-names >W.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhang</surname>, <given-names >X.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhou</surname>, <given-names >X.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhao</surname>, <given-names >Z.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhang</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Wang</surname>, <given-names >Y.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Xie</surname>, <given-names >W.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2023</year>
                <x >). Can GPT-4V(ision) serve medical
                    applications? Case studies on GPT-4V for multimodal medical diagnosis. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2310.09909">
                    https://doi.org/10.48550/arXiv.2310.09909</uri>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="353" class-name="RefText" seqid="R61"  data-id="R61" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2311.07562" data-refuntagged="true" data-untag="true">
            <label >61.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Yan</surname>, <given-names >A.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yang</surname>, <given-names >Z.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhu</surname>, <given-names >W.</given-names></name>
                    <x >, </x>
                    <name ><surname >Lin</surname>, <given-names >K.</given-names></name>
                    <x >, </x>
                    <name ><surname >Li</surname>, <given-names >L.</given-names></name>
                    <x >, </x>
                    <name ><surname >Wang</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yang</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhong</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >McAuley</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Gao</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Liu</surname>, <given-names >Z.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Wang</surname>, <given-names >L.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2023</year>
                <x >). </x>
                <article-title >GPT-4V in wonderland: Large
                    multimodal models for zero-shot smartphone GUI navigation</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2311.07562">
                    https://doi.org/10.48550/arXiv.2311.07562</uri>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="354" class-name="RefText" seqid="R62"  data-id="R62" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2401.08045" data-refuntagged="true" data-untag="true">
            <label >62.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Yan</surname>, <given-names >X.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhang</surname>, <given-names >H.</given-names></name>
                    <x >, </x>
                    <name ><surname >Cai</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Guo</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Qiu</surname>, <given-names >W.</given-names></name>
                    <x >, </x>
                    <name ><surname >Gao</surname>, <given-names >B.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhou</surname>, <given-names >K.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhao</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Jin</surname>, <given-names >H.</given-names></name>
                    <x >, </x>
                    <name ><surname >Gao</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Li</surname>, <given-names >Z.</given-names></name>
                    <x >, </x>
                    <name ><surname >Jiang</surname>, <given-names >L.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhang</surname>, <given-names >W.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhang</surname>, <given-names >H.</given-names></name>
                    <x >, </x>
                    <name ><surname >Dai</surname>, <given-names >D.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Liu</surname>, <given-names >B.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2024</year>
                <x >). </x>
                <article-title >Forging vision foundation
                    models for autonomous driving: Challenges, methodologies, and opportunities</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2401.08045">
                    https://doi.org/10.48550/arXiv.2401.08045</uri>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="355" class-name="RefText" seqid="R63"  data-id="R63" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-refuntagged="true" data-untag="true">
            <label >63.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Yang</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhang</surname>, <given-names >H.</given-names></name>
                    <x >, </x>
                    <name ><surname >Li</surname>, <given-names >F.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zou</surname>, <given-names >X.</given-names></name>
                    <x >, </x>
                    <name ><surname >Li</surname>, <given-names >C.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Gao</surname>, <given-names >J.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2023</year>
                <x >). </x>
                <article-title >Set-of-mark prompting
                    unleashes extraordinary visual grounding in GPT-4V</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2310.11441">
                    https://doi.org/10.48550/arXiv.2310.11441</uri>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="356" class-name="RefText" seqid="R64"  data-id="R64" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2309.17421" data-refuntagged="true" data-untag="true">
            <label >64.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Yang</surname>, <given-names >Z.</given-names></name>
                    <x >, </x>
                    <name ><surname >Li</surname>, <given-names >L.</given-names></name>
                    <x >, </x>
                    <name ><surname >Lin</surname>, <given-names >K.</given-names></name>
                    <x >, </x>
                    <name ><surname >Wang</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Lin</surname>, <given-names >C. C.</given-names></name>
                    <x >, </x>
                    <name ><surname >Liu</surname>, <given-names >Z.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Wang</surname>, <given-names >L.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2023</year>
                <x >). </x>
                <article-title >The dawn of LLMs:
                    Preliminary explorations with GPT-4V(ision)</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2309.17421">
                    https://doi.org/10.48550/arXiv.2309.17421</uri>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="357" class-name="RefText" seqid="R65"  data-id="R65" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2311.04257" data-refuntagged="true" data-untag="true">
            <label >65.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Ye</surname>, <given-names >Q.</given-names></name>
                    <x >, </x>
                    <name ><surname >Xu</surname>, <given-names >H.</given-names></name>
                    <x >, </x>
                    <name ><surname >Ye</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yan</surname>, <given-names >M.</given-names></name>
                    <x >, </x>
                    <name ><surname >Liu</surname>, <given-names >H.</given-names></name>
                    <x >, </x>
                    <name ><surname >Qian</surname>, <given-names >Q.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhang</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Huang</surname>, <given-names >F.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Zhou</surname>, <given-names >J.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2023</year>
                <x >). </x>
                <article-title >mPLUG-Owl2: Revolutionizing
                    multi-modal large language model with modality collaboration</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2311.04257">
                    https://doi.org/10.48550/arXiv.2311.04257</uri>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="358" class-name="RefText" seqid="R66"  data-id="R66" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2311.16502" data-refuntagged="true" data-untag="true">
            <label >66.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Yue</surname>, <given-names >X.</given-names></name>
                    <x >, </x>
                    <name ><surname >Ni</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhang</surname>, <given-names >K.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zheng</surname>, <given-names >T.</given-names></name>
                    <x >, </x>
                    <name ><surname >Liu</surname>, <given-names >R.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zhang</surname>, <given-names >G.</given-names></name>
                    <x >, </x>
                    <name ><surname >Stevens</surname>, <given-names >S.</given-names></name>
                    <x >, </x>
                    <name ><surname >Jiang</surname>, <given-names >D.</given-names></name>
                    <x >, </x>
                    <name ><surname >Ren</surname>, <given-names >W.</given-names></name>
                    <x >, </x>
                    <name ><surname >Sun</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Wei</surname>, <given-names >C.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yu</surname>, <given-names >B.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yuan</surname>, <given-names >R.</given-names></name>
                    <x >, </x>
                    <name ><surname >Sun</surname>, <given-names >R.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yin</surname>, <given-names >M.</given-names></name>
                    <x >, </x>
                    <name ><surname >Zheng</surname>, <given-names >B.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yang</surname>, <given-names >Z.</given-names></name>
                    <x >, </x>
                    <name ><surname >Liu</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Huang</surname>, <given-names >W.</given-names></name>
                    <x >, … </x>
                    <name ><surname >Chen</surname>, <given-names >W.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2023</year>
                <x >). </x>
                <article-title >MMMU: A massive
                    multi-discipline multimodal understanding and reasoning benchmark for expert AGI</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2311.16502">
                    https://doi.org/10.48550/arXiv.2311.16502</uri>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="359" class-name="RefText" seqid="R67"  data-id="R67" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2401.17600" data-refuntagged="true" data-untag="true">
            <label >67.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Zhang</surname>, <given-names >C.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Wang</surname>, <given-names >S.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2024</year>
                <x >). </x>
                <article-title >Good at captioning, bad at
                    counting: Benchmarking GPT-4V on Earth observation data</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2401.17600">
                    https://doi.org/10.48550/arXiv.2401.17600</uri>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="360" class-name="RefText" seqid="R68"  data-id="R68" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2401.02582" data-refuntagged="true" data-untag="true">
            <label >68.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Zhang</surname>, <given-names >D.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yang</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Lyu</surname>, <given-names >H.</given-names></name>
                    <x >, </x>
                    <name ><surname >Jin</surname>, <given-names >Z.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yao</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Chen</surname>, <given-names >M.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Luo</surname>, <given-names >J.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2024</year>
                <x >). </x>
                <article-title >CoCoT: Contrastive
                    chain-of-thought prompting for large multimodal models with multiple image
                    inputs</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2401.02582">
                    https://doi.org/10.48550/arXiv.2401.02582</uri>
            </element-citation>
        </ref>
        <ref  data-class="jrnlRefText" data-reftype="article-journal" data-reftype-name="Journal" data-csl="true" data-validated-online="true" data-title="Validated from CrossRef" data-doi="10.1016/j.landurbplan.2018.08.020" data-validated-log="true" data-validated="true" seqid="R69" data-id="R69" class-name="RefText">
            <label >69.</label>
            <element-citation publication-type="journal" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name >
                        <surname >Zhang</surname>
                        <given-names >F</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Zhou</surname>
                        <given-names >B</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Liu</surname>
                        <given-names >L</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Liu</surname>
                        <given-names >Y</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Fung</surname>
                        <given-names >HH</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Lin</surname>
                        <given-names >H</given-names>
                    </name>
                    <x >, </x>
                    <name >
                        <surname >Ratti</surname>
                        <given-names >C</given-names>
                    </name>
                </person-group>
                <x >. </x>
                <year >2018</year>
                <x > </x>
                <article-title >Measuring human perceptions
                    of a large-scale urban region using machine learning</article-title>
                <x >. </x>
                <source >
                    <italic >Landsc. Urban Plan.</italic>
                </source>
                <x > </x>
                <volume >
                    <bold >180</bold>
                </volume>
                <x >, </x>
                <fpage >148</fpage>
                <x >–</x>
                <lpage >160</lpage>
                <x >. (</x>
                <comment data-class="RefComments" >doi</comment>
                <x >:</x>
                <pub-id pub-id-type="doi" >
                    10.1016/j.landurbplan.2018.08.020</pub-id>
                <x >)</x>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="362" class-name="RefText" seqid="R70"  data-id="R70" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2311.01361" data-refuntagged="true" data-untag="true">
            <label >70.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Zhang</surname>, <given-names >X.</given-names></name>
                    <x >, </x>
                    <name ><surname >Lu</surname>, <given-names >Y.</given-names></name>
                    <x >, </x>
                    <name ><surname >Wang</surname>, <given-names >W.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yan</surname>, <given-names >A.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yan</surname>, <given-names >J.</given-names></name>
                    <x >, </x>
                    <name ><surname >Qin</surname>, <given-names >L.</given-names></name>
                    <x >, </x>
                    <name ><surname >Wang</surname>, <given-names >H.</given-names></name>
                    <x >, </x>
                    <name ><surname >Yan</surname>, <given-names >X.</given-names></name>
                    <x >, </x>
                    <name ><surname >Wang</surname>, <given-names >W. Y.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Petzold</surname>, <given-names >L. R.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2023</year>
                <x >). </x>
                <article-title >GPT-4V(ision) as a
                    generalist evaluator for vision-language tasks</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2311.01361">
                    https://doi.org/10.48550/arXiv.2311.01361</uri>
            </element-citation>
        </ref>
        <ref data-class="jrnlRefText" colorcode="#" data-pos-index="363" class-name="RefText" seqid="R71"  data-id="R71" data-styled="RefText" data-validated="false" data-validated-log="false" data-csl="true" data-reftype="webpage" data-doi="10.48550/arXiv.2402.02205" data-refuntagged="true" data-untag="true">
            <label >71.</label>
            <element-citation publication-type="website" >
                <x > </x>
                <person-group person-group-type="author" >
                    <name ><surname >Zhou</surname>, <given-names >X.</given-names></name>
                    <x >, &amp; </x>
                    <name ><surname >Knoll</surname>, <given-names >A. C.</given-names></name>
                </person-group>
                <x > (</x>
                <year >2024</year>
                <x >). </x>
                <article-title >GPT-4V as traffic
                    assistant: An in-depth look at vision language model on complex traffic events</article-title>
                <x >. arXiv. </x>
                <uri xmlns:xlink="http://www.w3.org/1999/xlink"  xlink:href="https://doi.org/10.48550/arXiv.2402.02205">
                    https://doi.org/10.48550/arXiv.2402.02205</uri>
            </element-citation>
        </ref>
        <title data-level="" data-pos-index="364" data-group-type="back" data-app-head="true" >Appendix</title>
        <p data-class="jrnlPara" data-pos-index="365" ><span lang="EN-GB" >
                <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/resources/integration/rs/rsos/RSOS-231676/resources/e9fcb770-fca9-11ee-8c71-098f5be83d68_files/image006.gif" />
            </span><span data-class="jrnlAppFigRef" data-tag-desc="APPFIGREF" data-title=" A1-F1 " data-tag-type="APPFIGREF" data-offset="0" data-match="Figure A1." data-pattern="" data-tag-index="230_1" data-citation-string=" A1-F1 " >
                <italic >Figure A1.</italic>
            </span>
            Results of YOLOv4 for 2 of the 210 images.</p>
        <p data-class="jrnlFootNotePara" data-id="BFN1" class-name="FootNotePara" data-pos-index="368" data-error-index="231_2" ><span data-class="label" >[1]</span>Regarding the
            findings in <xref  ref-type="fig" rid="F4" data-tag-desc="FIGTBLREF" data-tag-type="FIGTBLREF" data-offset="29" data-match="Figure 4" data-pattern="" data-tag-index="231_1" data-citation-string=" F4 ">Figure 4</xref>, the most frequent risk percentage was <named-content content-type="jrnlPatterns" data-title="LQUOTE" data-tag-desc="LQUOTE" data-tag-type="LQUOTE" data-offset="77" data-match="“" data-replace="‘" data-pattern="‘" data-tag-index="231_8" data-function="addTrackChanges" >“</named-content>
            20<named-content content-type="jrnlPatterns" data-title="RQUOTE" data-tag-desc="RQUOTE" data-tag-type="RQUOTE" data-offset="82" data-match="”" data-replace="’" data-pattern="’" data-tag-index="231_10" data-function="addTrackChanges" >”</named-content>,
            found in <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAL,UNT" data-tag-type="VAL" data-offset="96" data-match="17.9%" data-replace="17.9" data-pattern="$1" data-tag-index="231_4" >17.9</named-content><named-content content-type="jrnlPatterns" data-title="UNT" data-tag-desc="VAL,UNT" data-tag-type="UNT" data-offset="100" data-match="17.9%" data-replace="%" data-pattern="%" data-tag-index="231_4" >%</named-content>
            of all numeric outputs. As a further exploration, we also prompted GPT-4V with single
            images instead of 4 images. By submitting 210 images one at a time, each repeated 211
            times, GPT-4V was prompted <named-content content-type="jrnlPatterns" data-title="THSE" data-tag-desc="THSE" data-tag-type="THSE" data-offset="301" data-match="44,310" data-replace="44 310" data-pattern="44 310" data-tag-index="231_7" >44,310</named-content> times. Using this
            method, the output <named-content content-type="jrnlPatterns" data-title="LQUOTE" data-tag-desc="LQUOTE" data-tag-type="LQUOTE" data-offset="345" data-match="“" data-replace="‘" data-pattern="‘" data-tag-index="231_9" data-function="addTrackChanges" >“</named-content>20<named-content content-type="jrnlPatterns" data-title="RQUOTE" data-tag-desc="RQUOTE" data-tag-type="RQUOTE" data-offset="350" data-match="”" data-replace="’" data-pattern="’" data-tag-index="231_11" data-function="addTrackChanges" >”</named-content>
            appeared in <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAL,UNT" data-tag-type="VAL" data-offset="366" data-match="73.7%" data-replace="73.7" data-pattern="$1" data-tag-index="231_5" >73.7</named-content><named-content content-type="jrnlPatterns" data-title="UNT" data-tag-desc="VAL,UNT" data-tag-type="UNT" data-offset="370" data-match="73.7%" data-replace="%" data-pattern="%" data-tag-index="231_5" >%</named-content>
            of outputs. In other words, without a reference to other images, GPT-4V typically
            estimated the risk of a single traffic image at <named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAL,UNT" data-tag-type="VAL" data-offset="502" data-match="20%" data-replace="20" data-pattern="$1" data-tag-index="231_6" >
            20</named-content><named-content content-type="jrnlPatterns" data-title="UNT" data-tag-desc="VAL,UNT" data-tag-type="UNT" data-offset="504" data-match="20%" data-replace="%" data-pattern="%" data-tag-index="231_6" >%</named-content>. The validity
            coefficient for this single-image prompting approach was only <named-content content-type="jrnlPatterns" data-title="VAR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAR" data-offset="582" data-match="r = 0.38" data-replace="r" data-pattern="$1" data-tag-index="231_3" >
                <italic >r</italic>
            </named-content><named-content content-type="jrnlPatterns" data-title="OPR" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="OPR" data-offset="583" data-match="r = 0.38" data-replace=" = " data-pattern="default" data-tag-index="231_3" > = </named-content><named-content content-type="jrnlPatterns" data-title="VAL" data-tag-desc="VAR,OPR,VAL,OPRS,VALS,UNT" data-tag-type="VAL" data-offset="586" data-match="r = 0.38" data-replace="0.38" data-pattern="$3" data-tag-index="231_3" >0.38</named-content>, based on 211
            repetitions per image.</p>
    </back>
    
    
</article>